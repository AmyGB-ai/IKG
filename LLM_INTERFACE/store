CODE_SNIP-> import createJsonFeats # returnEmbed
import os, json, sys, traceback, db_utils
from rank_bm25 import BM25Okapi
import numpy as np
from scipy.spatial import distance

txt = sys.argv[1]

emb_ = createJsonFeats.returnEmbed( txt )

res_ = db_utils.searchSignature( {'docSignature': emb_} )
corpus_ = dict()

def pos( res_ ):
  if 'searchRes_' in res_:
    act_ = res_[ 'searchRes_' ]
    print( act_ )
    tokenized_hdr_info_ , tokenized_sample_summary_, tokenized_dates_, title = [], [], [], []
    hdr_info_D = dict()

    for res_nm, resD in act_.items():
        if 'payload' in resD and 'summary' in resD[ 'payload' ]:
            corpus_[ ( resD[ 'payload' ][ 'summary' ] ) ] = resD[ 'score' ]

            tokenized_hdr_info_.append( resD[ 'payload' ][ 'hdr_info' ] )
            tokenized_sample_summary_.append( 'sample' )
            tokenized_dates_.append( resD[ 'payload' ][ 'date_range' ] )
            title.append( resD[ 'payload' ]['file_name'] )

            hdr_info_D[ ( resD[ 'payload' ][ 'summary' ] ) ] = createJsonFeats.returnEmbed( resD['payload']['hdr_info'] )

    top_by_vector_score_ = dict( sorted( corpus_.items(), key=lambda x:x[1], reverse=True ) )
    for idx, key in enumerate( list( top_by_vector_score_.keys() )[:10] ):
        print('-----------------------------------------')
        cos_dist_ = distance.cosine( emb_, hdr_info_D[ key ] )
        print('Rank ',idx+1,' CONTEXT->', key, ' SCORE->', top_by_vector_score_[key], ' HDR DISTANCE->', cos_dist_ )
        print('-----------------------------------------')

    tokenized_corpus = [doc.split(" ") for doc in list( corpus_.keys() )]
    bm25_summary_, bm25_hdr_info_, bm25_sample_summary_, bm25_dates_, title = \
            BM25Okapi(tokenized_corpus), BM25Okapi( tokenized_hdr_info_ ), \
            BM25Okapi( tokenized_sample_summary_ ), BM25Okapi( tokenized_dates_ ), BM25Okapi( title )

    tokenized_query = txt.split(" ")
    bm25_score_summary_  = bm25_summary_.get_scores(tokenized_query)
    bm25_score_hdr_  = bm25_hdr_info_.get_scores(tokenized_query)
    bm25_score_sample_  = bm25_sample_summary_.get_scores(tokenized_query)
    bm25_score_dt_  = bm25_dates_.get_scores(tokenized_query)
    score_title_  = title.get_scores(tokenized_query)

    enum_doc_scores_ = list( enumerate( bm25_score_summary_ ) )
    sorted_doc_score_ = sorted( enum_doc_scores_, key=lambda x:x[1] , reverse=True )

    enum_doc_scores_ = list( enumerate( bm25_score_hdr_ ) )
    sorted_doc_score_1 = sorted( enum_doc_scores_, key=lambda x:x[1] , reverse=True )

    enum_doc_scores_ = list( enumerate( bm25_score_sample_ ) )
    sorted_doc_score_2 = sorted( enum_doc_scores_, key=lambda x:x[1] , reverse=True )

    enum_doc_scores_ = list( enumerate( bm25_score_dt_ ) )
    sorted_doc_score_3 = sorted( enum_doc_scores_, key=lambda x:x[1] , reverse=True )

    enum_doc_scores_ = list( enumerate( score_title_ ) )
    sorted_doc_score_4 = sorted( enum_doc_scores_, key=lambda x:x[1] , reverse=True )

    for keyid, keys in enumerate( list( corpus_.keys() ) ):
        print('--------', keyid, np.asarray( sorted_doc_score_ )[:3, :1])    
        if [keyid] not in np.asarray( sorted_doc_score_ )[:3, :1]: continue

        print( 'BM25 Summary :: Text: ', keys, ' Vector score: ', corpus_[ keys ],\
                ' BM25 : ', bm25_score_summary_[keyid] )

    for keyid, keys in enumerate( list( corpus_.keys() ) ):
        print('--------', keyid, np.asarray( sorted_doc_score_1 )[:3, :1])    
        if [keyid] not in np.asarray( sorted_doc_score_1 )[:3, :1]: continue

        print( 'BM25 HDR :: Text: ', keys, ' Vector score: ', corpus_[ keys ],\
                ' BM25 : ', bm25_score_hdr_[keyid] )

    for keyid, keys in enumerate( list( corpus_.keys() ) ):
        print('--------', keyid, np.asarray( sorted_doc_score_2 )[:3, :1])    
        if [keyid] not in np.asarray( sorted_doc_score_2 )[:3, :1]: continue

        print( 'BM25 Sample :: Text: ', keys, ' Vector score: ', corpus_[ keys ],\
                ' BM25 : ', bm25_score_sample_[keyid] )

    for keyid, keys in enumerate( list( corpus_.keys() ) ):
        print('--------', keyid, np.asarray( sorted_doc_score_3 )[:3, :1])    
        if [keyid] not in np.asarray( sorted_doc_score_3 )[:3, :1]: continue

        print( 'BM25 Date :: Text: ', keys, ' Vector score: ', corpus_[ keys ],\
                ' BM25 : ', bm25_score_dt_[keyid] )

    for keyid, keys in enumerate( list( corpus_.keys() ) ):
        print('--------', keyid, np.asarray( sorted_doc_score_4 )[:3, :1])    
        if [keyid] not in np.asarray( sorted_doc_score_4 )[:3, :1]: continue

        print( 'BM25 Date :: Text: ', keys, ' Vector score: ', corpus_[ keys ],\
                ' BM25 : ', score_title_[keyid] )


CODE_SNIP-> import createJsonFeats
import json, urllib

flask_url_ = 'http://0.0.0.0:5200/'
#flask_url_ = 'http://13.127.133.141:5000/'

url_insert = flask_url_ + 'dbInsert'
url_search = flask_url_ + 'dbSearch'
url_update = flask_url_ + 'dbUpdate'

def returnBlankDBRec():
    dbRec_ = dict()
    dbRec_['docID'] = ''
    dbRec_['docSignature'] = []
    dbRec_['tupArr'] = []
    dbRec_['ocr_op'] = [] ## assing raw ocr op ['lines']
    dbRec_['dimension'] = [] ## assing raw ocr op ht, wd
    dbRec_['tableFeedback'] = dict()
    dbRec_['feedbackDict'] = [ { 'config_field_nm': '',\
                               'field_co_ords':[],\
                               'field_datatype': '',\
                               'feedback_value': '',\
                               'local_neigh_dict': dict() } ]
    dbRec_['exception_feedback'] = [] ## will contain dicts of fmt -> 
            ## { 'docID':, 'failed_fields': [ { 'config_field_nm':, 'feedback_value':, 'feedback_co_ords':, 'comments;' } ]
    dbRec_['success_feedback'] = [] ## array of dicts        
    ## { 'docID':, 'passed_fields': [ { 'config_field_nm':, 'local_field':, 'feedback_value':, 'feedback_co_ords': , 'comments': } ]

    return dbRec_

def insertNewSignature( rec_ ):

    data = json.dumps( rec_ ).encode('utf-8')
    insert_request = urllib.request.Request( url_insert, data=data, method='POST', \
                                              headers={'Content-Type': 'application/json'})

    response = urllib.request.urlopen( insert_request )
    string = response.read().decode('utf-8')

    return string

def updateSignature( rec_ ):

    data = json.dumps( rec_ ).encode('utf-8')
    insert_request = urllib.request.Request( url_update, data=data, method='POST', \
                                              headers={'Content-Type': 'application/json'})

    response = urllib.request.urlopen( insert_request )
    string = response.read().decode('utf-8')

    return string

def searchSignature( rec_ ):

    data = json.dumps( rec_ ).encode('utf-8')
    search_request = urllib.request.Request( url_search, data=data, method='POST', \
                                                headers={'Content-Type': 'application/json'} )
    response = urllib.request.urlopen( search_request )
    string = response.read().decode('utf-8')
    json_obj = json.loads(string)

    return json_obj

CODE_SNIP-> import os
from openai import OpenAI

client = OpenAI(
    # This is the default and can be omitted
    api_key="sk-proj-jAfrcY6XXrnEcY8sfSexT3BlbkFJ2uzU6vTgITfQGj2NJpY3",
)

def returnOpenAI_response( dataframe ):

    completion = client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": '''You are an agent that specializes in summarizing the tabular content attached below. The summary needs to be condensed to 5 lines at max and needs to mention important notes like dates and any other frequently occuring information. \n''' + dataframe,
            }
        ],
        model="gpt-4-turbo",
    )

    print( 'FINAL RESP->', completion.choices[0].message.content )


CODE_SNIP-> import json, sys, random, cv2, traceback
import numpy as np
from scipy.spatial import distance
from sklearn.cluster import KMeans
from collections import Counter

total_feats_ = 11

def xOverlap( val, pts, ref_val, ref_pts, dist_=150 ):
    ## check if anything above or below
    #print( abs( pts[-1] - ref_pts[1] ), pts[0] >= ref_pts[0] and pts[2] <= ref_pts[2], pts, ref_pts )
    if abs( pts[-1] - ref_pts[1] ) <= dist_ or abs( pts[1] - ref_pts[-1] ) <= dist_:
        if ( pts[0] >= ref_pts[0] and pts[2] <= ref_pts[2] ) or \
           ( ref_pts[0] >= pts[0] and ref_pts[2] <= pts[2] ) or \
           ( pts[0] >= ref_pts[0] and pts[0] <= ref_pts[2] ) or \
           ( ref_pts[0] >= pts[0] and ref_pts[0] <= pts[2] ) or \
           ( ref_pts[0] < pts[0] and ref_pts[2] > pts[0] and ref_pts[2] <= pts[2] and ( abs( abs( ref_pts[0] + ref_pts[2] )/2 - ( abs( pts[0] + pts[2] ) )/2 ) )/( min( abs( ref_pts[0] - ref_pts[2]), abs( pts[0] - pts[2] )  )  ) < 0.8 )            or\
           ( pts[0] < ref_pts[0] and pts[2] > ref_pts[0] and pts[2] <= ref_pts[2] and ( abs( abs( ref_pts[0] + ref_pts[2] )/2 - ( abs( pts[0] + pts[2] ) )/2 ) )/( min( abs( ref_pts[0] - ref_pts[2]), abs( pts[0] - pts[2] )  )  ) < 0.8 ):
             #print( val, pts, ' X OVERLAPS with ', ref_val, ref_pts, abs( ref_pts[0] + ref_pts[2]), abs( pts[0] + pts[2] ), abs( ref_pts[0] + ref_pts[2] )/2,  abs( pts[0] + pts[2] )/2, abs( abs( ref_pts[0] + ref_pts[2] )/2 - ( abs( pts[0] + pts[2] ) )/2 )  )
             return True
    return False

def allNum( wd_, mode='NA' ):
  digs, special, illegal, digs2 =0, 0, 0, 0

  arr_ = wd_.split()
  ## fir conjoined DATE 31/12/2003 instead of just 21/12/2004
  if len( arr_ ) > 1 and len( arr_[-1] ) >= 3 and mode == 'NA':
    chk = arr_[-1]
    for char in chk:
      if ord(char) >= 48 and ord(char) <= 57: digs += 1
      if ord(char) >= 65 and ord(char) <= 90: digs2 += 1
      if ord(char) >= 97 and ord(char) <= 122: illegal += 1
      if char in [',','.','$','S','-','/']: special += 1
    if ( digs+digs2+special == len( chk ) and digs >= 1 ) or ( digs >= 4 and illegal <= 4 ): 
      #print('01')
      return True

  digs, special, illegal, digs2 =0, 0, 0, 0
  for char in wd_:
    if ord(char) >= 48 and ord(char) <= 57: digs += 1
    if ord(char) >= 65 and ord(char) <= 90: digs2 += 1
    if char in [',','.','$','S','-','/']: special += 1
    if ord(char) >= 97 and ord(char) <= 122: illegal += 1

  if ( digs+digs2+special == len( wd_ ) and digs >= 2 ) or ( digs >= 4 and illegal <= 4 ):
      #print('02')
      return True
  if mode == 'SPIKE' and ( digs >= 1 or digs+digs2+special == len( wd_ ) ):
      print('03')
      return True

  return False

def euclid( refpts, pts, json_ ):

    ht, wd = json_['height'], json_['width']
    ref_ = [ refpts[0]/wd, refpts[1]/ht, refpts[2]/wd, refpts[3]/ht ]
    pts_ = [ pts[0]/wd, pts[1]/ht, pts[2]/wd, pts[3]/ht ]

    return distance.euclidean( refpts, pts )

def findValFeats( refwd ):

    txt = refwd['text']
    if ':' in txt:
      txt = txt.split(':')[-1]

    numValFeats = 7
    returnFeats = np.zeros(( numValFeats ))

    if len( txt.split() ) < 1: return returnFeats

    if allNum( txt.split()[-1] ) and len( txt.split()[-1] ) >= 4 and not\
      ( ',' in txt or 'box' in txt.lower() ) and not allNum( txt.replace(' ','') ):
      txt = txt.split()[-1]

    digs, caps, small, special, begcaps = 0, 0, 0, 0, 0
    for char in txt:
      if ord(char) >= 48 and ord(char) <= 57: digs += 1
      if ord(char) >= 65 and ord(char) <= 90: caps += 1
      if ord(char) >= 97 and ord(char) <= 122: small += 1
      if char in [',','.','$','S','-','/',' ']: special += 1

    lenwds_ = []
    for wd in txt.split():
      if ord( wd[0] ) >= 65 and ord( wd[0] ) <= 90: begcaps += 1
      lenwds_.append( len(wd) )

    returnFeats[0] = digs
    returnFeats[1] = caps
    returnFeats[2] = small
    returnFeats[3] = len( txt.split() )
    returnFeats[4] = begcaps
    returnFeats[5] = len( txt )
    returnFeats[6] = np.median( lenwds_ )

    return returnFeats

def findWdFeats( refwd ):

    #print('GUAGING->', refwd)
    txt = refwd
    if ':' in txt and len( txt.split(':')[-1] ) > 1:
      txt = txt.split(':')[0]

    returnFeats = np.zeros((6))
    #returnFeats = np.zeros((total_feats_))

    if len( txt.split() ) < 1: return returnFeats
    #if len( txt.split() ) < 1: return returnFeats.tolist() + np.zeros((4)).tolist()

    if allNum( txt.split()[-1] ) and len( txt.split()[-1] ) >= 4 and not\
      ( ',' in txt or 'box' in txt.lower() ):
      txt = txt.split()[-1]

    digs, caps, small, special, begcaps = 0, 0, 0, 0, 0
    for char in txt:
      if ord(char) >= 48 and ord(char) <= 57: digs += 1
      if ord(char) >= 65 and ord(char) <= 90: caps += 1
      if ord(char) >= 97 and ord(char) <= 122: small += 1
      if char in [',','.','$','S','-','/',' ']: special += 1

    lenwds_ = []
    for wd in txt.split():
      if ord( wd[0] ) >= 65 and ord( wd[0] ) <= 90: begcaps += 1
      lenwds_.append( len(wd) )      

    returnFeats[0] = digs
    returnFeats[1] = min( caps, len( txt.split() ) ) if caps > 0 else caps ## dont want to diff KEYs that are in all caps
    returnFeats[2] = small
    returnFeats[3] = len( txt.split() )
    returnFeats[4] = begcaps
    returnFeats[5] = np.median( lenwds_ )

    print('MACADEMIA NUTS ->', txt,' returnFeats ',returnFeats, ' LEN = ', len(returnFeats)) 
    return returnFeats 

def findRaw( ids, json_raw ):

    txtarr, ptsarr = [], []
    consti_ = []

    for id_ in ids:
      for line in json_raw['lines']:
        for wdctr in range( len(line) ):
          wd = line[ wdctr ]
          if wd['id'] == id_:
            consti_.append( wd )

    txt_, pts_ = '', []
    for ctr in range( len(consti_) ):
      wd_ = consti_[ ctr ]
      #print('CHURNING->', wd_, txt_, pts_, ctr, consti_)
      if ctr < len( consti_ ) - 1:
        if abs( wd_['pts'][2] - consti_[ ctr + 1 ]['pts'][0] ) < 30 and len( pts_ ) == 0:
          wdpts = consti_[ ctr + 1 ]['pts']
          txt_, pts_ = wd_['text']+' '+ consti_[ ctr + 1 ]['text'], \
                       [ wd_['pts'][0], wd_['pts'][1], wdpts[2], wdpts[3] ]   
        elif abs( wd_['pts'][2] - consti_[ ctr + 1 ]['pts'][0] ) >= 30 and len( pts_ ) == 0:

          txt_, pts_ = wd_['text'], wd_['pts']  
        elif abs( wd_['pts'][2] - consti_[ ctr + 1 ]['pts'][0] ) < 30 and len( pts_ ) > 0\
          and wd_['text'] not in txt_:

          txt_ += ' ' + wd_['text']
          pts_ = [ pts_[0], pts_[1], wd_['pts'][2], wd_['pts'][3] ] 
        elif abs( wd_['pts'][2] - consti_[ ctr + 1 ]['pts'][0] ) < 30 and len( pts_ ) > 0\
          and wd_['text'] in txt_ and consti_[ ctr + 1 ]['text'] not in txt_:

          txt_ += ' ' + consti_[ ctr + 1 ]['text']
          pts_ = [ pts_[0], pts_[1], consti_[ ctr + 1 ]['pts'][2], consti_[ ctr + 1 ]['pts'][3] ] 
        elif abs( wd_['pts'][2] - consti_[ ctr + 1 ]['pts'][0] ) >= 30 and len( pts_ ) > 0:

          txtarr.append( txt_ )
          ptsarr.append( pts_ )
          txt_, pts_ = '', []
          #txt_, pts_ = wd_['text'], wd_['pts']  

      elif ctr >= len( consti_ ) - 1 and len( pts_ ) > 0 and wd_['text'] not in txt_:

          txt_ += ' ' + wd_['text']
          pts_ = [ pts_[0], pts_[1], wd_['pts'][2], wd_['pts'][3] ] 

      elif ctr >= len( consti_ ) - 1 and len( pts_ ) == 0:

          txt_, pts_ = wd_['text'], wd_['pts']  

    if len( pts_ ) > 0:
          txtarr.append( txt_ )
          ptsarr.append( pts_ )

    return txtarr, ptsarr 

def featNum( txt ):

    digs, caps, small, special = 0, 0, 0, 0
    for char in txt:
      if ord(char) >= 48 and ord(char) <= 57: digs += 1
      if ord(char) >= 65 and ord(char) <= 90: caps += 1
      if ord(char) >= 97 and ord(char) <= 122: small += 1
      if char in [',','.','$','-','/',' ',":"]: special += 1

    if '.0' in txt and digs == 1 and caps > 0 and special > 0: digs = 0
    #print( 'DESPO->', txt, ' digs, caps, special, small = ',digs, caps, special, small)
    if digs+special == len(txt) and digs > 0: return 1 # num
    if digs+caps+special == len(txt) and digs > 0 and not ( digs == 1 and '0' in txt and caps >=3 ): return 2 # alnum
    if digs+caps+special+small == len(txt) and digs > 0 and caps > 0 and not ( digs == 1 and '0' in txt and caps >= 3 ): return 2
    if digs+caps+special+small == len(txt) and digs >=4 : return 2
    if caps+special+small == len(txt) and digs == 0 and small > 0: return 3 # mixed str
    if special+small == len(txt) and digs == 0 and small > 0: return 4 # small cap
    if special+caps == len(txt) and digs == 0: return 5 # large cap

    return 3 # default val

def findNeighBour( ref_wd_ctr, lctr, json_, ref_txt, ref_pts ):

    ## look right 
    curr_wd = {'text': ref_txt, 'pts': ref_pts}

    for rt_wd_ctr in range( ref_wd_ctr, len( json_['lines'][lctr]) ):
      rt_wd = json_['lines'][lctr][rt_wd_ctr]

      if curr_wd['pts'][0] < rt_wd['pts'][0] and len( rt_wd['text'] ) > 1:

        currtype, nexttype = featNum( curr_wd['text'] ), featNum( rt_wd['text'] )  
        print( 'CHECKOUT-> curr_wd, rt_wd ', curr_wd, rt_wd, currtype, nexttype )
        if currtype in [ 3,5 ] and nexttype in [ 1, 2, 5 ] and currtype != nexttype:
          #print('RT NEIGH IS DIFF!!')
          return ( True, nexttype, rt_wd, findValFeats( rt_wd  ) )

        break

    ## in cases like Date: June 17, 2022 which come as one word ?
    if ':' in curr_wd['text'] and allNum( curr_wd['text'].split(':')[-1] ):
      #print('POSSIBLE MONSIEUR->', curr_wd['text'])

      neokey_ = curr_wd['text'].split(':')[0]
      neowd_ = { 'text': curr_wd['text'].split(':')[-1], 'pts': curr_wd['pts'] }
      currtype, nexttype = featNum( neokey_ ), featNum( neowd_['text'] ) 

      if currtype in [ 3,5 ] and nexttype in [ 1, 2, 5 ] and currtype != nexttype:
          #print('RT SPLIT NEIGH IS DIFF!!')
          return ( True, nexttype, neowd_, findValFeats( neowd_  ), neokey_ )


    if len( curr_wd['text'].split() ) > 1:
            tmp_arr_ = curr_wd['text'].split()
            #print('POSSIBLE MONSIEUR->', curr_wd['text'], len( tmp_arr_[-1] ), allNum( tmp_arr_[-1] ) )
            if len( tmp_arr_[-1] ) >= 3 and allNum( tmp_arr_[-1] ):
              txt1 = tmp_arr_[-1] 
              lt_neigh = ' '.join( tmp_arr_[:-1] )

              neokey_ = lt_neigh
              neowd_ = { 'text': tmp_arr_[-1], 'pts': curr_wd['pts'] }
              currtype, nexttype = featNum( neokey_ ), featNum( neowd_['text'] ) 

              if currtype in [ 3,5 ] and nexttype in [ 1, 2, 5 ] and currtype != nexttype:
                #print('RT SPLIT NEIGH IS DIFF!!')
                return ( True, nexttype, neowd_, findValFeats( neowd_  ), neokey_ )

    ## look below
    unsuitable_x_overlap = False
    for ctr in range( min( lctr + 1, len( json_['lines'] )-1 ), min( lctr + 4, len( json_['lines'] )-1 ) ):
      curr_line ,line_ = json_['lines'][lctr], json_['lines'][ctr]
      #print('ARRGHH BOTTOM FEEDERS!->', curr_line, line_)
      anyXfound_ = False
      for wd in curr_line:
        for nxtlinewd in line_:
          if wd['pts'] == curr_wd['pts']: continue 
          if xOverlap( nxtlinewd['text'], nxtlinewd['pts'], wd['text'], wd['pts'] ): 
            anyXfound_ = True
            #print('anyXfound_ is TRUE !! ', nxtlinewd, wd)

      for nxtlinewd in line_:
        if xOverlap( nxtlinewd['text'], nxtlinewd['pts'], curr_wd['text'], curr_wd['pts'] ):
          currtype, nexttype = featNum( curr_wd['text'] ), featNum( nxtlinewd['text'] )

          #print('CHECKOUT-> curr_wd, bot_wd ', curr_wd, nxtlinewd, currtype, nexttype )
          if currtype in [ 3,5 ] and nexttype in [ 1, 2, 5 ] and currtype != nexttype:
            #print('BOTTOM NEIGH IS DIFF!!')
            return ( True, nexttype, nxtlinewd, findValFeats( nxtlinewd ) )
          else:
            unsuitable_x_overlap = True

      ## if no xoverlap found in next line DONT assume we need to move to next + 1 .. sometimes the value below
      ## the key is absent and we SHOULD NOT pick value from next+1 ..hence we check if the next line has ANY
      ## content that xOVERLAPS with the current line ..if so then we break
      if unsuitable_x_overlap: 
      #if anyXfound_:
        #print('Nothing found TO THE RT or BELOW ->', ref_wd_ctr)
        return False, None 

    return False, None

def findValNeighBour( ref_wd_ctr, lctr, json_ ):

    ## look left

    curr_wd = json_['lines'][lctr][ref_wd_ctr]

    for lt_wd_ctr in range( max( 0, ref_wd_ctr-1 ), -1, -1 ):
      lt_wd = json_['lines'][lctr][lt_wd_ctr]
      #print('LEFT NEIGH FOR->', curr_wd)

      if curr_wd['pts'][0] > lt_wd['pts'][0] and len( lt_wd['text'] ) > 1:

        currtype, nexttype = featNum( curr_wd['text'] ), featNum( lt_wd['text'] )  
        print( 'CHECKOUT-> curr_wd, lt_wd ', curr_wd, lt_wd, currtype, nexttype )
        if currtype in [ 1,2 ] and nexttype in [ 3, 5 ] and currtype != nexttype:
          print('LT NEIGH IS DIFF!!')
          return ( True, nexttype, lt_wd, findValFeats( lt_wd  ) )

        break

    ## look above
    unsuitable_x_overlap = False
    for ctr in range( max( lctr - 1 , 0 ), max( lctr - 4, 0 ), -1 ):
      curr_line ,line_ = json_['lines'][lctr], json_['lines'][ctr]
      print('ARRGHH TOP FEEDERS!->', curr_line, line_)
      anyXfound_ = False
      for wd in curr_line:
        for nxtlinewd in line_:
          if wd['pts'] == curr_wd['pts']: continue 
          if xOverlap( nxtlinewd['text'], nxtlinewd['pts'], wd['text'], wd['pts'] ): 
            anyXfound_ = True
            print('anyXfound_ is TRUE !! ', nxtlinewd, wd)

      for nxtlinewd in line_:
        if xOverlap( nxtlinewd['text'], nxtlinewd['pts'], curr_wd['text'], curr_wd['pts'] ):
          currtype, nexttype = featNum( curr_wd['text'] ), featNum( nxtlinewd['text'] )

          print('CHECKOUT-> curr_wd, bot_wd ', curr_wd, nxtlinewd, currtype, nexttype )
          if currtype in [ 1,2 ] and nexttype in [ 3, 5 ] and currtype != nexttype:
            print('BOTTOM NEIGH IS DIFF!!')
            return ( True, nexttype, nxtlinewd, findValFeats( nxtlinewd ) )
          else:
            unsuitable_x_overlap = True

      ## if no xoverlap found in next line DONT assume we need to move to next + 1 .. sometimes the value below
      ## the key is absent and we SHOULD NOT pick value from next+1 ..hence we check if the next line has ANY
      ## content that xOVERLAPS with the current line ..if so then we break
      if unsuitable_x_overlap: 
      #if anyXfound_:
        print('Nothing found TO THE RT or BELOW ->', ref_wd_ctr)
        return False, None 

    return False, None

def neighContours( txt_, pts_, json_, conj_lt=None, conj_rt=None ):
    ## 3 neigh in either dirn
    upper_neigh, lower_neigh, left_neigh, rt_neigh = 3, 3, 3, 3

    x_vertical, y_horizontal = np.zeros((6, ( upper_neigh + lower_neigh ))), np.zeros((6, ( left_neigh + rt_neigh )))
    #TOP
    xover_upper, xover_lower, yover_prev, yover_next = dict() ,dict(), dict(), dict()
    print('NEIGHH->', txt_, pts_, conj_lt, conj_rt)

    for linectr in range( len( json_['lines'] ) ):
      line_ = json_['lines'][linectr]

      for wdctr in range( len(line_) ):
        txt1, pts1 = line_[ wdctr ]['text'], line_[ wdctr ]['pts']
        if xOverlap( txt1, pts1, 'NA', pts_, dist_=1000 ) and pts1[1] < pts_[1] and abs( pts1[1] - pts_[1]) > 10: 
          xover_upper[ pts1[1] ] = ( txt1, pts1 )
        elif xOverlap( txt1, pts1, 'NA', pts_, dist_=1000 ) and pts1[1] > pts_[1] and abs( pts1[1] - pts_[1]) > 10: 
          xover_lower[ pts1[1] ] = ( txt1, pts1 )
        elif abs( pts_[1] - pts1[1] ) < 20 and pts1[0] > pts_[2] and abs( pts1[0] - pts_[2] ) <= 900: 
          yover_next[ pts1[0] ] = ( txt1, pts1 )
          print('Adding HOR_NEXT->', ( txt1, pts1 ))
        elif abs( pts_[1] - pts1[1] ) < 20 and pts1[2] < pts_[0] and abs( pts1[2] - pts_[0] ) <= 900: 
          yover_prev[ pts1[0] ] = ( txt1, pts1 )
          print('Adding HOR_PREV->', ( txt1, pts1 ))

    if len( xover_upper ) > 0:
      rev_ = sorted( list( xover_upper.keys() ), reverse=True )
      for ctr in range( min( upper_neigh, len(rev_) ) ):
        x_vertical[ ctr ] = findWdFeats( xover_upper[ rev_[ctr] ][0] ).tolist()  

    if len( xover_lower ) > 0:
      rev_ = sorted( list( xover_lower.keys() ) )
      for ctr in range( min( lower_neigh, len(rev_) ) ):
        x_vertical[ ctr+lower_neigh ] = findWdFeats( xover_lower[ rev_[ctr] ][0] ).tolist()  

    if len( yover_prev ) > 0:
      rev_ = sorted( list( yover_prev.keys() ), reverse=True )
      for ctr in range( min( left_neigh, len(rev_) ) ):
        y_horizontal[ ctr ] = findWdFeats( yover_prev[ rev_[ctr] ][0] ).tolist()  
        print('ANACONDA-LT-HOR->', ctr, y_horizontal[ ctr ])

    if len( yover_next ) > 0:
      rev_ = sorted( list( yover_next.keys() ) )
      for ctr in range( min( rt_neigh, len(rev_) ) ):
        y_horizontal[ ctr+rt_neigh ] = findWdFeats( yover_next[ rev_[ctr] ][0] ).tolist()  
        print('ANACONDA-RT-HOR->', ctr, y_horizontal[ ctr+rt_neigh ])

    return x_vertical.tolist(), y_horizontal.tolist()

def neighContours_old( txt_, pts_, json_, conj_lt=None, conj_rt=None ):

    xover_upper, xover_lower, yover_prev, yover_next = dict() ,dict(), dict(), dict()
    print('NEIGHH->', txt_, pts_, conj_lt, conj_rt)

    for linectr in range( len( json_['lines'] ) ):
      line_ = json_['lines'][linectr]

      for wdctr in range( len(line_) ):
        txt1, pts1 = line_[ wdctr ]['text'], line_[ wdctr ]['pts']
        if xOverlap( txt1, pts1, 'NA', pts_, dist_=1500 ) and pts1[1] < pts_[1] and abs( pts1[1] - pts_[1]) > 10: 
          xover_upper[ pts1[1] ] = ( txt1, pts1 )
        elif xOverlap( txt1, pts1, 'NA', pts_, dist_=1500 ) and pts1[1] > pts_[1] and abs( pts1[1] - pts_[1]) > 10: 
          xover_lower[ pts1[1] ] = ( txt1, pts1 )
        elif abs( pts_[1] - pts1[1] ) <= 10 and pts1[0] > pts_[2] and abs( pts1[0] - pts_[2] ) <= 700: 
          yover_next[ pts1[0] ] = ( txt1, pts1 )
        elif abs( pts_[1] - pts1[1] ) <= 10 and pts1[2] < pts_[0] and abs( pts1[2] - pts_[0] ) <= 700: 
          yover_prev[ pts1[0] ] = ( txt1, pts1 )

    #findWdFeats
    respFeat_ = np.zeros((6*4)) 
    if len( xover_upper ) > 0:
      key_ = sorted( list( xover_upper.keys() ) )[-1]
      print('Nearest TOP X->', xover_upper[ key_ ], txt_, pts_ ) 
      respFeat_[0:6] = findWdFeats( xover_upper[ key_ ][0] ).tolist()

    if len( xover_lower ) > 0:
      key_ = sorted( list( xover_lower.keys() ) )[0]
      print('Nearest BOTTOM X->', xover_lower[ key_ ], txt_, pts_ ) 
      respFeat_[6:12] = findWdFeats( xover_lower[ key_ ][0] ).tolist()

    if len( yover_next ) > 0:
      key_ = sorted( list( yover_next.keys() ) )[0]
      print('Nearest RT Y->', yover_next[ key_ ], txt_, pts_ ) 
      if conj_rt is None:
        respFeat_[12:18] = findWdFeats( yover_next[ key_ ][0] ).tolist()
      else:
        respFeat_[12:18] = findWdFeats( conj_rt ).tolist()

    if len( yover_prev ) > 0:
      key_ = sorted( list( yover_prev.keys() ) )[-1]
      print('Nearest LT Y->', yover_prev[ key_ ], txt_, pts_ ) 
      if conj_lt is None:
        respFeat_[18:24] = findWdFeats( yover_prev[ key_ ][0] ).tolist()
      else:
        respFeat_[12:18] = findWdFeats( conj_lt ).tolist()

    return respFeat_.tolist()

def isNum( txt ):
    for char in txt:
        if ord(char) >= 48 and ord(char) <= 57: return True
    return False    

def processNeighbours( json_, json_raw, fileNm ):

    neighDict_txt, neighDict_num, medstore = dict(), dict(), []
    contour_arr_, pts_arr_, conjoined_neigh_ = [], [], []
    ht_, wd_ = json_['height'], json_['width']

    for linectr in range( len( json_['lines'] ) ):
      line_ = json_['lines'][linectr]
      print( 'TWIN->', line_, json_raw['lines'][linectr] )
      for wdctr in range( len(line_) ):
        #print( wdctr )
        txt1, pts1 = line_[ wdctr ]['text'], line_[ wdctr ]['pts']
        print( 'Before RAW->', txt1, pts1, line_[ wdctr ]['ids'] )
        txtarr, ptsarr = findRaw( line_[ wdctr ]['ids'], json_raw )
        print('POST RAW->', txtarr, ptsarr)
        fullText_ = txt1 
        #if True:
        #if allNum( txt ):
        for ctr in range( len(txtarr) ):

          pts_arr_.append( ptsarr[ ctr ] )
          _tmp = txtarr[ ctr ]
          if len( _tmp ) > 2 and len( _tmp.split()[0] ) <= 2 : 
            _tmp = _tmp[2:].strip()
            txt1 = txt1[2:].strip() 

          contour_arr_.append( ( _tmp, txt1 ) )
          txt1 , pts1, typeOfNeighbour = _tmp, ptsarr[ ctr ], \
                                         findNeighBour( wdctr , linectr, json_, _tmp, ptsarr[ ctr ] )

          rt_neigh, lt_neigh = None, None

          if len( txt1.split() ) > 1:
            tmp_arr_ = txt1.split()
            if len( tmp_arr_[0] ) <= 2:
              tmp_arr_ = tmp_arr_[1:]

            #if len( tmp_arr_[0] ) >= 3 and allNum( tmp_arr_[0] ):
            #  txt1 = tmp_arr_[0] 
            #  rt_neigh = ' '.join( tmp_arr_[1:] )
            #elif len( tmp_arr_[-1] ) >= 3 and allNum( tmp_arr_[-1] ):
            if len( tmp_arr_[-1] ) >= 3 and allNum( tmp_arr_[-1] ):
              txt1 = tmp_arr_[-1] 
              lt_neigh = ' '.join( tmp_arr_[:-1] )
              print('CONJOINED LEFT->', txt1)
            elif ':' in txt1 and len( txt1.split(':') ) > 1 and allNum( ''.join( txt1.split(':')[1:] ) ):
              lt_neigh = ' '.join( tmp_arr_[:-1] )
              print('CONJOINED LEFT->', txt1)

          conjoined_neigh_.append( (lt_neigh, rt_neigh) )

          if allNum( txt1 ):
            typeOfNeighForVal = findValNeighBour( wdctr , linectr, json_)
          else:
            typeOfNeighForVal = (False, None )

          print('BOJI->', txt1, typeOfNeighbour, typeOfNeighForVal )
          ## in case of Date: 12-Jan-2023 ..meaning conjoined key values, replace KEY text
          if len( typeOfNeighbour ) == 5:
            txt1 = typeOfNeighbour[-1]
            print('REPLACING txt1 with split text->', txt1)

          if ( allNum( txt1 ) and typeOfNeighForVal[0] is False ) or len( txt1 ) <= 3 or len( txt1.split() ) >= 8 \
              or ( typeOfNeighbour[0] is False and typeOfNeighForVal[0] is False ) or ',' in txt1: 
              print('IS / HAS NUM / typeOfNeighbour False ..IGNORE ->', txt1, allNum( txt1 ),\
                     typeOfNeighForVal[0], len( txt1.split() ) )
              continue
          resp_ = findWdFeats( txt1 )
          ##now add the "VALUE" feat as well .. this way we ll cluster K and V
          medstore.append( len( resp_ ) )

          if len( typeOfNeighForVal ) == 2: 
            if len( typeOfNeighbour ) == 4:
              valContour = typeOfNeighbour[-2]
              tmpTxt = valContour['text']

              if len( tmpTxt.split() ) >= 2 and allNum( tmpTxt.split()[0] ) \
                                            and not allNum( ' '.join( tmpTxt.split()[1:] ) ):
                print('IS / HAS NUM / typeOfNeighbour False ..IGNORE ->', tmpTxt, allNum( tmpTxt ),\
                     typeOfNeighForVal[0], len( tmpTxt.split() ) )
                continue

              kvresp_ = resp_.tolist() + typeOfNeighbour[-1].tolist()
            elif len( typeOfNeighbour ) == 5:
              valContour = typeOfNeighbour[-3]
              tmpTxt = valContour['text']

              if len( tmpTxt.split() ) >= 2 and allNum( tmpTxt.split()[0] ) \
                                            and not allNum( ' '.join( tmpTxt.split()[1:] ) ):
                print('IS / HAS NUM / typeOfNeighbour False ..IGNORE ->', tmpTxt, allNum( tmpTxt ),\
                     typeOfNeighForVal[0], len( tmpTxt.split() ) )
                continue

              kvresp_ = resp_.tolist() + typeOfNeighbour[-2].tolist()

            kvtext_ = txt1 + '--' + valContour['text']
            kvpts_  = ( pts1, valContour['pts'] )
            kvtxt_arr = ( txt1, valContour['text'] )

          elif len( typeOfNeighForVal ) > 2: 
            #try:
              print('INCOMING->', typeOfNeighForVal, resp_ )
              if len( typeOfNeighForVal ) == 4:
                valContour = typeOfNeighForVal[-2]

                if len( valContour['text'] ) >= 3 and allNum( valContour['text'].split()[0] ) and not\
                  allNum( ' '.join( valContour['text'].split()[1:] ) ):
                  print('MOVE MOVE')
                  continue

                kvresp_ = resp_.tolist() + typeOfNeighForVal[-1].tolist()
              elif len( typeOfNeighForVal ) == 5:
                valContour = typeOfNeighForVal[-3]

                if len( valContour['text'] ) >= 3 and allNum( valContour['text'].split()[0] ) and not\
                  allNum( ' '.join( valContour['text'].split()[1:] ) ):
                  print('MOVE MOVE')
                  continue

                kvresp_ = resp_.tolist() + typeOfNeighForVal[-2].tolist()

              kvtext_ = valContour['text'] + '--' + txt1
              kvpts_  = ( valContour['pts'], pts1 )
              kvtxt_arr = ( txt1, valContour['text'] )

            #except:
            #  print('Somme issue->')

          if kvtext_ in neighDict_num:
            neighDict_num[ kvtext_+'_0' ] = ( kvresp_, kvpts_, kvtxt_arr )
          else: 
            neighDict_num[ kvtext_ ] = ( kvresp_, kvpts_, kvtxt_arr )

          print('PUZZLE->', kvtext_, ( kvresp_, kvpts_ ))


    print('MEDICI->', np.median( np.asarray( medstore ) ) )
    cluster_input_txt, simple_input, cluster_input_num = dict(), dict(), dict()

    ## create input for XFORMER
    finalInp_ = []
    for ctr in range( len(contour_arr_) ):#, pts_arr_
      if len( contour_arr_[ ctr ][0] ) == 0: continue
      conjoined_lt, conjoined_rt = conjoined_neigh_[ ctr ]

      ( cont_, fullTxt ), pts_, (neigh_vert, neigh_hor) = contour_arr_[ ctr ], pts_arr_[ ctr ], \
                          neighContours( contour_arr_[ ctr ],pts_arr_[ ctr ], json_, conjoined_lt, conjoined_rt )
      marked_ = False
      #print('Getting PUTIN->',cont_)  
      for key, ( _, ptsarr, txtarr ) in neighDict_num.items():
        keypts, valpts = ptsarr
        keytxt, valtxt = txtarr
        #print('BUTLER->', keypts, valpts, keytxt, valtxt)
        if pts_ == keypts and abs( valpts[1] - keypts[1] ) < 20 and \
          ( valpts[0] > keypts[0] or ( valpts == keypts and keytxt != valtxt ) ) and \
          not( ':' in cont_ and len( cont_.split(':')[-1] ) >= 2 ):
          pts_ = [ pts_[0]/wd_, pts_[1]/ht_, pts_[2]/wd_, pts_[3]/ht_ ] 
          print('MADNESS->', cont_, len( cont_.split(':')[-1] ))
          finalInp_.append( ( cont_, pts_, 'KEY-VALUE-RIGHT', neigh_vert, neigh_hor ) )
          marked_ = True

        elif pts_ == keypts and xOverlap( 'NA', valpts, 'NA', keypts ) and conjoined_lt is None\
          and not( ':' in cont_ and len( cont_.split(':') ) >= 2 ) :
          pts_ = [ pts_[0]/wd_, pts_[1]/ht_, pts_[2]/wd_, pts_[3]/ht_ ] 
          finalInp_.append( ( cont_, pts_, 'KEY-VALUE-BELOW', neigh_vert, neigh_hor ) )
          marked_ = True

        elif pts_ == valpts and abs( valpts[1] - keypts[1] ) < 20 and valpts[0] > keypts[0]:
          pts_ = [ pts_[0]/wd_, pts_[1]/ht_, pts_[2]/wd_, pts_[3]/ht_ ] 
          finalInp_.append( ( cont_, pts_, 'VALUE-KEY-LEFT', neigh_vert, neigh_hor ) )
          marked_ = True

        elif pts_ == valpts and xOverlap( 'NA', valpts, 'NA', keypts ) and \
          not( ':' in cont_ and len( cont_.split(':')[-1] ) >= 2 ):
          pts_ = [ pts_[0]/wd_, pts_[1]/ht_, pts_[2]/wd_, pts_[3]/ht_ ] 
          finalInp_.append( ( cont_, pts_, 'VALUE-KEY-TOP', neigh_vert, neigh_hor ) )
          marked_ = True

      if marked_ is False: 
          pts_ = [ pts_[0]/wd_, pts_[1]/ht_, pts_[2]/wd_, pts_[3]/ht_ ] 
          ## check if words like "Invoice 123123" slipped away
          txt_ = fullTxt
          #print('PRE_->', txt_.split(':'), txt_.split(), allNum( txt_.split()[-1] ), allNum( txt_.split()[0] ),\
          #                               neigh_hor )
          #print('PREQUALIFIER->', len( txt_.split(':')[-1] ) > 1, allNum( txt_.split(':')[-1], 'SPIKE' ),\
          #                        len( txt_.split(':')[0] ), allNum( txt_.split(':')[0] ) )

          if ( ( ':' in txt_ and len( txt_.split(':')[-1] ) > 1 and allNum( txt_.split(':')[-1], 'SPIKE' )\
            and len( txt_.split(':')[0] ) >= 3 and not allNum( txt_.split(':')[0] ) ) or \
            ( len( txt_.split() ) > 1 and len( txt_.split()[-1] ) >= 3 and allNum( txt_.split()[-1], 'SPIKE' )\
            and len( txt_.split()[0] ) >= 3 and not allNum( txt_.split()[0] ) ) ) and len( txt_.split() ) <= 5:
            if ':' in txt_:
                keyTxt, valTxt = ' '.join( txt_.split(':')[:-1] ), txt_.split(':')[-1] 
            else:
                keyTxt, valTxt = ' '.join( txt_.split()[:-1] ), txt_.split()[-1] 

            keypts, valpts = [ pts_[0], pts_[1], \
                                      pts_[0] + ( pts_[2] - pts_[0] )*( len(keyTxt)/ len(keyTxt+valTxt) ), pts_[3] ],\
                                 [ pts_[0] + ( pts_[2] - pts_[0] )*( len(keyTxt)/ len(keyTxt+valTxt) ), pts_[1],\
                                   pts_[2], pts_[3] ]
            keyF, valF = findWdFeats( keyTxt ), findWdFeats( valTxt )
            ## for KEY, neigh_hor 4th elem must be mod and for VAL neigh_hor 3rd element
            neo_hor = [ neigh_hor[0], neigh_hor[1], neigh_hor[2], valF.tolist(), neigh_hor[4], neigh_hor[5] ]
            finalInp_.append( ( keyTxt, keypts, 'KEY-VALUE-RIGHT', neigh_vert, neo_hor ) )

            neo_hor1 = [ neigh_hor[0], neigh_hor[1], keyF.tolist(), neigh_hor[3], neigh_hor[4], neigh_hor[5] ]
            finalInp_.append( ( valTxt, valpts, 'VALUE-KEY-LEFT', neigh_vert, neo_hor1 ) )

            print('BOOMER->', finalInp_[-2], finalInp_[-1] )

          elif len( txt_.split() ) >= 2 and len( txt_.split()[0] ) >= 3 and allNum( txt_.split()[0] ) and\
            not allNum( ' '.join( txt_.split()[1:] ), 'CHECK' ):
            valTxt, keyTxt = txt_.split()[0], ' '.join( txt_.split()[1:] )

            valpts, keypts = [ pts_[0], pts_[1], \
                                      pts_[0] + ( pts_[2] - pts_[0] )*( len(valTxt)/ len(keyTxt+valTxt) ), pts_[3] ],\
                                 [ pts_[0] + ( pts_[2] - pts_[0] )*( len(valTxt)/ len(keyTxt+valTxt) ), pts_[1],\
                                   pts_[2], pts_[3] ]

            keyF, valF = findWdFeats( keyTxt ), findWdFeats( valTxt )
            ## for KEY, neigh_hor 4th elem must be mod and for VAL neigh_hor 3rd element
            #neo_hor = [ neigh_hor[0], neigh_hor[1], valF.tolist(), neigh_hor[3], neigh_hor[4], neigh_hor[5] ]
            finalInp_.append( ( keyTxt, keypts, 'KEY-VALUE-RIGHT', neigh_vert, neigh_hor ) )

            #neo_hor1 = [ neigh_hor[0], neigh_hor[1], neigh_hor[2], keyF.tolist(), neigh_hor[4], neigh_hor[5] ]
            #neo_hor1 = neo_hor 
            finalInp_.append( ( valTxt, valpts, 'VALUE-KEY-LEFT', neigh_vert, neigh_hor ) )

            print('BOOMER VAL FIRST->', finalInp_[-2], finalInp_[-1] )

          else:
            #print('CYCLOPS->', txt_.split(), len( txt_.split()[0] ) >= 3 , allNum( txt_.split()[0] )\
            #  , allNum( ' '.join( txt_.split()[1:] ), 'CHECK' ) )
            finalInp_.append( ( cont_, pts_, 'IRR-NA', neigh_vert, neigh_hor ) )

    print('TRENCH WARFARE', finalInp_)
    resInp_ = []

    for elem in finalInp_:
      if elem[2] == 'IRR-NA':
        changed_ = False

        for inner in finalInp_:
          if inner[1][0] > elem[1][2] and abs( elem[1][1] - inner[1][1] ) <= 0.004 and \
            abs( elem[1][-1] - inner[1][-1] ) <= 0.004 and inner[2] == 'VALUE-KEY-LEFT':
            elem = ( elem[0], elem[1], 'KEY-VALUE-RIGHT', elem[3], elem[4] )
            print('Changed->', elem )
            changed_ = True
            elem[4][3] = findWdFeats( inner[0] ).tolist()
            resInp_.append( elem )
            break

        if changed_ is False: resInp_.append( elem )

      else:
        if elem[2] == 'VALUE-KEY-LEFT':
          hor_ = elem[-1]
          if np.sum( np.asarray( hor_[2] ) ) == 0:
            for local_ in finalInp_:
              if local_[1][2] < elem[1][0] and abs( elem[1][1] - local_[1][1] ) <= 0.004 and \
                abs( elem[1][-1] - local_[1][-1] ) <= 0.004:
                hor_[2] = findWdFeats( local_[0] ).tolist()            

        elif elem[2] == 'KEY-VALUE-RIGHT':
          hor_ = elem[-1]
          if np.sum( np.asarray( hor_[3] ) ) == 0:
            for local_ in finalInp_:
              if local_[1][0] > elem[1][2] and abs( elem[1][1] - local_[1][1] ) <= 0.004 and \
                abs( elem[1][-1] - local_[1][-1] ) <= 0.004:
                hor_[3] = findWdFeats( local_[0] ).tolist()            

        resInp_.append( elem )

    #print('EFFING RESULT->', resInp_[0])
    key_tuples_ = []
    for elem in resInp_:
        if ( 'KEY' in elem[2][:4] or 'IRR' in elem[2][:4] ) and isNum( elem[0] ) is False:
            print( elem[0] )
            print( elem[1] )
            key_tuples_.append( ( elem[0],  elem[1] ) )

    return key_tuples_        

    #cv2.imwrite( './RES/contoured_'+fileNm, imgFile_ )  

if __name__ == '__main__':

    src_0 = '/home/ubuntu/ROHITH/S3_TASK/S3_DOWNLOADS_NEW/output/home/ubuntu/ABHIJEET/INVOICES/REQUORDIT/DEV/ALL_OCR_OUTPUT/'
    src_raw = '/home/ubuntu/ROHITH/S3_TASK/S3_DOWNLOADS_NEW/raw/home/ubuntu/ABHIJEET/INVOICES/REQUORDIT/DEV/ALL_OCR_OUTPUT_ORIGINAL/'

    fnm_ = sys.argv[1]
    folder_ = sys.argv[2]

    import os
    ll = os.listdir( src_0 )
    if folder_ != 'NA':
      img_list_ = os.listdir( folder_ )
    else:
      img_list_ = [ fnm_+'.jpg' ] 

    #for fnm in img_list_:
    for fnm in ll:
      if 'output' in fnm or 'input' in fnm or 'global' in fnm: continue

      ''' 
      for fileNm in img_list_: 
        if '.jpg' not in fileNm: continue
        fnm_ = fileNm.split('.jpg')[0] 
        if fnm_ in fnm:
          print('PROCESSING-> IMG ', fileNm, ' JSON ',fnm) 
      ''' 
      if fnm_ not in fnm: continue
      print('PROCESSING->', src_0 + fnm)
      if True:
        try:
          with open( src_raw + fnm, 'r' ) as fp:
            json_raw = json.load( fp )

          with open( src_0 + fnm, 'r' ) as fp:
            json_ = json.load( fp )

          #imgFile_ = cv2.imread( './safe/' + fileNm )
          processNeighbours( json_, json_raw, fnm ) 
          #processNeighbours( json_, json_raw, imgFile_, fileNm ) 
        except:
          print('FAILED for ->', fnm, traceback.format_exc())

CODE_SNIP-> from openpyxl import load_workbook

def find_table_bounds(sheet):
    max_row = sheet.max_row
    max_col = sheet.max_column

    # Initialize variables to track the bounds
    start_row ,end_row ,start_col ,end_col = None, None, None, None

    # Iterate over rows to find the start and end rows
    for row in range(1, max_row + 1):
        if all(sheet.cell(row=row, column=col).value is None for col in range(1, max_col + 1)):
            if start_row is None:
                continue  # Skip empty rows before the table
            else:
                end_row = row - 1
                break
        elif start_row is None:
            start_row = row

    # Iterate over columns to find the start and end columns
    for col in range(1, max_col + 1):
        #for row in range(start_row, end_row):
        #    print('ROW NUM->', col, ' VALUE: ', sheet.cell(row=row, column=col).value )

        if all(sheet.cell(row=row, column=col).value is None for row in range(start_row, end_row)):
            if start_col is None:
                continue  # Skip empty columns before the table
            else:
                end_col = col - 1
                break
        elif start_col is None:
            start_col = col

    return start_row, end_row, start_col, end_col

# Example usage
workbook = load_workbook( './DATA/Mark Training samples % .xlsx' )
sheet = workbook.active

start_row, end_row, start_col, end_col = find_table_bounds(sheet)
print("Table bounds: Start Row =", start_row, ", End Row =", end_row, ", Start Column =", start_col, ", End Column =", end_col, sheet)

CODE_SNIP-> import numpy as np
from sklearn.decomposition import PCA
X = np.array([  [1, 1, 1], \
                [2, 1, 2], \
                [10, 1, 2], \
                [21, 1, 2], \
                [30, 1, 2] \
            ])

pca = PCA()
pca.fit(X)
import time
start_ = time.time()
print(pca.explained_variance_ratio_)
print(pca.n_components_)

print(pca.components_)

CODE_SNIP-> import os, ast, time, math, json

with open( 'prompts.json', 'r' ) as fp:
    prompts_dict_ = json.load( fp )

with open( 'openai_config.json', 'r' ) as fp:
    cfg_ = json.load( fp )

from openai import OpenAI

client = OpenAI(
    api_key=cfg_["KEY"],
)

def returnDocSummary( data_frame ):
        try:
            chat_completion = client.chat.completions.create(
                messages=[
                    {
                        "role": "user",
                        "content": prompts_dict_["SUMMARIZE_SAMPLE"] + data_frame
                    }
                ],
                model=cfg_["BASE_MODEL"],
            )

            kk = ( chat_completion.choices[0].message.content )

            return kk
        except:
            return 'NO RESPONSE'

def augmentHeaderInformation( header_info_ ):

        try:
            chat_completion = client.chat.completions.create(
                messages=[
                    {
                        "role": "user",
                        "content": prompts_dict_["SUMMARIZE_HEADER_VALUES"] + data_frame
                    }
                ],
                model=groq_cfg_["BASE_MODEL"],
            )

            kk = ( chat_completion.choices[0].message.content )

            return kk
        except:
            return 'NO RESPONSE'

CODE_SNIP-> import os, ast, time, math, json

with open( 'prompts.json', 'r' ) as fp:
    prompts_dict_ = json.load( fp )

with open( 'groq_config.json', 'r' ) as fp:
    groq_cfg_ = json.load( fp )

from groq import Groq

client = Groq(
    api_key=groq_cfg_["GROQ_KEY"],
)

def returnDocSummary( data_frame, high_variance_cols_ ):

        if len( high_variance_cols_ ) > 0:
            content_ = prompts_dict_["SUMMARIZE_SAMPLE_FOCUS"] + ' , '.join( high_variance_cols_ ).strip() + '\n'
        else:
            content_ = prompts_dict_["SUMMARIZE_SAMPLE"]

        print( 'GETTIN IN->', content_ )

        try:
            chat_completion = client.chat.completions.create(
                messages=[
                    {
                        "role": "user",
                        "content": content_ + data_frame
                    }
                ],
                model=groq_cfg_["GROQ_BASE_MODEL"],
            )

            kk = ( chat_completion.choices[0].message.content )

            return kk
        except:
            return 'NO RESPONSE'

def augmentHeaderInformation( header_info_ ):
        print('INCOMING HDR->', header_info_)
        try:
            chat_completion = client.chat.completions.create(
                messages=[
                    {
                        "role": "user",
                        "content": prompts_dict_["SUMMARIZE_HEADER_VALUES"] + header_info_
                    }
                ],
                model=groq_cfg_["GROQ_BASE_MODEL"],
            )

            kk = ( chat_completion.choices[0].message.content )

            return kk
        except:
            return 'NO RESPONSE'

if __name__ == '__main__':

CODE_SNIP-> import os, ast, time, math, json

with open( 'prompts.json', 'r' ) as fp:
    prompts_dict_ = json.load( fp )

with open( 'groq_config.json', 'r' ) as fp:
    groq_cfg_ = json.load( fp )

from groq import Groq

client = Groq(
    api_key=groq_cfg_["GROQ_KEY"],
)

def returnDocSummary( data_frame, high_variance_cols_ ):

        if len( high_variance_cols_ ) > 0:
            content_ = prompts_dict_["SUMMARIZE_SAMPLE_FOCUS"] + ' , '.join( high_variance_cols_ ).strip() + '\n'
        else:
            content_ = prompts_dict_["SUMMARIZE_SAMPLE"]

        print( 'GETTIN IN->', content_ )

        try:
            chat_completion = client.chat.completions.create(
                messages=[
                    {
                        "role": "user",
                        "content": content_ + data_frame
                    }
                ],
                model=groq_cfg_["GROQ_BASE_MODEL"],
            )

            kk = ( chat_completion.choices[0].message.content )

            return kk
        except:
            return 'NO RESPONSE'

def augmentHeaderInformation( header_info_ ):
        print('INCOMING HDR->', header_info_)
        try:
            chat_completion = client.chat.completions.create(
                messages=[
                    {
                        "role": "user",
                        "content": prompts_dict_["SUMMARIZE_HEADER_VALUES"] + header_info_
                    }
                ],
                model=groq_cfg_["GROQ_BASE_MODEL"],
            )

            kk = ( chat_completion.choices[0].message.content )

            return kk
        except:
            return 'NO RESPONSE'


CODE_SNIP-> import openpyxl
import groq_first_pass
from openpyxl.utils import column_index_from_string
import time
finalJson_ = dict()

def read_excel_file(file_path):
    # Load the workbook
    workbook = openpyxl.load_workbook( file_path, read_only=True )
    # Get the specified sheet in the workbook
    localD = dict()

    for sheet_name in workbook.sheetnames:
        sheet = workbook[sheet_name]
        time.sleep( 1 )
        print('Iterating over sheet->', file_path, sheet_name)

        num_rows_to_consider_ , frame_ = 4, ''
        try:
            for rowidx, row in enumerate( sheet.iter_rows(values_only=True) ):

                if rowidx > num_rows_to_consider_: break
                for cell in row:
                    frame_ += str(cell) + '\t'

                frame_ += '\n'

            print('Sending to LLM for summary->', frame_)

            summary_ = groq_first_pass.returnLLMResponse( frame_ )
            ## append file name, sheet name
            localD[ sheet_name ] = file_path.split('/')[-1] + ' ' + sheet_name + ' ' + summary_
        except:
            print( 'EXCPN-> '+file_path + ' ' + sheet_name + ' ' + traceback.format_exc() )

    finalJson_[ file_path.split('/')[-1] ] = localD


if __name__ == "__main__":
    # Example usage:
    import os, json
    file_ll_ = os.listdir( './DATA' )

    for file_path in file_ll_:
        print('ITERATING ->', file_path)
        read_excel_file( './DATA/' + file_path )

    with open( 'SpreadSheetSummary.json', 'a' ) as fp:

CODE_SNIP-> import openpyxl
import groq_first_pass
from openpyxl.utils import column_index_from_string
import time
finalJson_ = dict()

def read_excel_file(file_path):
    # Load the workbook
    workbook = openpyxl.load_workbook( file_path, read_only=True )
    # Get the specified sheet in the workbook
    localD = dict()

    for sheet_name in workbook.sheetnames:
        sheet = workbook[sheet_name]
        time.sleep( 1 )
        print('Iterating over sheet->', file_path, sheet_name)

        num_rows_to_consider_ , frame_ = 4, ''
        try:
            for rowidx, row in enumerate( sheet.iter_rows(values_only=True) ):

                if rowidx > num_rows_to_consider_: break
                for cell in row:
                    frame_ += str(cell) + '\t'

                frame_ += '\n'

            print('Sending to LLM for summary->', frame_)

            summary_ = groq_first_pass.returnLLMResponse( frame_ )
            ## append file name, sheet name
            localD[ sheet_name ] = file_path.split('/')[-1] + ' ' + sheet_name + ' ' + summary_
        except:
            print( 'EXCPN-> '+file_path + ' ' + sheet_name + ' ' + traceback.format_exc() )

    finalJson_[ file_path.split('/')[-1] ] = localD


if __name__ == "__main__":
    # Example usage:
    import os, json
    file_ll_ = os.listdir( './DATA' )

    for file_path in file_ll_:
        print('ITERATING ->', file_path)
        read_excel_file( './DATA/' + file_path )


CODE_SNIP-> import createJsonFeats as cj
from scipy.spatial import distance

#a, b = cj.returnEmbed(""), cj.returnEmbed("")
a, b = cj.returnEmbed('''Based on the table, it appears to be a report or dataset with various columns. Here are some possible purposes: 1. **Financial Report**: The table could be a financial report, with columns for input values (e.g., STR), calculations (e.g., INC, DEC), and output values (e.g., OUT). The rows might represent different accounts, transactions, or periods. 2. **Data Analysis**: The table might be a dataset for data analysis, where each row represents a single observation or sample, and the columns contain measured variables or features. The "None" values could indicate missing data. 3. **Geographic Mapping**: The column names like "TRIANGULATION" and "header" suggest a geographic or mapping-related purpose. The table might be used to draw boundaries, create maps, or perform spatial analysis. 4. **Database Backup**: The table could be a backup or extract of a larger database, containing specific columns and values. The "None" values might indicate that certain fields are not populated. 5. **Log File**: The table might be a log file or a record of events, with each row representing a specific event or transaction. The columns could contain metadata about the events, such as timestamps, IDs, and status information. Without more context or information about the table's origin and purpose, it's difficult to pinpoint a specific purpose. If you have more details or context, I may be able to provide a more accurate answer.'''), cj.returnEmbed("What are the reports that are associated with time trials for table detection and accuracy")


CODE_SNIP-> import createJsonFeats # returnEmbed
import os, json, sys, traceback, db_utils

with open( 'SpreadSheetSummary.json', 'r' ) as fp:
    js_ = json.load( fp )

cnt_ = 0

def addToDB():
    for fnm, sheets in js_.items():
        for sheetname, txt in sheets.items():
            cnt_ += 1
            emb_ = createJsonFeats.returnEmbed( txt )
            dd_ = { 'text': txt, 'docSignature': emb_, 'docID': cnt_ }

            db_utils.insertNewSignature( dd_ )

if __name__ == "__main__":

CODE_SNIP-> import createJsonFeats # returnEmbed
import os, json, sys, traceback, db_utils

with open( 'SpreadSheetSummary.json', 'r' ) as fp:
    js_ = json.load( fp )

cnt_ = 0

def addToDB():
    for fnm, sheets in js_.items():
        for sheetname, txt in sheets.items():
            cnt_ += 1
            emb_ = createJsonFeats.returnEmbed( txt )
            dd_ = { 'text': txt, 'docSignature': emb_, 'docID': cnt_ }

            db_utils.insertNewSignature( dd_ )


CODE_SNIP-> import json, math, sys, traceback, copy, multiprocessing, os
from dateutil import parser
import numpy as np
import openpyxl
from openpyxl.utils import column_index_from_string
import time, random, datetime
import pandas as pd
from sklearn.decomposition import PCA

import query_llama3_via_groq as llama3
import query_gpt_via_groq as openai
import createJsonFeats
import db_utils

def is_date( input_str):
        ## first check for INT and FLOAT since parser foolishly accepts ints
        try:
            _ = int( input_str )
            return None
        except:
            pass

        try:
            _ = float( input_str )
            return None
        except:
            pass

        try:
            return parser.parse(input_str)
        except ValueError:
            return None

def process( colNum, sheet, tbl ):
        dt_counts_ = []

        for rw in range( tbl['START_ROW'], tbl['END_ROW'] ):
                dtVal_ = is_date( str( sheet.cell(row=rw, column=colNum).value ) )

                if dtVal_ is not None : 
                    dt_counts_.append( dtVal_ ) 

        if len( dt_counts_ ) >= ( tbl['END_ROW'] - tbl['START_ROW'] )/2: 
                ## defensive chk to ensure dt counts are high
                print('Dt Col found !', colNum)
                ## sort the values to get range
                sorted_dates_ = sorted( dt_counts_ )
                print('Dt range->', sorted_dates_[0], sorted_dates_[-1] )

                return ( True, sorted_dates_[0].strftime('%B %d, %Y'), sorted_dates_[-1].strftime('%B %d, %Y') )

        return ( False, None, None )

class GenerateXLMetaInfo:
    def __init__(self, file_path, llm='LLAMA'):
        """
        Initialize the GenerateXmlMetaInfo class with the XML file.

        Parameters:
        - xml_file (str): The path to the XML file.
        """
        self.file_path = file_path
        self.masterInfo_ = dict()
        self.llm_framework_ = llm
        self.sheet = None
        self.sklearn_pca_object_ = PCA()
        self.add_ai_summary_to_embedding_ = True
        self.chunk_size_ = 500 ## approx 1024 tokens

        self.sz_of_phrase_, self.unique_str_thresh_, self.number_str_thresh_ = 5, 0.5, 0.4
        self.pca_var_min_contrib, self.feature_contribution_per_thresh_ = 0.5, 0.3
        self.max_tables_per_sheet_ = 10 ## at times a single sheet can have multiple tables
        self.num_rows_to_consider_, self.col_thresh_, self.minElemsInTable, self.max_rows_variance = 4, 0.8, 6, 100
        self.default_max_col_ , self.default_max_row_, self.max_cols_for_review_, \
                self.min_num_distinct_values_, self.max_elements_for_summary_ = 50, 50, 10, 3, 15

        if llm == 'OPENAI':
            self.query_fn_ = openai
        else:
            ## default , llama3 inferred via groq
            self.query_fn_ = llama3

    def find_bounds( self, sheet, max_row, max_col, start_row ,end_row ,start_col ,end_col ):

        # Iterate over rows to find the start and end rows
        start_row_idx_ = 1 if start_row is None else start_row
        start_col_idx_ = 1 if start_col is None else start_col

        ## need to add 2 to max rw and col since max_row of sheet returns the last row with data
        ## and range will stop at max_row - 1 
        for row in range( start_row_idx_ , max_row + 2):
            if all( self.sheet.cell(row=row, column=col).value is None for col in range(1, max_col + 2)):
                if start_row is None:
                    continue  # Skip empty rows before the table
                else:
                    end_row = row - 1
                    break
            elif start_row is None:
                start_row = row

        # Iterate over columns to find the start and end columns
        for col in range( start_col_idx_, max_col + 2):
            #for row in range(start_row, end_row):
            #    print('ROW NUM->', col, ' VALUE: ', sheet.cell(row=row, column=col).value )
            if end_row is None: continue

            if all( self.sheet.cell(row=row, column=col).value is None for row in range(start_row, end_row)):
                if start_col is None:
                    continue  # Skip empty columns before the table
                else:
                    end_col = col - 1
                    break
            elif start_col is None:
                start_col = col


        #print('Found tables between-> start_row, end_row, start_col, end_col = ',\
        #        start_row, end_row, start_col, end_col )

        return start_row, end_row, start_col, end_col

    def is_hdr_row_format( self, tbl_bound, sheet ):

        num_str_cols_ = 0
        for col_ctr in range( tbl_bound['START_COL'], tbl_bound['END_COL'] ):
            if type( self.sheet.cell(row=tbl_bound['START_ROW'], column=col_ctr).value ) == str:
                num_str_cols_ += 1

        if num_str_cols_ < ( tbl_bound['END_COL'] - tbl_bound['START_COL'] ): return False

        return True

    def find_tables(self, sheet):
        ## NOTE -> sheet.max_row and sheet.max_column is NOT WORKING !! NEED TO FIX
        ## default is stop gap
        max_row = sheet.max_row if sheet.max_row is not None else self.default_max_row_
        max_col = sheet.max_column if sheet.max_column is not None else self.default_max_col_
        table_bounds_ = []

        print('KKR->', max_row, max_col)
        timer_ = time.time()
        # Initialize variables to track the bounds
        start_row ,end_row ,start_col ,end_col = None, None, 1, sheet.max_column

        ## do a first pass to find the first table
        start_row, end_row, start_col, end_col = self.find_bounds( sheet, max_row, max_col, start_row ,\
                                                                         end_row ,start_col ,end_col )

        print('Time taken->', time.time() - timer_, start_row, end_row, start_col, end_col)    
        init_end_col = copy.copy( end_col )

        table_bounds_.append( { 'START_ROW': start_row, 'END_ROW': end_row,\
                                    'START_COL': start_col, 'END_COL': end_col } ) 

        ## now iterate from end_row to max_row to find all tables row wise
        while end_row is not None:
            end_row += 2 ## increment by 2 since we need to look ahead and see if any more tables exist !
            ##              if u increment by 1 then u will end up on the same blank line that stopped the prev tbl  
            ## start_row is assigned the value of end_row from above and end_row is made None
            if end_row >= max_row: break

            #print('DUM ROW->', end_row)
            start_row, end_row, start_col, end_col = self.find_bounds( sheet, max_row, max_col, end_row ,\
                                                                             None , None , None )

            if ( start_col is None or end_col is None ) or \
                    ( abs( start_row - end_row )*abs( start_col - end_col ) ) <= self.minElemsInTable: continue    

            table_bounds_.append( { 'START_ROW': start_row, 'END_ROW': end_row,\
                                    'START_COL': start_col, 'END_COL': end_col } ) 

        ## now iterate from end_col to max_col to find all tables cols wise
        while init_end_col is not None:
            init_end_col += 2 ## increment by 1 since we need to look ahead and see if any more tables exist !
            ## start_row is assigned the value of end_row from above and end_row is made None
            if init_end_col >= max_col: break

            #print('DUM COL->', init_end_col)
            start_row, end_row, start_col, end_col = self.find_bounds( sheet, max_row, max_col, None ,\
                                                                             None , init_end_col , None )

            if ( start_col >= end_col ): continue    

            table_bounds_.append( { 'START_ROW': start_row, 'END_ROW': end_row,\
                                    'START_COL': start_col, 'END_COL': end_col } ) 

        ## init star and end col to min and max
        for tab in table_bounds_: tab['START_COL'] = 1; tab['END_COL'] = max_col;
        ## remove dupes
        tmp_, dupe = [], set()

        for idx1, tab1 in enumerate( table_bounds_ ):
            for idx2, tab2 in enumerate( table_bounds_ ):
                if idx1 <= idx2: continue
                if tab2['START_ROW'] >= tab1['START_ROW'] and tab2['END_ROW'] <= tab1['END_ROW']:
                    dupe.add( idx2 )

        for idx, tab in enumerate( table_bounds_ ):
            if idx not in list( dupe ):
                tmp_.append( tab )

        ## blend tables - in case the rows are FPs
        final_resp_ = []
        if len( tmp_ ) > 1:
            last_tbl_ = tmp_[0]
            final_resp_.append( last_tbl_ )
            ## check if the first row is not all STR
            for ctr in range( 1, len( tmp_ ) ):
                if self.is_hdr_row_format( tmp_[ctr], sheet ) == False:
                    ## blend with the last table
                    final_resp_[-1]['END_ROW'] = tmp_[ctr]['END_ROW']
                else:
                    final_resp_.append( table_bounds_ )
        else:
            final_resp_ = tmp_

        return final_resp_[ : min( self.max_tables_per_sheet_, len( final_resp_ ) ) ]

    def findDateRange( self, tbl ):

        colRange_ = list( range( tbl['START_COL'], tbl['END_COL'] ) )

        for col in colRange_:
            ## process was taken out of the class only because multi processing map refused to pickle
            ## a method that was part of the class ..and it proved way slower ..so parallels been removed..lol
            results = process(col, self.sheet, tbl)
            if results[0] is True:
                    return str( results[1] ) +' To '+ str( results[2] )

        return (None, None)

    def findHeaderInfo(self, tbl):
        """
        Find header information from the XL file.
        take the first 2 rows and then to be on the safe side also take the 
        first 2 columns ( in case the col headers are just numbers / % etc and the row contain item name in the first col )
        send it to the LLM for a summary
        ALSO there's no need to take all ROWS and COLS .. some 10-15 elements are more than enough but can be adjustedfor domains that need more
        """

        hdr_row_start_ = self.findHdrRow( tbl )
        row_starter_ = tbl['START_ROW'] if hdr_row_start_ is None else hdr_row_start_

        col_frame_ = ''

        for rw in range( row_starter_ , min( row_starter_ + self.num_rows_to_consider_, tbl['END_ROW'] ) ):
            for col in range( tbl['START_COL'], min( self.max_elements_for_summary_, tbl['END_COL'] + 1 ) ):
                col_frame_ += '\t' + str( self.sheet.cell(row=rw, column=col).value )

            col_frame_ += '\n'

        return col_frame_

    def findHighVarianceColumns(self, start_hdr_row_, sheet, tbl ):
        '''
        iterate through columns that have numeric values and figure out the more important columns
        num of rows - we can restrict it to lets say 1k rows ..should suffice 
        '''
        numeric_frame_, high_var_indices_, hdr_col_names_ = dict(), set(), []
        end_row_ =  min( tbl['START_ROW'] + self.max_rows_variance , tbl['END_ROW'] + 1 )
        ## add 1 to the start row since we dont want to include the header value
        start_row_ = ( tbl['START_ROW'] if start_hdr_row_ is None else start_hdr_row_ ) + 1 

        print('BIGGIE-> start_row_, end_row_ = ', start_row_, end_row_)

        for col_ctr in range( tbl['START_COL'], tbl['END_COL']+1 ):
                hdr_col_names_.append( str( self.sheet.cell(row=start_row_-1, column=col_ctr).value ) )

        try:
            for col_ctr in range( tbl['START_COL'], tbl['END_COL']+1 ):
                col_arr_ = [ 'NA' for x in range( ( end_row_ - start_row_ ) + 1 ) ]

                for idx, row_ctr in enumerate( range( start_row_, end_row_ ) ):
                    col_arr_[ idx ] = str( self.sheet.cell(row=row_ctr, column=col_ctr).value )

                ## standardize the column since PCA better be done on std values
                col_set_ = set( col_arr_ )
                ## convert the variables into unique IDs
                uid = [ list( col_set_ ).index( x ) for x in col_arr_ ]
                max_uid_ = np.max( uid )
                ## normalize the int values
                numeric_frame_[ col_ctr ] = [ x/max_uid_ for x in uid ]

            if len( numeric_frame_.keys() ) > 0:
                ## now transpose the contents of the frame since we want it to retain the shape of a column
                transposed_ = np.transpose( np.asarray( list( numeric_frame_.values() ) ) )
                #print('The val of transposed_->', transposed_)
                ## perform PCA and pick the most high variance columns
                ## the number of components to be picked will be decided by the thresh self.percent_pca_var_
                self.sklearn_pca_object_.fit( transposed_ )
                ## components_loading_ will give you principal component wise contribution of the features
                components_loading_ = self.sklearn_pca_object_.components_
                ## only consider those components that contribute to 90% or whatever threshold level of variance
                relevant_loading_ = components_loading_[0] \
                                    if self.sklearn_pca_object_.explained_variance_ratio_[0] > self.pca_var_min_contrib \
                                    else []

                #print('LOADING AND REL_LOADING->', components_loading_, relevant_loading_)
                key_list_ = list( numeric_frame_.keys() )

                for feat_idx, feat_contribution in enumerate( relevant_loading_ ):
                        if feat_contribution >= self.feature_contribution_per_thresh_: 

                            high_var_indices_.add( hdr_col_names_[ key_list_[ feat_idx ] ] )

                            #print('Adding ', hdr_col_names_[ key_list_[ feat_idx ] ],' As a high variance col')
        except:
            pass

        return list( high_var_indices_ ), hdr_col_names_


    def returnSummary(self, tbl ):
        '''
        take the first few rows to try and generate a coherent summary for the type of the data present
        i am also considering transposing the first few rows to see how different the summary looks
        ALSO maybe limiting the number of columns makes sense
        '''
        frame_, transposed_frame_, start_hdr_row_ = '', '', self.findHdrRow( tbl )

        time_ = time.time()
        high_variance_cols_, hdr_col_names_ = self.findHighVarianceColumns( start_hdr_row_, self.sheet, tbl )
        print('Time taken to find high var cols ->', time.time() - time_)
        print('AND THEY ARE->', high_variance_cols_)

        frame_num_contours_, transposed_frame_contours_ = 0, 0
        ## NATURAL order -> left to right, top to bottom
        for row_ctr in range( tbl['START_ROW'] if start_hdr_row_ is None else start_hdr_row_\
                              , min( tbl['START_ROW']+self.num_rows_to_consider_ , tbl['END_ROW']+1 ) ):

            for col_ctr in range( tbl['START_COL'], min( self.max_elements_for_summary_, tbl['END_COL']+1 ) ):

                frame_ += '\t' + str( self.sheet.cell(row=row_ctr, column=col_ctr).value )
                frame_num_contours_ += 1

            frame_ += '\n'

        return frame_, high_variance_cols_, list( set(hdr_col_names_) )

    def findHdrRow( self, tbl ):

        total_cols_ = tbl['END_COL'] - tbl['START_COL']

        for row_ctr in range( tbl['START_ROW'], \
                              min( tbl['START_ROW']+self.num_rows_to_consider_ , tbl['END_ROW']+1 ) ):
            num_non_blank_ = 0

            for col_ctr in range( tbl['START_COL'], tbl['END_COL'] ):
                if self.sheet.cell(row=row_ctr, column=col_ctr).value is not None and \
                        len( str( self.sheet.cell(row=row_ctr, column=col_ctr).value ) ) > 0: 
                    num_non_blank_ += 1

            ## only if the number of hdr columns is in the ballpark w.r.t. total number of columns
            ## should we start the table ..at times the header table is split across more than 1 row
            if total_cols_ > 1 and (num_non_blank_/total_cols_) > self.col_thresh_:
                return row_ctr

        return None # so default value of row #1 applies to table start

    def createDBRec( self, summary_D, mode='NORM' ):

        insertRec = dict()
        insertRec['docID'] = random.randint( 1000, 100000 )
        ## combine all necessary fields to form vector signature
        ## keys-> 'sample_summary'; 'date_range' ; 'hdr_info'

        hdr_info = summary_D['hdr_info']
        sample_summary_ = summary_D['sample_summary']

        unified_key_ =   'Date Range : '+ str( summary_D['date_range'] ) + '\n' \
                       + 'Column Headers : '+ ' , '.join( summary_D['col_names_'] ).strip() + '\n' \
                       + 'LLM Summary : '+ ( sample_summary_ ) if self.add_ai_summary_to_embedding_ is True else ''

        emb_ = createJsonFeats.returnEmbed( unified_key_ )
        insertRec['docSignature'] = emb_
        insertRec['summary'] = unified_key_
        insertRec['file_path'] = summary_D['file_path']
        insertRec['file_name'] = summary_D['file_path'].split('/')[-1]
        insertRec['sheet_name'] = summary_D['sheet_name']
        insertRec['date_range'] = summary_D['date_range']
        insertRec['hdr_info'] = hdr_info

        print('Inserting RECORD->', insertRec['file_name'], insertRec['sheet_name'], unified_key_ )
        return insertRec

    def mergeAndInsert( self, summary_D ):
        '''
        we shall be inserting 2 records for every table
        a) the normal table structure
        b) the transposed table structure
        along with all meta info
        '''
        ##NORM TBL STRUCT
        rec_ = self.createDBRec( summary_D, 'NORM' )
        db_utils.insertNewSignature( rec_ )

    def returnEntireSheet( self, tbl_, sheet_name ):
        '''
        find if the entire sheet contains mostly textual information. If so, then we should simply
        chunk the whole sheet , after concatenating 
        A simple rule of thumb can be the length of the cell contents in any column.
        If the lenght of the cell contents is greater than some threshold say 10 words
        '''
        use_entire_sheet_, chunks_ = False, []

        for col_ctr in range( tbl_['START_COL'], tbl_['END_COL'] ):
            num_str_, unique_, ignore = 0, set(), False
            for row_ctr in range( tbl_['START_ROW'], tbl_['END_ROW'] ):
                if type( self.sheet.cell(row=row_ctr, column=col_ctr).value ) == str and\
                        len( (self.sheet.cell(row=row_ctr, column=col_ctr).value).split() ) >= self.sz_of_phrase_:
                            num_str_ += 1
                unique_.add( ( self.sheet.cell(row=row_ctr, column=col_ctr).value ) )

            ## if num of unique strings in col is low it means, this value is being repeated
            ## HENCE its mostly observations being selected from a drop down and does NOT need
            ## the entire doc chunked
            if len( unique_ ) < self.unique_str_thresh_*( tbl_['END_ROW'] - tbl_['START_ROW'] ): ignore = True

            print('returnEntireSheet->', sheet_name, tbl_, num_str_, ( tbl_['END_ROW'] - tbl_['START_ROW'] ), ignore)
            if num_str_ >= self.number_str_thresh_*( tbl_['END_ROW'] - tbl_['START_ROW'] ) and ignore is False:

                use_entire_sheet_ = True
                ## aggregate all text and chunk using self.chunk_size_
                frame_ = ''
                for row_ctr in range( tbl_['START_ROW'], tbl_['END_ROW'] ):
                    for col_ctr in range( tbl_['START_COL'], tbl_['END_COL'] ):

                        if len( frame_ ) >= self.chunk_size_:
                            chunks_.append( frame_ )
                            frame_ = ''

                        frame_ += '\t'+ str( self.sheet.cell(row=row_ctr, column=col_ctr).value )
                    frame_ += '\n'

                if len( frame_ ) > 0: chunks_.append( frame_ )

        return chunks_, use_entire_sheet_

    def process_full_frame_( self, full_frame_, summary_D ):

        for chunk in full_frame_:
           summary_D['sample_summary'] = chunk
           self.mergeAndInsert( summary_D )

    def read_excel_file(self):
        # Load the workbook
        main_timer_ = time.time()
        workbook = openpyxl.load_workbook( self.file_path )
        #workbook = openpyxl.load_workbook( self.file_path, read_only=True )
        # Get the specified sheet in the workbook
        summary_D = dict()

        print( ' Time taken to open workbook->', time.time() - main_timer_)
        for sheet_obj in workbook:
            tt_ = time.time()
            self.sheet = sheet_obj
            sheet_name = self.sheet.title
            ## find all tables in the sheet
            #if 'Testing' not in self.sheet.title: continue

            print('Iterating over sheet->', self.sheet.title, self.sheet.max_row)

            all_tables_ = self.find_tables( self.sheet )
            print( 'ALL TABLES in the sheet->', sheet_name, all_tables_)
            print('TIMER: self.find_tables :: ', time.time() - tt_)

            for tblidx, tbl_ in enumerate( all_tables_ ):
                frame_, transposed_frame_ = '', ''
                print('xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx')

                if tbl_['START_ROW'] is None or tbl_['END_ROW'] is None or tbl_['START_COL'] is None\
                        or tbl_['END_COL'] is None:
                            print('The tbl and sheet->', self.sheet.title,' no data!')

                try:
                    frame_, high_variance_cols_, col_names_ = self.returnSummary( tbl_ )
                    print('TIMER: self.returnSummary :: ', time.time() - tt_)
                    full_frame_, is_full_frame_necessary_ = self.returnEntireSheet( tbl_, sheet_name )
                    ## find date range if available
                    summary_D['date_range'] = self.findDateRange( tbl_ )
                    summary_D['file_path'] = self.file_path
                    summary_D['sheet_name'] = sheet_name
                    summary_D['col_names_'] = col_names_
                    ## summarize hdr info
                    hdr_frame_ = self.findHeaderInfo( tbl_ )
                    print('TIMER: self.findHeaderInfo :: ', time.time() - tt_)

                    #summary_D['hdr_info'] = self.query_fn_.augmentHeaderInformation( hdr_frame_ )
                    summary_D['hdr_info'] = self.file_path.split('/')[-1] + ' ' + sheet_name + ' ' + \
                                            hdr_frame_ #+ ' ' + summary_D['hdr_info']

                    print('TIMER: self.findDateRange :: ', time.time() - tt_)

                    if summary_D['date_range'] == ( None, None ):
                        ## just add the timestamp of the file ..backup , BUT better than no time dimension
                        summary_D['date_range'] = \
                            datetime.datetime.fromtimestamp( os.path.getmtime(self.file_path) ).strftime('%B %d, %Y')

                    if is_full_frame_necessary_ == True:
                        self.process_full_frame_( full_frame_, summary_D )
                        print('All TEXT ..hence saving chunks!')
                        continue

                    summary_ = self.query_fn_.returnDocSummary( frame_, high_variance_cols_ )

                    print( tblidx,' :: ', tbl_, '::', '\n', frame_, '\n LLAMA3: ', summary_  )
                    print('TIMER: self.query_fn_.returnDocSummary :: ', time.time() - tt_)
                    time.sleep(1) ## groq APIs while testing this were timing out like crazy

                    #print('Sending to LLM for summary->', summary_, '\n', summary_transposed_)
                    ## append file name, sheet name
                    print('Time taken for first 2 LLM calls->', time.time() - tt_)
                    summary_D['sample_summary'] = self.file_path.split('/')[-1] + ' ' + sheet_name + ' ' + summary_

                    print('Time Taken->', time.time() - tt_)
                    print('Time taken for last LLM calls->', time.time() - tt_)

                    #summary_D['pandas_dataframe'] = self.convertToPandas( tbl_ )
                    #print('Time taken for pandas calls->', time.time() - tt_)
                    ## now MERGE all the info and push into vector DB
                    self.mergeAndInsert( summary_D )
                    print('TIMER: self.mergeAndInsert :: ', time.time() - tt_)
                except:
                    print( 'EXCPN-> '+self.file_path + ' ' + sheet_name + ' ' + traceback.format_exc() )
                    continue

                self.masterInfo_[ sheet_name ] = summary_D

if __name__ == '__main__':
    files_ = os.listdir( './DATA/' )

    for file_ in files_:
        try:
            if 'Indexing Process' not in file_: continue

            get_meta_ = GenerateXLMetaInfo( './DATA/' + file_ )
            get_meta_.read_excel_file()
        except:
            print('EXCPN2-> File Loader FAIL = '+'./DATA/' + file_)
            print( traceback.format_exc() )
            continue

    '''

    get_meta_ = GenerateXLMetaInfo( './DATA/Time & Accuracy.xlsx' )
    get_meta_.read_excel_file()


CODE_SNIP-> import json, math, sys, traceback, copy, multiprocessing, os
from dateutil import parser
import numpy as np
import openpyxl
from openpyxl.utils import column_index_from_string
import time, random, datetime
import pandas as pd
from sklearn.decomposition import PCA

import query_llama3_via_groq as llama3
import query_gpt_via_groq as openai
import createJsonFeats
import db_utils

def is_date( input_str):
        ## first check for INT and FLOAT since parser foolishly accepts ints
        try:
            _ = int( input_str )
            return None
        except:
            pass

        try:
            _ = float( input_str )
            return None
        except:
            pass

        try:
            return parser.parse(input_str)
        except ValueError:
            return None

def process( colNum, sheet, tbl ):
        dt_counts_ = []

        for rw in range( tbl['START_ROW'], tbl['END_ROW'] ):
                dtVal_ = is_date( str( sheet.cell(row=rw, column=colNum).value ) )

                if dtVal_ is not None : 
                    dt_counts_.append( dtVal_ ) 

        if len( dt_counts_ ) >= ( tbl['END_ROW'] - tbl['START_ROW'] )/2: 
                ## defensive chk to ensure dt counts are high
                print('Dt Col found !', colNum)
                ## sort the values to get range
                sorted_dates_ = sorted( dt_counts_ )
                print('Dt range->', sorted_dates_[0], sorted_dates_[-1] )

                return ( True, sorted_dates_[0].strftime('%B %d, %Y'), sorted_dates_[-1].strftime('%B %d, %Y') )

        return ( False, None, None )

class GenerateXLMetaInfo:
    def __init__(self, file_path, llm='LLAMA'):
        """
        Initialize the GenerateXmlMetaInfo class with the XML file.

        Parameters:
        - xml_file (str): The path to the XML file.
        """
        self.file_path = file_path
        self.masterInfo_ = dict()
        self.llm_framework_ = llm
        self.sheet = None
        self.sklearn_pca_object_ = PCA()
        self.add_ai_summary_to_embedding_ = True
        self.chunk_size_ = 500 ## approx 1024 tokens

        self.sz_of_phrase_, self.unique_str_thresh_, self.number_str_thresh_ = 5, 0.5, 0.4
        self.pca_var_min_contrib, self.feature_contribution_per_thresh_ = 0.5, 0.3
        self.max_tables_per_sheet_ = 10 ## at times a single sheet can have multiple tables
        self.num_rows_to_consider_, self.col_thresh_, self.minElemsInTable, self.max_rows_variance = 4, 0.8, 6, 100
        self.default_max_col_ , self.default_max_row_, self.max_cols_for_review_, \
                self.min_num_distinct_values_, self.max_elements_for_summary_ = 50, 50, 10, 3, 15

        if llm == 'OPENAI':
            self.query_fn_ = openai
        else:
            ## default , llama3 inferred via groq
            self.query_fn_ = llama3

    def find_bounds( self, sheet, max_row, max_col, start_row ,end_row ,start_col ,end_col ):

        # Iterate over rows to find the start and end rows
        start_row_idx_ = 1 if start_row is None else start_row
        start_col_idx_ = 1 if start_col is None else start_col

        ## need to add 2 to max rw and col since max_row of sheet returns the last row with data
        ## and range will stop at max_row - 1 
        for row in range( start_row_idx_ , max_row + 2):
            if all( self.sheet.cell(row=row, column=col).value is None for col in range(1, max_col + 2)):
                if start_row is None:
                    continue  # Skip empty rows before the table
                else:
                    end_row = row - 1
                    break
            elif start_row is None:
                start_row = row

        # Iterate over columns to find the start and end columns
        for col in range( start_col_idx_, max_col + 2):
            #for row in range(start_row, end_row):
            #    print('ROW NUM->', col, ' VALUE: ', sheet.cell(row=row, column=col).value )
            if end_row is None: continue

            if all( self.sheet.cell(row=row, column=col).value is None for row in range(start_row, end_row)):
                if start_col is None:
                    continue  # Skip empty columns before the table
                else:
                    end_col = col - 1
                    break
            elif start_col is None:
                start_col = col


        #print('Found tables between-> start_row, end_row, start_col, end_col = ',\
        #        start_row, end_row, start_col, end_col )

        return start_row, end_row, start_col, end_col

    def is_hdr_row_format( self, tbl_bound, sheet ):

        num_str_cols_ = 0
        for col_ctr in range( tbl_bound['START_COL'], tbl_bound['END_COL'] ):
            if type( self.sheet.cell(row=tbl_bound['START_ROW'], column=col_ctr).value ) == str:
                num_str_cols_ += 1

        if num_str_cols_ < ( tbl_bound['END_COL'] - tbl_bound['START_COL'] ): return False

        return True

    def find_tables(self, sheet):
        ## NOTE -> sheet.max_row and sheet.max_column is NOT WORKING !! NEED TO FIX
        ## default is stop gap
        max_row = sheet.max_row if sheet.max_row is not None else self.default_max_row_
        max_col = sheet.max_column if sheet.max_column is not None else self.default_max_col_
        table_bounds_ = []

        print('KKR->', max_row, max_col)
        timer_ = time.time()
        # Initialize variables to track the bounds
        start_row ,end_row ,start_col ,end_col = None, None, 1, sheet.max_column

        ## do a first pass to find the first table
        start_row, end_row, start_col, end_col = self.find_bounds( sheet, max_row, max_col, start_row ,\
                                                                         end_row ,start_col ,end_col )

        print('Time taken->', time.time() - timer_, start_row, end_row, start_col, end_col)    
        init_end_col = copy.copy( end_col )

        table_bounds_.append( { 'START_ROW': start_row, 'END_ROW': end_row,\
                                    'START_COL': start_col, 'END_COL': end_col } ) 

        ## now iterate from end_row to max_row to find all tables row wise
        while end_row is not None:
            end_row += 2 ## increment by 2 since we need to look ahead and see if any more tables exist !
            ##              if u increment by 1 then u will end up on the same blank line that stopped the prev tbl  
            ## start_row is assigned the value of end_row from above and end_row is made None
            if end_row >= max_row: break

            #print('DUM ROW->', end_row)
            start_row, end_row, start_col, end_col = self.find_bounds( sheet, max_row, max_col, end_row ,\
                                                                             None , None , None )

            if ( start_col is None or end_col is None ) or \
                    ( abs( start_row - end_row )*abs( start_col - end_col ) ) <= self.minElemsInTable: continue    

            table_bounds_.append( { 'START_ROW': start_row, 'END_ROW': end_row,\
                                    'START_COL': start_col, 'END_COL': end_col } ) 

        ## now iterate from end_col to max_col to find all tables cols wise
        while init_end_col is not None:
            init_end_col += 2 ## increment by 1 since we need to look ahead and see if any more tables exist !
            ## start_row is assigned the value of end_row from above and end_row is made None
            if init_end_col >= max_col: break

            #print('DUM COL->', init_end_col)
            start_row, end_row, start_col, end_col = self.find_bounds( sheet, max_row, max_col, None ,\
                                                                             None , init_end_col , None )

            if ( start_col >= end_col ): continue    

            table_bounds_.append( { 'START_ROW': start_row, 'END_ROW': end_row,\
                                    'START_COL': start_col, 'END_COL': end_col } ) 

        ## init star and end col to min and max
        for tab in table_bounds_: tab['START_COL'] = 1; tab['END_COL'] = max_col;
        ## remove dupes
        tmp_, dupe = [], set()

        for idx1, tab1 in enumerate( table_bounds_ ):
            for idx2, tab2 in enumerate( table_bounds_ ):
                if idx1 <= idx2: continue
                if tab2['START_ROW'] >= tab1['START_ROW'] and tab2['END_ROW'] <= tab1['END_ROW']:
                    dupe.add( idx2 )

        for idx, tab in enumerate( table_bounds_ ):
            if idx not in list( dupe ):
                tmp_.append( tab )

        ## blend tables - in case the rows are FPs
        final_resp_ = []
        if len( tmp_ ) > 1:
            last_tbl_ = tmp_[0]
            final_resp_.append( last_tbl_ )
            ## check if the first row is not all STR
            for ctr in range( 1, len( tmp_ ) ):
                if self.is_hdr_row_format( tmp_[ctr], sheet ) == False:
                    ## blend with the last table
                    final_resp_[-1]['END_ROW'] = tmp_[ctr]['END_ROW']
                else:
                    final_resp_.append( table_bounds_ )
        else:
            final_resp_ = tmp_

        return final_resp_[ : min( self.max_tables_per_sheet_, len( final_resp_ ) ) ]

    def findDateRange( self, tbl ):

        colRange_ = list( range( tbl['START_COL'], tbl['END_COL'] ) )

        for col in colRange_:
            ## process was taken out of the class only because multi processing map refused to pickle
            ## a method that was part of the class ..and it proved way slower ..so parallels been removed..lol
            results = process(col, self.sheet, tbl)
            if results[0] is True:
                    return str( results[1] ) +' To '+ str( results[2] )

        return (None, None)

    def findHeaderInfo(self, tbl):
        """
        Find header information from the XL file.
        take the first 2 rows and then to be on the safe side also take the 
        first 2 columns ( in case the col headers are just numbers / % etc and the row contain item name in the first col )
        send it to the LLM for a summary
        ALSO there's no need to take all ROWS and COLS .. some 10-15 elements are more than enough but can be adjustedfor domains that need more
        """

        hdr_row_start_ = self.findHdrRow( tbl )
        row_starter_ = tbl['START_ROW'] if hdr_row_start_ is None else hdr_row_start_

        col_frame_ = ''

        for rw in range( row_starter_ , min( row_starter_ + self.num_rows_to_consider_, tbl['END_ROW'] ) ):
            for col in range( tbl['START_COL'], min( self.max_elements_for_summary_, tbl['END_COL'] + 1 ) ):
                col_frame_ += '\t' + str( self.sheet.cell(row=rw, column=col).value )

            col_frame_ += '\n'

        return col_frame_

    def findHighVarianceColumns(self, start_hdr_row_, sheet, tbl ):
        '''
        iterate through columns that have numeric values and figure out the more important columns
        num of rows - we can restrict it to lets say 1k rows ..should suffice 
        '''
        numeric_frame_, high_var_indices_, hdr_col_names_ = dict(), set(), []
        end_row_ =  min( tbl['START_ROW'] + self.max_rows_variance , tbl['END_ROW'] + 1 )
        ## add 1 to the start row since we dont want to include the header value
        start_row_ = ( tbl['START_ROW'] if start_hdr_row_ is None else start_hdr_row_ ) + 1 

        print('BIGGIE-> start_row_, end_row_ = ', start_row_, end_row_)

        for col_ctr in range( tbl['START_COL'], tbl['END_COL']+1 ):
                hdr_col_names_.append( str( self.sheet.cell(row=start_row_-1, column=col_ctr).value ) )

        try:
            for col_ctr in range( tbl['START_COL'], tbl['END_COL']+1 ):
                col_arr_ = [ 'NA' for x in range( ( end_row_ - start_row_ ) + 1 ) ]

                for idx, row_ctr in enumerate( range( start_row_, end_row_ ) ):
                    col_arr_[ idx ] = str( self.sheet.cell(row=row_ctr, column=col_ctr).value )

                ## standardize the column since PCA better be done on std values
                col_set_ = set( col_arr_ )
                ## convert the variables into unique IDs
                uid = [ list( col_set_ ).index( x ) for x in col_arr_ ]
                max_uid_ = np.max( uid )
                ## normalize the int values
                numeric_frame_[ col_ctr ] = [ x/max_uid_ for x in uid ]

            if len( numeric_frame_.keys() ) > 0:
                ## now transpose the contents of the frame since we want it to retain the shape of a column
                transposed_ = np.transpose( np.asarray( list( numeric_frame_.values() ) ) )
                #print('The val of transposed_->', transposed_)
                ## perform PCA and pick the most high variance columns
                ## the number of components to be picked will be decided by the thresh self.percent_pca_var_
                self.sklearn_pca_object_.fit( transposed_ )
                ## components_loading_ will give you principal component wise contribution of the features
                components_loading_ = self.sklearn_pca_object_.components_
                ## only consider those components that contribute to 90% or whatever threshold level of variance
                relevant_loading_ = components_loading_[0] \
                                    if self.sklearn_pca_object_.explained_variance_ratio_[0] > self.pca_var_min_contrib \
                                    else []

                #print('LOADING AND REL_LOADING->', components_loading_, relevant_loading_)
                key_list_ = list( numeric_frame_.keys() )

                for feat_idx, feat_contribution in enumerate( relevant_loading_ ):
                        if feat_contribution >= self.feature_contribution_per_thresh_: 

                            high_var_indices_.add( hdr_col_names_[ key_list_[ feat_idx ] ] )

                            #print('Adding ', hdr_col_names_[ key_list_[ feat_idx ] ],' As a high variance col')
        except:
            pass

        return list( high_var_indices_ ), hdr_col_names_


    def returnSummary(self, tbl ):
        '''
        take the first few rows to try and generate a coherent summary for the type of the data present
        i am also considering transposing the first few rows to see how different the summary looks
        ALSO maybe limiting the number of columns makes sense
        '''
        frame_, transposed_frame_, start_hdr_row_ = '', '', self.findHdrRow( tbl )

        time_ = time.time()
        high_variance_cols_, hdr_col_names_ = self.findHighVarianceColumns( start_hdr_row_, self.sheet, tbl )
        print('Time taken to find high var cols ->', time.time() - time_)
        print('AND THEY ARE->', high_variance_cols_)

        frame_num_contours_, transposed_frame_contours_ = 0, 0
        ## NATURAL order -> left to right, top to bottom
        for row_ctr in range( tbl['START_ROW'] if start_hdr_row_ is None else start_hdr_row_\
                              , min( tbl['START_ROW']+self.num_rows_to_consider_ , tbl['END_ROW']+1 ) ):

            for col_ctr in range( tbl['START_COL'], min( self.max_elements_for_summary_, tbl['END_COL']+1 ) ):

                frame_ += '\t' + str( self.sheet.cell(row=row_ctr, column=col_ctr).value )
                frame_num_contours_ += 1

            frame_ += '\n'

        return frame_, high_variance_cols_, list( set(hdr_col_names_) )

    def findHdrRow( self, tbl ):

        total_cols_ = tbl['END_COL'] - tbl['START_COL']

        for row_ctr in range( tbl['START_ROW'], \
                              min( tbl['START_ROW']+self.num_rows_to_consider_ , tbl['END_ROW']+1 ) ):
            num_non_blank_ = 0

            for col_ctr in range( tbl['START_COL'], tbl['END_COL'] ):
                if self.sheet.cell(row=row_ctr, column=col_ctr).value is not None and \
                        len( str( self.sheet.cell(row=row_ctr, column=col_ctr).value ) ) > 0: 
                    num_non_blank_ += 1

            ## only if the number of hdr columns is in the ballpark w.r.t. total number of columns
            ## should we start the table ..at times the header table is split across more than 1 row
            if total_cols_ > 1 and (num_non_blank_/total_cols_) > self.col_thresh_:
                return row_ctr

        return None # so default value of row #1 applies to table start

    def createDBRec( self, summary_D, mode='NORM' ):

        insertRec = dict()
        insertRec['docID'] = random.randint( 1000, 100000 )
        ## combine all necessary fields to form vector signature
        ## keys-> 'sample_summary'; 'date_range' ; 'hdr_info'

        hdr_info = summary_D['hdr_info']
        sample_summary_ = summary_D['sample_summary']

        unified_key_ =   'Date Range : '+ str( summary_D['date_range'] ) + '\n' \
                       + 'Column Headers : '+ ' , '.join( summary_D['col_names_'] ).strip() + '\n' \
                       + 'LLM Summary : '+ ( sample_summary_ ) if self.add_ai_summary_to_embedding_ is True else ''

        emb_ = createJsonFeats.returnEmbed( unified_key_ )
        insertRec['docSignature'] = emb_
        insertRec['summary'] = unified_key_
        insertRec['file_path'] = summary_D['file_path']
        insertRec['file_name'] = summary_D['file_path'].split('/')[-1]
        insertRec['sheet_name'] = summary_D['sheet_name']
        insertRec['date_range'] = summary_D['date_range']
        insertRec['hdr_info'] = hdr_info

        print('Inserting RECORD->', insertRec['file_name'], insertRec['sheet_name'], unified_key_ )
        return insertRec

    def mergeAndInsert( self, summary_D ):
        '''
        we shall be inserting 2 records for every table
        a) the normal table structure
        b) the transposed table structure
        along with all meta info
        '''
        ##NORM TBL STRUCT
        rec_ = self.createDBRec( summary_D, 'NORM' )
        db_utils.insertNewSignature( rec_ )

    def returnEntireSheet( self, tbl_, sheet_name ):
        '''
        find if the entire sheet contains mostly textual information. If so, then we should simply
        chunk the whole sheet , after concatenating 
        A simple rule of thumb can be the length of the cell contents in any column.
        If the lenght of the cell contents is greater than some threshold say 10 words
        '''
        use_entire_sheet_, chunks_ = False, []

        for col_ctr in range( tbl_['START_COL'], tbl_['END_COL'] ):
            num_str_, unique_, ignore = 0, set(), False
            for row_ctr in range( tbl_['START_ROW'], tbl_['END_ROW'] ):
                if type( self.sheet.cell(row=row_ctr, column=col_ctr).value ) == str and\
                        len( (self.sheet.cell(row=row_ctr, column=col_ctr).value).split() ) >= self.sz_of_phrase_:
                            num_str_ += 1
                unique_.add( ( self.sheet.cell(row=row_ctr, column=col_ctr).value ) )

            ## if num of unique strings in col is low it means, this value is being repeated
            ## HENCE its mostly observations being selected from a drop down and does NOT need
            ## the entire doc chunked
            if len( unique_ ) < self.unique_str_thresh_*( tbl_['END_ROW'] - tbl_['START_ROW'] ): ignore = True

            print('returnEntireSheet->', sheet_name, tbl_, num_str_, ( tbl_['END_ROW'] - tbl_['START_ROW'] ), ignore)
            if num_str_ >= self.number_str_thresh_*( tbl_['END_ROW'] - tbl_['START_ROW'] ) and ignore is False:

                use_entire_sheet_ = True
                ## aggregate all text and chunk using self.chunk_size_
                frame_ = ''
                for row_ctr in range( tbl_['START_ROW'], tbl_['END_ROW'] ):
                    for col_ctr in range( tbl_['START_COL'], tbl_['END_COL'] ):

                        if len( frame_ ) >= self.chunk_size_:
                            chunks_.append( frame_ )
                            frame_ = ''

                        frame_ += '\t'+ str( self.sheet.cell(row=row_ctr, column=col_ctr).value )
                    frame_ += '\n'

                if len( frame_ ) > 0: chunks_.append( frame_ )

        return chunks_, use_entire_sheet_

    def process_full_frame_( self, full_frame_, summary_D ):

        for chunk in full_frame_:
           summary_D['sample_summary'] = chunk
           self.mergeAndInsert( summary_D )

    def read_excel_file(self):
        # Load the workbook
        main_timer_ = time.time()
        workbook = openpyxl.load_workbook( self.file_path )
        #workbook = openpyxl.load_workbook( self.file_path, read_only=True )
        # Get the specified sheet in the workbook
        summary_D = dict()

        print( ' Time taken to open workbook->', time.time() - main_timer_)
        for sheet_obj in workbook:
            tt_ = time.time()
            self.sheet = sheet_obj
            sheet_name = self.sheet.title
            ## find all tables in the sheet
            #if 'Testing' not in self.sheet.title: continue

            print('Iterating over sheet->', self.sheet.title, self.sheet.max_row)

            all_tables_ = self.find_tables( self.sheet )
            print( 'ALL TABLES in the sheet->', sheet_name, all_tables_)
            print('TIMER: self.find_tables :: ', time.time() - tt_)

            for tblidx, tbl_ in enumerate( all_tables_ ):
                frame_, transposed_frame_ = '', ''
                print('xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx')

                if tbl_['START_ROW'] is None or tbl_['END_ROW'] is None or tbl_['START_COL'] is None\
                        or tbl_['END_COL'] is None:
                            print('The tbl and sheet->', self.sheet.title,' no data!')

                try:
                    frame_, high_variance_cols_, col_names_ = self.returnSummary( tbl_ )
                    print('TIMER: self.returnSummary :: ', time.time() - tt_)
                    full_frame_, is_full_frame_necessary_ = self.returnEntireSheet( tbl_, sheet_name )
                    ## find date range if available
                    summary_D['date_range'] = self.findDateRange( tbl_ )
                    summary_D['file_path'] = self.file_path
                    summary_D['sheet_name'] = sheet_name
                    summary_D['col_names_'] = col_names_
                    ## summarize hdr info
                    hdr_frame_ = self.findHeaderInfo( tbl_ )
                    print('TIMER: self.findHeaderInfo :: ', time.time() - tt_)

                    #summary_D['hdr_info'] = self.query_fn_.augmentHeaderInformation( hdr_frame_ )
                    summary_D['hdr_info'] = self.file_path.split('/')[-1] + ' ' + sheet_name + ' ' + \
                                            hdr_frame_ #+ ' ' + summary_D['hdr_info']

                    print('TIMER: self.findDateRange :: ', time.time() - tt_)

                    if summary_D['date_range'] == ( None, None ):
                        ## just add the timestamp of the file ..backup , BUT better than no time dimension
                        summary_D['date_range'] = \
                            datetime.datetime.fromtimestamp( os.path.getmtime(self.file_path) ).strftime('%B %d, %Y')

                    if is_full_frame_necessary_ == True:
                        self.process_full_frame_( full_frame_, summary_D )
                        print('All TEXT ..hence saving chunks!')
                        continue

                    summary_ = self.query_fn_.returnDocSummary( frame_, high_variance_cols_ )

                    print( tblidx,' :: ', tbl_, '::', '\n', frame_, '\n LLAMA3: ', summary_  )
                    print('TIMER: self.query_fn_.returnDocSummary :: ', time.time() - tt_)
                    time.sleep(1) ## groq APIs while testing this were timing out like crazy

                    #print('Sending to LLM for summary->', summary_, '\n', summary_transposed_)
                    ## append file name, sheet name
                    print('Time taken for first 2 LLM calls->', time.time() - tt_)
                    summary_D['sample_summary'] = self.file_path.split('/')[-1] + ' ' + sheet_name + ' ' + summary_

                    print('Time Taken->', time.time() - tt_)
                    print('Time taken for last LLM calls->', time.time() - tt_)

                    #summary_D['pandas_dataframe'] = self.convertToPandas( tbl_ )
                    #print('Time taken for pandas calls->', time.time() - tt_)
                    ## now MERGE all the info and push into vector DB
                    self.mergeAndInsert( summary_D )
                    print('TIMER: self.mergeAndInsert :: ', time.time() - tt_)
                except:
                    print( 'EXCPN-> '+self.file_path + ' ' + sheet_name + ' ' + traceback.format_exc() )
                    continue

                self.masterInfo_[ sheet_name ] = summary_D

if __name__ == '__main__':
    files_ = os.listdir( './DATA/' )

    for file_ in files_:
        try:
            if 'Indexing Process' not in file_: continue

            get_meta_ = GenerateXLMetaInfo( './DATA/' + file_ )
            get_meta_.read_excel_file()
        except:
            print('EXCPN2-> File Loader FAIL = '+'./DATA/' + file_)
            print( traceback.format_exc() )
            continue

    '''

    get_meta_ = GenerateXLMetaInfo( './DATA/Time & Accuracy.xlsx' )
    get_meta_.read_excel_file()

CODE_SNIP-> import json, math, sys, traceback, copy, multiprocessing, os
from dateutil import parser
import numpy as np
import openpyxl
from openpyxl.utils import column_index_from_string
import time, random, datetime
import pandas as pd
from sklearn.decomposition import PCA

import query_llama3_via_groq as llama3
import query_gpt_via_groq as openai
import createJsonFeats
import db_utils

def is_date( input_str):
        ## first check for INT and FLOAT since parser foolishly accepts ints
        try:
            _ = int( input_str )
            return None
        except:
            pass

        try:
            _ = float( input_str )
            return None
        except:
            pass

        try:
            return parser.parse(input_str)
        except ValueError:
            return None

def process( colNum, sheet, tbl ):
        dt_counts_ = []

        for rw in range( tbl['START_ROW'], tbl['END_ROW'] ):
                dtVal_ = is_date( str( sheet.cell(row=rw, column=colNum).value ) )

                if dtVal_ is not None : 
                    dt_counts_.append( dtVal_ ) 

        if len( dt_counts_ ) >= ( tbl['END_ROW'] - tbl['START_ROW'] )/2: 
                ## defensive chk to ensure dt counts are high
                print('Dt Col found !', colNum)
                ## sort the values to get range
                sorted_dates_ = sorted( dt_counts_ )
                print('Dt range->', sorted_dates_[0], sorted_dates_[-1] )

                return ( True, sorted_dates_[0].strftime('%B %d, %Y'), sorted_dates_[-1].strftime('%B %d, %Y') )

        return ( False, None, None )

class GenerateXLMetaInfo:
    def __init__(self, file_path, llm='LLAMA'):
        """
        Initialize the GenerateXmlMetaInfo class with the XML file.

        Parameters:
        - xml_file (str): The path to the XML file.
        """
        self.file_path = file_path
        self.masterInfo_ = dict()
        self.llm_framework_ = llm
        self.sheet = None
        self.sklearn_pca_object_ = PCA()
        self.add_ai_summary_to_embedding_ = True
        self.chunk_size_ = 500 ## approx 1024 tokens

        self.sz_of_phrase_, self.unique_str_thresh_, self.number_str_thresh_ = 5, 0.5, 0.4
        self.pca_var_min_contrib, self.feature_contribution_per_thresh_ = 0.5, 0.3
        self.max_tables_per_sheet_ = 10 ## at times a single sheet can have multiple tables
        self.num_rows_to_consider_, self.col_thresh_, self.minElemsInTable, self.max_rows_variance = 4, 0.8, 6, 100
        self.default_max_col_ , self.default_max_row_, self.max_cols_for_review_, \
                self.min_num_distinct_values_, self.max_elements_for_summary_ = 50, 50, 10, 3, 15

        if llm == 'OPENAI':
            self.query_fn_ = openai
        else:
            ## default , llama3 inferred via groq
            self.query_fn_ = llama3

    def find_bounds( self, sheet, max_row, max_col, start_row ,end_row ,start_col ,end_col ):

        # Iterate over rows to find the start and end rows
        start_row_idx_ = 1 if start_row is None else start_row
        start_col_idx_ = 1 if start_col is None else start_col

        ## need to add 2 to max rw and col since max_row of sheet returns the last row with data
        ## and range will stop at max_row - 1 
        for row in range( start_row_idx_ , max_row + 2):
            if all( self.sheet.cell(row=row, column=col).value is None for col in range(1, max_col + 2)):
                if start_row is None:
                    continue  # Skip empty rows before the table
                else:
                    end_row = row - 1
                    break
            elif start_row is None:
                start_row = row

        # Iterate over columns to find the start and end columns
        for col in range( start_col_idx_, max_col + 2):
            #for row in range(start_row, end_row):
            #    print('ROW NUM->', col, ' VALUE: ', sheet.cell(row=row, column=col).value )
            if end_row is None: continue

            if all( self.sheet.cell(row=row, column=col).value is None for row in range(start_row, end_row)):
                if start_col is None:
                    continue  # Skip empty columns before the table
                else:
                    end_col = col - 1
                    break
            elif start_col is None:
                start_col = col


        #print('Found tables between-> start_row, end_row, start_col, end_col = ',\
        #        start_row, end_row, start_col, end_col )

        return start_row, end_row, start_col, end_col

    def is_hdr_row_format( self, tbl_bound, sheet ):

        num_str_cols_ = 0
        for col_ctr in range( tbl_bound['START_COL'], tbl_bound['END_COL'] ):
            if type( self.sheet.cell(row=tbl_bound['START_ROW'], column=col_ctr).value ) == str:
                num_str_cols_ += 1

        if num_str_cols_ < ( tbl_bound['END_COL'] - tbl_bound['START_COL'] ): return False

        return True

    def find_tables(self, sheet):
        ## NOTE -> sheet.max_row and sheet.max_column is NOT WORKING !! NEED TO FIX
        ## default is stop gap
        max_row = sheet.max_row if sheet.max_row is not None else self.default_max_row_
        max_col = sheet.max_column if sheet.max_column is not None else self.default_max_col_
        table_bounds_ = []

        print('KKR->', max_row, max_col)
        timer_ = time.time()
        # Initialize variables to track the bounds
        start_row ,end_row ,start_col ,end_col = None, None, 1, sheet.max_column

        ## do a first pass to find the first table
        start_row, end_row, start_col, end_col = self.find_bounds( sheet, max_row, max_col, start_row ,\
                                                                         end_row ,start_col ,end_col )

        print('Time taken->', time.time() - timer_, start_row, end_row, start_col, end_col)    
        init_end_col = copy.copy( end_col )

        table_bounds_.append( { 'START_ROW': start_row, 'END_ROW': end_row,\
                                    'START_COL': start_col, 'END_COL': end_col } ) 

        ## now iterate from end_row to max_row to find all tables row wise
        while end_row is not None:
            end_row += 2 ## increment by 2 since we need to look ahead and see if any more tables exist !
            ##              if u increment by 1 then u will end up on the same blank line that stopped the prev tbl  
            ## start_row is assigned the value of end_row from above and end_row is made None
            if end_row >= max_row: break

            #print('DUM ROW->', end_row)
            start_row, end_row, start_col, end_col = self.find_bounds( sheet, max_row, max_col, end_row ,\
                                                                             None , None , None )

            if ( start_col is None or end_col is None ) or \
                    ( abs( start_row - end_row )*abs( start_col - end_col ) ) <= self.minElemsInTable: continue    

            table_bounds_.append( { 'START_ROW': start_row, 'END_ROW': end_row,\
                                    'START_COL': start_col, 'END_COL': end_col } ) 

        ## now iterate from end_col to max_col to find all tables cols wise
        while init_end_col is not None:
            init_end_col += 2 ## increment by 1 since we need to look ahead and see if any more tables exist !
            ## start_row is assigned the value of end_row from above and end_row is made None
            if init_end_col >= max_col: break

            #print('DUM COL->', init_end_col)
            start_row, end_row, start_col, end_col = self.find_bounds( sheet, max_row, max_col, None ,\
                                                                             None , init_end_col , None )

            if ( start_col >= end_col ): continue    

            table_bounds_.append( { 'START_ROW': start_row, 'END_ROW': end_row,\
                                    'START_COL': start_col, 'END_COL': end_col } ) 

        ## init star and end col to min and max
        for tab in table_bounds_: tab['START_COL'] = 1; tab['END_COL'] = max_col;
        ## remove dupes
        tmp_, dupe = [], set()

        for idx1, tab1 in enumerate( table_bounds_ ):
            for idx2, tab2 in enumerate( table_bounds_ ):
                if idx1 <= idx2: continue
                if tab2['START_ROW'] >= tab1['START_ROW'] and tab2['END_ROW'] <= tab1['END_ROW']:
                    dupe.add( idx2 )

        for idx, tab in enumerate( table_bounds_ ):
            if idx not in list( dupe ):
                tmp_.append( tab )

        ## blend tables - in case the rows are FPs
        final_resp_ = []
        if len( tmp_ ) > 1:
            last_tbl_ = tmp_[0]
            final_resp_.append( last_tbl_ )
            ## check if the first row is not all STR
            for ctr in range( 1, len( tmp_ ) ):
                if self.is_hdr_row_format( tmp_[ctr], sheet ) == False:
                    ## blend with the last table
                    final_resp_[-1]['END_ROW'] = tmp_[ctr]['END_ROW']
                else:
                    final_resp_.append( table_bounds_ )
        else:
            final_resp_ = tmp_

        return final_resp_[ : min( self.max_tables_per_sheet_, len( final_resp_ ) ) ]

    def findDateRange( self, tbl ):

        colRange_ = list( range( tbl['START_COL'], tbl['END_COL'] ) )

        for col in colRange_:
            ## process was taken out of the class only because multi processing map refused to pickle
            ## a method that was part of the class ..and it proved way slower ..so parallels been removed..lol
            results = process(col, self.sheet, tbl)
            if results[0] is True:
                    return str( results[1] ) +' To '+ str( results[2] )

        return (None, None)

    def findHeaderInfo(self, tbl):
        """
        Find header information from the XL file.
        take the first 2 rows and then to be on the safe side also take the 
        first 2 columns ( in case the col headers are just numbers / % etc and the row contain item name in the first col )
        send it to the LLM for a summary
        ALSO there's no need to take all ROWS and COLS .. some 10-15 elements are more than enough but can be adjustedfor domains that need more
        """

        hdr_row_start_ = self.findHdrRow( tbl )
        row_starter_ = tbl['START_ROW'] if hdr_row_start_ is None else hdr_row_start_

        col_frame_ = ''

        for rw in range( row_starter_ , min( row_starter_ + self.num_rows_to_consider_, tbl['END_ROW'] ) ):
            for col in range( tbl['START_COL'], min( self.max_elements_for_summary_, tbl['END_COL'] + 1 ) ):
                col_frame_ += '\t' + str( self.sheet.cell(row=rw, column=col).value )

            col_frame_ += '\n'

        return col_frame_

    def findHighVarianceColumns(self, start_hdr_row_, sheet, tbl ):
        '''
        iterate through columns that have numeric values and figure out the more important columns
        num of rows - we can restrict it to lets say 1k rows ..should suffice 
        '''
        numeric_frame_, high_var_indices_, hdr_col_names_ = dict(), set(), []
        end_row_ =  min( tbl['START_ROW'] + self.max_rows_variance , tbl['END_ROW'] + 1 )
        ## add 1 to the start row since we dont want to include the header value
        start_row_ = ( tbl['START_ROW'] if start_hdr_row_ is None else start_hdr_row_ ) + 1 

        print('BIGGIE-> start_row_, end_row_ = ', start_row_, end_row_)

        for col_ctr in range( tbl['START_COL'], tbl['END_COL']+1 ):
                hdr_col_names_.append( str( self.sheet.cell(row=start_row_-1, column=col_ctr).value ) )

        try:
            for col_ctr in range( tbl['START_COL'], tbl['END_COL']+1 ):
                col_arr_ = [ 'NA' for x in range( ( end_row_ - start_row_ ) + 1 ) ]

                for idx, row_ctr in enumerate( range( start_row_, end_row_ ) ):
                    col_arr_[ idx ] = str( self.sheet.cell(row=row_ctr, column=col_ctr).value )

                ## standardize the column since PCA better be done on std values
                col_set_ = set( col_arr_ )
                ## convert the variables into unique IDs
                uid = [ list( col_set_ ).index( x ) for x in col_arr_ ]
                max_uid_ = np.max( uid )
                ## normalize the int values
                numeric_frame_[ col_ctr ] = [ x/max_uid_ for x in uid ]

            if len( numeric_frame_.keys() ) > 0:
                ## now transpose the contents of the frame since we want it to retain the shape of a column
                transposed_ = np.transpose( np.asarray( list( numeric_frame_.values() ) ) )
                #print('The val of transposed_->', transposed_)
                ## perform PCA and pick the most high variance columns
                ## the number of components to be picked will be decided by the thresh self.percent_pca_var_
                self.sklearn_pca_object_.fit( transposed_ )
                ## components_loading_ will give you principal component wise contribution of the features
                components_loading_ = self.sklearn_pca_object_.components_
                ## only consider those components that contribute to 90% or whatever threshold level of variance
                relevant_loading_ = components_loading_[0] \
                                    if self.sklearn_pca_object_.explained_variance_ratio_[0] > self.pca_var_min_contrib \
                                    else []

                #print('LOADING AND REL_LOADING->', components_loading_, relevant_loading_)
                key_list_ = list( numeric_frame_.keys() )

                for feat_idx, feat_contribution in enumerate( relevant_loading_ ):
                        if feat_contribution >= self.feature_contribution_per_thresh_: 

                            high_var_indices_.add( hdr_col_names_[ key_list_[ feat_idx ] ] )

                            #print('Adding ', hdr_col_names_[ key_list_[ feat_idx ] ],' As a high variance col')
        except:
            pass

        return list( high_var_indices_ ), hdr_col_names_


    def returnSummary(self, tbl ):
        '''
        take the first few rows to try and generate a coherent summary for the type of the data present
        i am also considering transposing the first few rows to see how different the summary looks
        ALSO maybe limiting the number of columns makes sense
        '''
        frame_, transposed_frame_, start_hdr_row_ = '', '', self.findHdrRow( tbl )

        time_ = time.time()
        high_variance_cols_, hdr_col_names_ = self.findHighVarianceColumns( start_hdr_row_, self.sheet, tbl )
        print('Time taken to find high var cols ->', time.time() - time_)
        print('AND THEY ARE->', high_variance_cols_)

        frame_num_contours_, transposed_frame_contours_ = 0, 0
        ## NATURAL order -> left to right, top to bottom
        for row_ctr in range( tbl['START_ROW'] if start_hdr_row_ is None else start_hdr_row_\
                              , min( tbl['START_ROW']+self.num_rows_to_consider_ , tbl['END_ROW']+1 ) ):

            for col_ctr in range( tbl['START_COL'], min( self.max_elements_for_summary_, tbl['END_COL']+1 ) ):

                frame_ += '\t' + str( self.sheet.cell(row=row_ctr, column=col_ctr).value )
                frame_num_contours_ += 1

            frame_ += '\n'

        return frame_, high_variance_cols_, list( set(hdr_col_names_) )

    def findHdrRow( self, tbl ):

        total_cols_ = tbl['END_COL'] - tbl['START_COL']

        for row_ctr in range( tbl['START_ROW'], \
                              min( tbl['START_ROW']+self.num_rows_to_consider_ , tbl['END_ROW']+1 ) ):
            num_non_blank_ = 0

            for col_ctr in range( tbl['START_COL'], tbl['END_COL'] ):
                if self.sheet.cell(row=row_ctr, column=col_ctr).value is not None and \
                        len( str( self.sheet.cell(row=row_ctr, column=col_ctr).value ) ) > 0: 
                    num_non_blank_ += 1

            ## only if the number of hdr columns is in the ballpark w.r.t. total number of columns
            ## should we start the table ..at times the header table is split across more than 1 row
            if total_cols_ > 1 and (num_non_blank_/total_cols_) > self.col_thresh_:
                return row_ctr

        return None # so default value of row #1 applies to table start

    def createDBRec( self, summary_D, mode='NORM' ):

        insertRec = dict()
        insertRec['docID'] = random.randint( 1000, 100000 )
        ## combine all necessary fields to form vector signature
        ## keys-> 'sample_summary'; 'date_range' ; 'hdr_info'

        hdr_info = summary_D['hdr_info']
        sample_summary_ = summary_D['sample_summary']

        unified_key_ =   'Date Range : '+ str( summary_D['date_range'] ) + '\n' \
                       + 'Column Headers : '+ ' , '.join( summary_D['col_names_'] ).strip() + '\n' \
                       + 'LLM Summary : '+ ( sample_summary_ ) if self.add_ai_summary_to_embedding_ is True else ''

        emb_ = createJsonFeats.returnEmbed( unified_key_ )
        insertRec['docSignature'] = emb_
        insertRec['summary'] = unified_key_
        insertRec['file_path'] = summary_D['file_path']
        insertRec['file_name'] = summary_D['file_path'].split('/')[-1]
        insertRec['sheet_name'] = summary_D['sheet_name']
        insertRec['date_range'] = summary_D['date_range']
        insertRec['hdr_info'] = hdr_info

        print('Inserting RECORD->', insertRec['file_name'], insertRec['sheet_name'], unified_key_ )
        return insertRec

    def mergeAndInsert( self, summary_D ):
        '''
        we shall be inserting 2 records for every table
        a) the normal table structure
        b) the transposed table structure
        along with all meta info
        '''
        ##NORM TBL STRUCT
        rec_ = self.createDBRec( summary_D, 'NORM' )
        db_utils.insertNewSignature( rec_ )

    def returnEntireSheet( self, tbl_, sheet_name ):
        '''
        find if the entire sheet contains mostly textual information. If so, then we should simply
        chunk the whole sheet , after concatenating 
        A simple rule of thumb can be the length of the cell contents in any column.
        If the lenght of the cell contents is greater than some threshold say 10 words
        '''
        use_entire_sheet_, chunks_ = False, []

        for col_ctr in range( tbl_['START_COL'], tbl_['END_COL'] ):
            num_str_, unique_, ignore = 0, set(), False
            for row_ctr in range( tbl_['START_ROW'], tbl_['END_ROW'] ):
                if type( self.sheet.cell(row=row_ctr, column=col_ctr).value ) == str and\
                        len( (self.sheet.cell(row=row_ctr, column=col_ctr).value).split() ) >= self.sz_of_phrase_:
                            num_str_ += 1
                unique_.add( ( self.sheet.cell(row=row_ctr, column=col_ctr).value ) )

            ## if num of unique strings in col is low it means, this value is being repeated
            ## HENCE its mostly observations being selected from a drop down and does NOT need
            ## the entire doc chunked
            if len( unique_ ) < self.unique_str_thresh_*( tbl_['END_ROW'] - tbl_['START_ROW'] ): ignore = True

            print('returnEntireSheet->', sheet_name, tbl_, num_str_, ( tbl_['END_ROW'] - tbl_['START_ROW'] ), ignore)
            if num_str_ >= self.number_str_thresh_*( tbl_['END_ROW'] - tbl_['START_ROW'] ) and ignore is False:

                use_entire_sheet_ = True
                ## aggregate all text and chunk using self.chunk_size_
                frame_ = ''
                for row_ctr in range( tbl_['START_ROW'], tbl_['END_ROW'] ):
                    for col_ctr in range( tbl_['START_COL'], tbl_['END_COL'] ):

                        if len( frame_ ) >= self.chunk_size_:
                            chunks_.append( frame_ )
                            frame_ = ''

                        frame_ += '\t'+ str( self.sheet.cell(row=row_ctr, column=col_ctr).value )
                    frame_ += '\n'

                if len( frame_ ) > 0: chunks_.append( frame_ )

        return chunks_, use_entire_sheet_

    def process_full_frame_( self, full_frame_, summary_D ):

        for chunk in full_frame_:
           summary_D['sample_summary'] = chunk
           self.mergeAndInsert( summary_D )

    def read_excel_file(self):
        # Load the workbook
        main_timer_ = time.time()
        workbook = openpyxl.load_workbook( self.file_path )
        #workbook = openpyxl.load_workbook( self.file_path, read_only=True )
        # Get the specified sheet in the workbook
        summary_D = dict()

        print( ' Time taken to open workbook->', time.time() - main_timer_)
        for sheet_obj in workbook:
            tt_ = time.time()
            self.sheet = sheet_obj
            sheet_name = self.sheet.title
            ## find all tables in the sheet
            #if 'Testing' not in self.sheet.title: continue

            print('Iterating over sheet->', self.sheet.title, self.sheet.max_row)

            all_tables_ = self.find_tables( self.sheet )
            print( 'ALL TABLES in the sheet->', sheet_name, all_tables_)
            print('TIMER: self.find_tables :: ', time.time() - tt_)

            for tblidx, tbl_ in enumerate( all_tables_ ):
                frame_, transposed_frame_ = '', ''
                print('xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx')

                if tbl_['START_ROW'] is None or tbl_['END_ROW'] is None or tbl_['START_COL'] is None\
                        or tbl_['END_COL'] is None:
                            print('The tbl and sheet->', self.sheet.title,' no data!')

                try:
                    frame_, high_variance_cols_, col_names_ = self.returnSummary( tbl_ )
                    print('TIMER: self.returnSummary :: ', time.time() - tt_)
                    full_frame_, is_full_frame_necessary_ = self.returnEntireSheet( tbl_, sheet_name )
                    ## find date range if available
                    summary_D['date_range'] = self.findDateRange( tbl_ )
                    summary_D['file_path'] = self.file_path
                    summary_D['sheet_name'] = sheet_name
                    summary_D['col_names_'] = col_names_
                    ## summarize hdr info
                    hdr_frame_ = self.findHeaderInfo( tbl_ )
                    print('TIMER: self.findHeaderInfo :: ', time.time() - tt_)

                    #summary_D['hdr_info'] = self.query_fn_.augmentHeaderInformation( hdr_frame_ )
                    summary_D['hdr_info'] = self.file_path.split('/')[-1] + ' ' + sheet_name + ' ' + \
                                            hdr_frame_ #+ ' ' + summary_D['hdr_info']

                    print('TIMER: self.findDateRange :: ', time.time() - tt_)

                    if summary_D['date_range'] == ( None, None ):
                        ## just add the timestamp of the file ..backup , BUT better than no time dimension
                        summary_D['date_range'] = \
                            datetime.datetime.fromtimestamp( os.path.getmtime(self.file_path) ).strftime('%B %d, %Y')

                    if is_full_frame_necessary_ == True:
                        self.process_full_frame_( full_frame_, summary_D )
                        print('All TEXT ..hence saving chunks!')
                        continue

                    summary_ = self.query_fn_.returnDocSummary( frame_, high_variance_cols_ )

                    print( tblidx,' :: ', tbl_, '::', '\n', frame_, '\n LLAMA3: ', summary_  )
                    print('TIMER: self.query_fn_.returnDocSummary :: ', time.time() - tt_)
                    time.sleep(1) ## groq APIs while testing this were timing out like crazy

                    #print('Sending to LLM for summary->', summary_, '\n', summary_transposed_)
                    ## append file name, sheet name
                    print('Time taken for first 2 LLM calls->', time.time() - tt_)
                    summary_D['sample_summary'] = self.file_path.split('/')[-1] + ' ' + sheet_name + ' ' + summary_

                    print('Time Taken->', time.time() - tt_)
                    print('Time taken for last LLM calls->', time.time() - tt_)

                    #summary_D['pandas_dataframe'] = self.convertToPandas( tbl_ )
                    #print('Time taken for pandas calls->', time.time() - tt_)
                    ## now MERGE all the info and push into vector DB
                    self.mergeAndInsert( summary_D )
                    print('TIMER: self.mergeAndInsert :: ', time.time() - tt_)
                except:
                    print( 'EXCPN-> '+self.file_path + ' ' + sheet_name + ' ' + traceback.format_exc() )
                    continue

                self.masterInfo_[ sheet_name ] = summary_D

if __name__ == '__main__':
    files_ = os.listdir( './DATA/' )

    for file_ in files_:
        try:
            if 'Indexing Process' not in file_: continue

            get_meta_ = GenerateXLMetaInfo( './DATA/' + file_ )
            get_meta_.read_excel_file()
        except:
            print('EXCPN2-> File Loader FAIL = '+'./DATA/' + file_)
            print( traceback.format_exc() )
            continue

    '''

    get_meta_ = GenerateXLMetaInfo( './DATA/Time & Accuracy.xlsx' )

CODE_SNIP-> import json, math, sys, traceback, copy, multiprocessing, os
from dateutil import parser
import numpy as np
import openpyxl
from openpyxl.utils import column_index_from_string
import time, random, datetime
import pandas as pd
from sklearn.decomposition import PCA

import query_llama3_via_groq as llama3
import query_gpt_via_groq as openai
import createJsonFeats
import db_utils

def is_date( input_str):
        ## first check for INT and FLOAT since parser foolishly accepts ints
        try:
            _ = int( input_str )
            return None
        except:
            pass

        try:
            _ = float( input_str )
            return None
        except:
            pass

        try:
            return parser.parse(input_str)
        except ValueError:
            return None

def process( colNum, sheet, tbl ):
        dt_counts_ = []

        for rw in range( tbl['START_ROW'], tbl['END_ROW'] ):
                dtVal_ = is_date( str( sheet.cell(row=rw, column=colNum).value ) )

                if dtVal_ is not None : 
                    dt_counts_.append( dtVal_ ) 

        if len( dt_counts_ ) >= ( tbl['END_ROW'] - tbl['START_ROW'] )/2: 
                ## defensive chk to ensure dt counts are high
                print('Dt Col found !', colNum)
                ## sort the values to get range
                sorted_dates_ = sorted( dt_counts_ )
                print('Dt range->', sorted_dates_[0], sorted_dates_[-1] )

                return ( True, sorted_dates_[0].strftime('%B %d, %Y'), sorted_dates_[-1].strftime('%B %d, %Y') )

        return ( False, None, None )

class GenerateXLMetaInfo:
    def __init__(self, file_path, llm='LLAMA'):
        """
        Initialize the GenerateXmlMetaInfo class with the XML file.

        Parameters:
        - xml_file (str): The path to the XML file.
        """
        self.file_path = file_path
        self.masterInfo_ = dict()
        self.llm_framework_ = llm
        self.sheet = None
        self.sklearn_pca_object_ = PCA()
        self.add_ai_summary_to_embedding_ = True
        self.chunk_size_ = 500 ## approx 1024 tokens

        self.sz_of_phrase_, self.unique_str_thresh_, self.number_str_thresh_ = 5, 0.5, 0.4
        self.pca_var_min_contrib, self.feature_contribution_per_thresh_ = 0.5, 0.3
        self.max_tables_per_sheet_ = 10 ## at times a single sheet can have multiple tables
        self.num_rows_to_consider_, self.col_thresh_, self.minElemsInTable, self.max_rows_variance = 4, 0.8, 6, 100
        self.default_max_col_ , self.default_max_row_, self.max_cols_for_review_, \
                self.min_num_distinct_values_, self.max_elements_for_summary_ = 50, 50, 10, 3, 15

        if llm == 'OPENAI':
            self.query_fn_ = openai
        else:
            ## default , llama3 inferred via groq
            self.query_fn_ = llama3

    def find_bounds( self, sheet, max_row, max_col, start_row ,end_row ,start_col ,end_col ):

        # Iterate over rows to find the start and end rows
        start_row_idx_ = 1 if start_row is None else start_row
        start_col_idx_ = 1 if start_col is None else start_col

        ## need to add 2 to max rw and col since max_row of sheet returns the last row with data
        ## and range will stop at max_row - 1 
        for row in range( start_row_idx_ , max_row + 2):
            if all( self.sheet.cell(row=row, column=col).value is None for col in range(1, max_col + 2)):
                if start_row is None:
                    continue  # Skip empty rows before the table
                else:
                    end_row = row - 1
                    break
            elif start_row is None:
                start_row = row

        # Iterate over columns to find the start and end columns
        for col in range( start_col_idx_, max_col + 2):
            #for row in range(start_row, end_row):
            #    print('ROW NUM->', col, ' VALUE: ', sheet.cell(row=row, column=col).value )
            if end_row is None: continue

            if all( self.sheet.cell(row=row, column=col).value is None for row in range(start_row, end_row)):
                if start_col is None:
                    continue  # Skip empty columns before the table
                else:
                    end_col = col - 1
                    break
            elif start_col is None:
                start_col = col


        #print('Found tables between-> start_row, end_row, start_col, end_col = ',\
        #        start_row, end_row, start_col, end_col )

        return start_row, end_row, start_col, end_col

    def is_hdr_row_format( self, tbl_bound, sheet ):

        num_str_cols_ = 0
        for col_ctr in range( tbl_bound['START_COL'], tbl_bound['END_COL'] ):
            if type( self.sheet.cell(row=tbl_bound['START_ROW'], column=col_ctr).value ) == str:
                num_str_cols_ += 1

        if num_str_cols_ < ( tbl_bound['END_COL'] - tbl_bound['START_COL'] ): return False

        return True

    def find_tables(self, sheet):
        ## NOTE -> sheet.max_row and sheet.max_column is NOT WORKING !! NEED TO FIX
        ## default is stop gap
        max_row = sheet.max_row if sheet.max_row is not None else self.default_max_row_
        max_col = sheet.max_column if sheet.max_column is not None else self.default_max_col_
        table_bounds_ = []

        print('KKR->', max_row, max_col)
        timer_ = time.time()
        # Initialize variables to track the bounds
        start_row ,end_row ,start_col ,end_col = None, None, 1, sheet.max_column

        ## do a first pass to find the first table
        start_row, end_row, start_col, end_col = self.find_bounds( sheet, max_row, max_col, start_row ,\
                                                                         end_row ,start_col ,end_col )

        print('Time taken->', time.time() - timer_, start_row, end_row, start_col, end_col)    
        init_end_col = copy.copy( end_col )

        table_bounds_.append( { 'START_ROW': start_row, 'END_ROW': end_row,\
                                    'START_COL': start_col, 'END_COL': end_col } ) 

        ## now iterate from end_row to max_row to find all tables row wise
        while end_row is not None:
            end_row += 2 ## increment by 2 since we need to look ahead and see if any more tables exist !
            ##              if u increment by 1 then u will end up on the same blank line that stopped the prev tbl  
            ## start_row is assigned the value of end_row from above and end_row is made None
            if end_row >= max_row: break

            #print('DUM ROW->', end_row)
            start_row, end_row, start_col, end_col = self.find_bounds( sheet, max_row, max_col, end_row ,\
                                                                             None , None , None )

            if ( start_col is None or end_col is None ) or \
                    ( abs( start_row - end_row )*abs( start_col - end_col ) ) <= self.minElemsInTable: continue    

            table_bounds_.append( { 'START_ROW': start_row, 'END_ROW': end_row,\
                                    'START_COL': start_col, 'END_COL': end_col } ) 

        ## now iterate from end_col to max_col to find all tables cols wise
        while init_end_col is not None:
            init_end_col += 2 ## increment by 1 since we need to look ahead and see if any more tables exist !
            ## start_row is assigned the value of end_row from above and end_row is made None
            if init_end_col >= max_col: break

            #print('DUM COL->', init_end_col)
            start_row, end_row, start_col, end_col = self.find_bounds( sheet, max_row, max_col, None ,\
                                                                             None , init_end_col , None )

            if ( start_col >= end_col ): continue    

            table_bounds_.append( { 'START_ROW': start_row, 'END_ROW': end_row,\
                                    'START_COL': start_col, 'END_COL': end_col } ) 

        ## init star and end col to min and max
        for tab in table_bounds_: tab['START_COL'] = 1; tab['END_COL'] = max_col;
        ## remove dupes
        tmp_, dupe = [], set()

        for idx1, tab1 in enumerate( table_bounds_ ):
            for idx2, tab2 in enumerate( table_bounds_ ):
                if idx1 <= idx2: continue
                if tab2['START_ROW'] >= tab1['START_ROW'] and tab2['END_ROW'] <= tab1['END_ROW']:
                    dupe.add( idx2 )

        for idx, tab in enumerate( table_bounds_ ):
            if idx not in list( dupe ):
                tmp_.append( tab )

        ## blend tables - in case the rows are FPs
        final_resp_ = []
        if len( tmp_ ) > 1:
            last_tbl_ = tmp_[0]
            final_resp_.append( last_tbl_ )
            ## check if the first row is not all STR
            for ctr in range( 1, len( tmp_ ) ):
                if self.is_hdr_row_format( tmp_[ctr], sheet ) == False:
                    ## blend with the last table
                    final_resp_[-1]['END_ROW'] = tmp_[ctr]['END_ROW']
                else:
                    final_resp_.append( table_bounds_ )
        else:
            final_resp_ = tmp_

        return final_resp_[ : min( self.max_tables_per_sheet_, len( final_resp_ ) ) ]

    def findDateRange( self, tbl ):

        colRange_ = list( range( tbl['START_COL'], tbl['END_COL'] ) )

        for col in colRange_:
            ## process was taken out of the class only because multi processing map refused to pickle
            ## a method that was part of the class ..and it proved way slower ..so parallels been removed..lol
            results = process(col, self.sheet, tbl)
            if results[0] is True:
                    return str( results[1] ) +' To '+ str( results[2] )

        return (None, None)

    def findHeaderInfo(self, tbl):
        """
        Find header information from the XL file.
        take the first 2 rows and then to be on the safe side also take the 
        first 2 columns ( in case the col headers are just numbers / % etc and the row contain item name in the first col )
        send it to the LLM for a summary
        ALSO there's no need to take all ROWS and COLS .. some 10-15 elements are more than enough but can be adjustedfor domains that need more
        """

        hdr_row_start_ = self.findHdrRow( tbl )
        row_starter_ = tbl['START_ROW'] if hdr_row_start_ is None else hdr_row_start_

        col_frame_ = ''

        for rw in range( row_starter_ , min( row_starter_ + self.num_rows_to_consider_, tbl['END_ROW'] ) ):
            for col in range( tbl['START_COL'], min( self.max_elements_for_summary_, tbl['END_COL'] + 1 ) ):
                col_frame_ += '\t' + str( self.sheet.cell(row=rw, column=col).value )

            col_frame_ += '\n'

        return col_frame_

    def findHighVarianceColumns(self, start_hdr_row_, sheet, tbl ):
        '''
        iterate through columns that have numeric values and figure out the more important columns
        num of rows - we can restrict it to lets say 1k rows ..should suffice 
        '''
        numeric_frame_, high_var_indices_, hdr_col_names_ = dict(), set(), []
        end_row_ =  min( tbl['START_ROW'] + self.max_rows_variance , tbl['END_ROW'] + 1 )
        ## add 1 to the start row since we dont want to include the header value
        start_row_ = ( tbl['START_ROW'] if start_hdr_row_ is None else start_hdr_row_ ) + 1 

        print('BIGGIE-> start_row_, end_row_ = ', start_row_, end_row_)

        for col_ctr in range( tbl['START_COL'], tbl['END_COL']+1 ):
                hdr_col_names_.append( str( self.sheet.cell(row=start_row_-1, column=col_ctr).value ) )

        try:
            for col_ctr in range( tbl['START_COL'], tbl['END_COL']+1 ):
                col_arr_ = [ 'NA' for x in range( ( end_row_ - start_row_ ) + 1 ) ]

                for idx, row_ctr in enumerate( range( start_row_, end_row_ ) ):
                    col_arr_[ idx ] = str( self.sheet.cell(row=row_ctr, column=col_ctr).value )

                ## standardize the column since PCA better be done on std values
                col_set_ = set( col_arr_ )
                ## convert the variables into unique IDs
                uid = [ list( col_set_ ).index( x ) for x in col_arr_ ]
                max_uid_ = np.max( uid )
                ## normalize the int values
                numeric_frame_[ col_ctr ] = [ x/max_uid_ for x in uid ]

            if len( numeric_frame_.keys() ) > 0:
                ## now transpose the contents of the frame since we want it to retain the shape of a column
                transposed_ = np.transpose( np.asarray( list( numeric_frame_.values() ) ) )
                #print('The val of transposed_->', transposed_)
                ## perform PCA and pick the most high variance columns
                ## the number of components to be picked will be decided by the thresh self.percent_pca_var_
                self.sklearn_pca_object_.fit( transposed_ )
                ## components_loading_ will give you principal component wise contribution of the features
                components_loading_ = self.sklearn_pca_object_.components_
                ## only consider those components that contribute to 90% or whatever threshold level of variance
                relevant_loading_ = components_loading_[0] \
                                    if self.sklearn_pca_object_.explained_variance_ratio_[0] > self.pca_var_min_contrib \
                                    else []

                #print('LOADING AND REL_LOADING->', components_loading_, relevant_loading_)
                key_list_ = list( numeric_frame_.keys() )

                for feat_idx, feat_contribution in enumerate( relevant_loading_ ):
                        if feat_contribution >= self.feature_contribution_per_thresh_: 

                            high_var_indices_.add( hdr_col_names_[ key_list_[ feat_idx ] ] )

                            #print('Adding ', hdr_col_names_[ key_list_[ feat_idx ] ],' As a high variance col')
        except:
            pass

        return list( high_var_indices_ ), hdr_col_names_


    def returnSummary(self, tbl ):
        '''
        take the first few rows to try and generate a coherent summary for the type of the data present
        i am also considering transposing the first few rows to see how different the summary looks
        ALSO maybe limiting the number of columns makes sense
        '''
        frame_, transposed_frame_, start_hdr_row_ = '', '', self.findHdrRow( tbl )

        time_ = time.time()
        high_variance_cols_, hdr_col_names_ = self.findHighVarianceColumns( start_hdr_row_, self.sheet, tbl )
        print('Time taken to find high var cols ->', time.time() - time_)
        print('AND THEY ARE->', high_variance_cols_)

        frame_num_contours_, transposed_frame_contours_ = 0, 0
        ## NATURAL order -> left to right, top to bottom
        for row_ctr in range( tbl['START_ROW'] if start_hdr_row_ is None else start_hdr_row_\
                              , min( tbl['START_ROW']+self.num_rows_to_consider_ , tbl['END_ROW']+1 ) ):

            for col_ctr in range( tbl['START_COL'], min( self.max_elements_for_summary_, tbl['END_COL']+1 ) ):

                frame_ += '\t' + str( self.sheet.cell(row=row_ctr, column=col_ctr).value )
                frame_num_contours_ += 1

            frame_ += '\n'

        return frame_, high_variance_cols_, list( set(hdr_col_names_) )

    def findHdrRow( self, tbl ):

        total_cols_ = tbl['END_COL'] - tbl['START_COL']

        for row_ctr in range( tbl['START_ROW'], \
                              min( tbl['START_ROW']+self.num_rows_to_consider_ , tbl['END_ROW']+1 ) ):
            num_non_blank_ = 0

            for col_ctr in range( tbl['START_COL'], tbl['END_COL'] ):
                if self.sheet.cell(row=row_ctr, column=col_ctr).value is not None and \
                        len( str( self.sheet.cell(row=row_ctr, column=col_ctr).value ) ) > 0: 
                    num_non_blank_ += 1

            ## only if the number of hdr columns is in the ballpark w.r.t. total number of columns
            ## should we start the table ..at times the header table is split across more than 1 row
            if total_cols_ > 1 and (num_non_blank_/total_cols_) > self.col_thresh_:
                return row_ctr

        return None # so default value of row #1 applies to table start

    def createDBRec( self, summary_D, mode='NORM' ):

        insertRec = dict()
        insertRec['docID'] = random.randint( 1000, 100000 )
        ## combine all necessary fields to form vector signature
        ## keys-> 'sample_summary'; 'date_range' ; 'hdr_info'

        hdr_info = summary_D['hdr_info']
        sample_summary_ = summary_D['sample_summary']

        unified_key_ =   'Date Range : '+ str( summary_D['date_range'] ) + '\n' \
                       + 'Column Headers : '+ ' , '.join( summary_D['col_names_'] ).strip() + '\n' \
                       + 'LLM Summary : '+ ( sample_summary_ ) if self.add_ai_summary_to_embedding_ is True else ''

        emb_ = createJsonFeats.returnEmbed( unified_key_ )
        insertRec['docSignature'] = emb_
        insertRec['summary'] = unified_key_
        insertRec['file_path'] = summary_D['file_path']
        insertRec['file_name'] = summary_D['file_path'].split('/')[-1]
        insertRec['sheet_name'] = summary_D['sheet_name']
        insertRec['date_range'] = summary_D['date_range']
        insertRec['hdr_info'] = hdr_info

        print('Inserting RECORD->', insertRec['file_name'], insertRec['sheet_name'], unified_key_ )
        return insertRec

    def mergeAndInsert( self, summary_D ):
        '''
        we shall be inserting 2 records for every table
        a) the normal table structure
        b) the transposed table structure
        along with all meta info
        '''
        ##NORM TBL STRUCT
        rec_ = self.createDBRec( summary_D, 'NORM' )
        db_utils.insertNewSignature( rec_ )

    def returnEntireSheet( self, tbl_, sheet_name ):
        '''
        find if the entire sheet contains mostly textual information. If so, then we should simply
        chunk the whole sheet , after concatenating 
        A simple rule of thumb can be the length of the cell contents in any column.
        If the lenght of the cell contents is greater than some threshold say 10 words
        '''
        use_entire_sheet_, chunks_ = False, []

        for col_ctr in range( tbl_['START_COL'], tbl_['END_COL'] ):
            num_str_, unique_, ignore = 0, set(), False
            for row_ctr in range( tbl_['START_ROW'], tbl_['END_ROW'] ):
                if type( self.sheet.cell(row=row_ctr, column=col_ctr).value ) == str and\
                        len( (self.sheet.cell(row=row_ctr, column=col_ctr).value).split() ) >= self.sz_of_phrase_:
                            num_str_ += 1
                unique_.add( ( self.sheet.cell(row=row_ctr, column=col_ctr).value ) )

            ## if num of unique strings in col is low it means, this value is being repeated
            ## HENCE its mostly observations being selected from a drop down and does NOT need
            ## the entire doc chunked
            if len( unique_ ) < self.unique_str_thresh_*( tbl_['END_ROW'] - tbl_['START_ROW'] ): ignore = True

            print('returnEntireSheet->', sheet_name, tbl_, num_str_, ( tbl_['END_ROW'] - tbl_['START_ROW'] ), ignore)
            if num_str_ >= self.number_str_thresh_*( tbl_['END_ROW'] - tbl_['START_ROW'] ) and ignore is False:

                use_entire_sheet_ = True
                ## aggregate all text and chunk using self.chunk_size_
                frame_ = ''
                for row_ctr in range( tbl_['START_ROW'], tbl_['END_ROW'] ):
                    for col_ctr in range( tbl_['START_COL'], tbl_['END_COL'] ):

                        if len( frame_ ) >= self.chunk_size_:
                            chunks_.append( frame_ )
                            frame_ = ''

                        frame_ += '\t'+ str( self.sheet.cell(row=row_ctr, column=col_ctr).value )
                    frame_ += '\n'

                if len( frame_ ) > 0: chunks_.append( frame_ )

        return chunks_, use_entire_sheet_

    def process_full_frame_( self, full_frame_, summary_D ):

        for chunk in full_frame_:
           summary_D['sample_summary'] = chunk
           self.mergeAndInsert( summary_D )

    def read_excel_file(self):
        # Load the workbook
        main_timer_ = time.time()
        workbook = openpyxl.load_workbook( self.file_path )
        #workbook = openpyxl.load_workbook( self.file_path, read_only=True )
        # Get the specified sheet in the workbook
        summary_D = dict()

        print( ' Time taken to open workbook->', time.time() - main_timer_)
        for sheet_obj in workbook:
            tt_ = time.time()
            self.sheet = sheet_obj
            sheet_name = self.sheet.title
            ## find all tables in the sheet
            #if 'Testing' not in self.sheet.title: continue

            print('Iterating over sheet->', self.sheet.title, self.sheet.max_row)

            all_tables_ = self.find_tables( self.sheet )
            print( 'ALL TABLES in the sheet->', sheet_name, all_tables_)
            print('TIMER: self.find_tables :: ', time.time() - tt_)

            for tblidx, tbl_ in enumerate( all_tables_ ):
                frame_, transposed_frame_ = '', ''
                print('xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx')

                if tbl_['START_ROW'] is None or tbl_['END_ROW'] is None or tbl_['START_COL'] is None\
                        or tbl_['END_COL'] is None:
                            print('The tbl and sheet->', self.sheet.title,' no data!')

                try:
                    frame_, high_variance_cols_, col_names_ = self.returnSummary( tbl_ )
                    print('TIMER: self.returnSummary :: ', time.time() - tt_)
                    full_frame_, is_full_frame_necessary_ = self.returnEntireSheet( tbl_, sheet_name )
                    ## find date range if available
                    summary_D['date_range'] = self.findDateRange( tbl_ )
                    summary_D['file_path'] = self.file_path
                    summary_D['sheet_name'] = sheet_name
                    summary_D['col_names_'] = col_names_
                    ## summarize hdr info
                    hdr_frame_ = self.findHeaderInfo( tbl_ )
                    print('TIMER: self.findHeaderInfo :: ', time.time() - tt_)

                    #summary_D['hdr_info'] = self.query_fn_.augmentHeaderInformation( hdr_frame_ )
                    summary_D['hdr_info'] = self.file_path.split('/')[-1] + ' ' + sheet_name + ' ' + \
                                            hdr_frame_ #+ ' ' + summary_D['hdr_info']

                    print('TIMER: self.findDateRange :: ', time.time() - tt_)

                    if summary_D['date_range'] == ( None, None ):
                        ## just add the timestamp of the file ..backup , BUT better than no time dimension
                        summary_D['date_range'] = \
                            datetime.datetime.fromtimestamp( os.path.getmtime(self.file_path) ).strftime('%B %d, %Y')

                    if is_full_frame_necessary_ == True:
                        self.process_full_frame_( full_frame_, summary_D )
                        print('All TEXT ..hence saving chunks!')
                        continue

                    summary_ = self.query_fn_.returnDocSummary( frame_, high_variance_cols_ )

                    print( tblidx,' :: ', tbl_, '::', '\n', frame_, '\n LLAMA3: ', summary_  )
                    print('TIMER: self.query_fn_.returnDocSummary :: ', time.time() - tt_)
                    time.sleep(1) ## groq APIs while testing this were timing out like crazy

                    #print('Sending to LLM for summary->', summary_, '\n', summary_transposed_)
                    ## append file name, sheet name
                    print('Time taken for first 2 LLM calls->', time.time() - tt_)
                    summary_D['sample_summary'] = self.file_path.split('/')[-1] + ' ' + sheet_name + ' ' + summary_

                    print('Time Taken->', time.time() - tt_)
                    print('Time taken for last LLM calls->', time.time() - tt_)

                    #summary_D['pandas_dataframe'] = self.convertToPandas( tbl_ )
                    #print('Time taken for pandas calls->', time.time() - tt_)
                    ## now MERGE all the info and push into vector DB
                    self.mergeAndInsert( summary_D )
                    print('TIMER: self.mergeAndInsert :: ', time.time() - tt_)
                except:
                    print( 'EXCPN-> '+self.file_path + ' ' + sheet_name + ' ' + traceback.format_exc() )
                    continue

                self.masterInfo_[ sheet_name ] = summary_D

if __name__ == '__main__':
    files_ = os.listdir( './DATA/' )

    for file_ in files_:
        try:
            if 'Indexing Process' not in file_: continue

            get_meta_ = GenerateXLMetaInfo( './DATA/' + file_ )
            get_meta_.read_excel_file()
        except:
            print('EXCPN2-> File Loader FAIL = '+'./DATA/' + file_)
            print( traceback.format_exc() )
            continue

    '''


CODE_SNIP-> import json, math, sys, traceback, copy, multiprocessing, os
from dateutil import parser
import numpy as np
import openpyxl
from openpyxl.utils import column_index_from_string
import time, random, datetime
import pandas as pd
from sklearn.decomposition import PCA

import query_llama3_via_groq as llama3
import query_gpt_via_groq as openai
import createJsonFeats
import db_utils

def is_date( input_str):
        ## first check for INT and FLOAT since parser foolishly accepts ints
        try:
            _ = int( input_str )
            return None
        except:
            pass

        try:
            _ = float( input_str )
            return None
        except:
            pass

        try:
            return parser.parse(input_str)
        except ValueError:
            return None

def process( colNum, sheet, tbl ):
        dt_counts_ = []

        for rw in range( tbl['START_ROW'], tbl['END_ROW'] ):
                dtVal_ = is_date( str( sheet.cell(row=rw, column=colNum).value ) )

                if dtVal_ is not None : 
                    dt_counts_.append( dtVal_ ) 

        if len( dt_counts_ ) >= ( tbl['END_ROW'] - tbl['START_ROW'] )/2: 
                ## defensive chk to ensure dt counts are high
                print('Dt Col found !', colNum)
                ## sort the values to get range
                sorted_dates_ = sorted( dt_counts_ )
                print('Dt range->', sorted_dates_[0], sorted_dates_[-1] )

                return ( True, sorted_dates_[0].strftime('%B %d, %Y'), sorted_dates_[-1].strftime('%B %d, %Y') )

        return ( False, None, None )

class GenerateXLMetaInfo:
    def __init__(self, file_path, llm='LLAMA'):
        """
        Initialize the GenerateXmlMetaInfo class with the XML file.

        Parameters:
        - xml_file (str): The path to the XML file.
        """
        self.file_path = file_path
        self.masterInfo_ = dict()
        self.llm_framework_ = llm
        self.sheet = None
        self.sklearn_pca_object_ = PCA()
        self.add_ai_summary_to_embedding_ = True
        self.chunk_size_ = 500 ## approx 1024 tokens

        self.sz_of_phrase_, self.unique_str_thresh_, self.number_str_thresh_ = 5, 0.5, 0.4
        self.pca_var_min_contrib, self.feature_contribution_per_thresh_ = 0.5, 0.3
        self.max_tables_per_sheet_ = 10 ## at times a single sheet can have multiple tables
        self.num_rows_to_consider_, self.col_thresh_, self.minElemsInTable, self.max_rows_variance = 4, 0.8, 6, 100
        self.default_max_col_ , self.default_max_row_, self.max_cols_for_review_, \
                self.min_num_distinct_values_, self.max_elements_for_summary_ = 50, 50, 10, 3, 15

        if llm == 'OPENAI':
            self.query_fn_ = openai
        else:
            ## default , llama3 inferred via groq
            self.query_fn_ = llama3

    def find_bounds( self, sheet, max_row, max_col, start_row ,end_row ,start_col ,end_col ):

        # Iterate over rows to find the start and end rows
        start_row_idx_ = 1 if start_row is None else start_row
        start_col_idx_ = 1 if start_col is None else start_col

        ## need to add 2 to max rw and col since max_row of sheet returns the last row with data
        ## and range will stop at max_row - 1 
        for row in range( start_row_idx_ , max_row + 2):
            if all( self.sheet.cell(row=row, column=col).value is None for col in range(1, max_col + 2)):
                if start_row is None:
                    continue  # Skip empty rows before the table
                else:
                    end_row = row - 1
                    break
            elif start_row is None:
                start_row = row

        # Iterate over columns to find the start and end columns
        for col in range( start_col_idx_, max_col + 2):
            #for row in range(start_row, end_row):
            #    print('ROW NUM->', col, ' VALUE: ', sheet.cell(row=row, column=col).value )
            if end_row is None: continue

            if all( self.sheet.cell(row=row, column=col).value is None for row in range(start_row, end_row)):
                if start_col is None:
                    continue  # Skip empty columns before the table
                else:
                    end_col = col - 1
                    break
            elif start_col is None:
                start_col = col


        #print('Found tables between-> start_row, end_row, start_col, end_col = ',\
        #        start_row, end_row, start_col, end_col )

        return start_row, end_row, start_col, end_col

    def is_hdr_row_format( self, tbl_bound, sheet ):

        num_str_cols_ = 0
        for col_ctr in range( tbl_bound['START_COL'], tbl_bound['END_COL'] ):
            if type( self.sheet.cell(row=tbl_bound['START_ROW'], column=col_ctr).value ) == str:
                num_str_cols_ += 1

        if num_str_cols_ < ( tbl_bound['END_COL'] - tbl_bound['START_COL'] ): return False

        return True

    def find_tables(self, sheet):
        ## NOTE -> sheet.max_row and sheet.max_column is NOT WORKING !! NEED TO FIX
        ## default is stop gap
        max_row = sheet.max_row if sheet.max_row is not None else self.default_max_row_
        max_col = sheet.max_column if sheet.max_column is not None else self.default_max_col_
        table_bounds_ = []

        print('KKR->', max_row, max_col)
        timer_ = time.time()
        # Initialize variables to track the bounds
        start_row ,end_row ,start_col ,end_col = None, None, 1, sheet.max_column

        ## do a first pass to find the first table
        start_row, end_row, start_col, end_col = self.find_bounds( sheet, max_row, max_col, start_row ,\
                                                                         end_row ,start_col ,end_col )

        print('Time taken->', time.time() - timer_, start_row, end_row, start_col, end_col)    
        init_end_col = copy.copy( end_col )

        table_bounds_.append( { 'START_ROW': start_row, 'END_ROW': end_row,\
                                    'START_COL': start_col, 'END_COL': end_col } ) 

        ## now iterate from end_row to max_row to find all tables row wise
        while end_row is not None:
            end_row += 2 ## increment by 2 since we need to look ahead and see if any more tables exist !
            ##              if u increment by 1 then u will end up on the same blank line that stopped the prev tbl  
            ## start_row is assigned the value of end_row from above and end_row is made None
            if end_row >= max_row: break

            #print('DUM ROW->', end_row)
            start_row, end_row, start_col, end_col = self.find_bounds( sheet, max_row, max_col, end_row ,\
                                                                             None , None , None )

            if ( start_col is None or end_col is None ) or \
                    ( abs( start_row - end_row )*abs( start_col - end_col ) ) <= self.minElemsInTable: continue    

            table_bounds_.append( { 'START_ROW': start_row, 'END_ROW': end_row,\
                                    'START_COL': start_col, 'END_COL': end_col } ) 

        ## now iterate from end_col to max_col to find all tables cols wise
        while init_end_col is not None:
            init_end_col += 2 ## increment by 1 since we need to look ahead and see if any more tables exist !
            ## start_row is assigned the value of end_row from above and end_row is made None
            if init_end_col >= max_col: break

            #print('DUM COL->', init_end_col)
            start_row, end_row, start_col, end_col = self.find_bounds( sheet, max_row, max_col, None ,\
                                                                             None , init_end_col , None )

            if ( start_col >= end_col ): continue    

            table_bounds_.append( { 'START_ROW': start_row, 'END_ROW': end_row,\
                                    'START_COL': start_col, 'END_COL': end_col } ) 

        ## init star and end col to min and max
        for tab in table_bounds_: tab['START_COL'] = 1; tab['END_COL'] = max_col;
        ## remove dupes
        tmp_, dupe = [], set()

        for idx1, tab1 in enumerate( table_bounds_ ):
            for idx2, tab2 in enumerate( table_bounds_ ):
                if idx1 <= idx2: continue
                if tab2['START_ROW'] >= tab1['START_ROW'] and tab2['END_ROW'] <= tab1['END_ROW']:
                    dupe.add( idx2 )

        for idx, tab in enumerate( table_bounds_ ):
            if idx not in list( dupe ):
                tmp_.append( tab )

        ## blend tables - in case the rows are FPs
        final_resp_ = []
        if len( tmp_ ) > 1:
            last_tbl_ = tmp_[0]
            final_resp_.append( last_tbl_ )
            ## check if the first row is not all STR
            for ctr in range( 1, len( tmp_ ) ):
                if self.is_hdr_row_format( tmp_[ctr], sheet ) == False:
                    ## blend with the last table
                    final_resp_[-1]['END_ROW'] = tmp_[ctr]['END_ROW']
                else:
                    final_resp_.append( table_bounds_ )
        else:
            final_resp_ = tmp_

        return final_resp_[ : min( self.max_tables_per_sheet_, len( final_resp_ ) ) ]

    def findDateRange( self, tbl ):

        colRange_ = list( range( tbl['START_COL'], tbl['END_COL'] ) )

        for col in colRange_:
            ## process was taken out of the class only because multi processing map refused to pickle
            ## a method that was part of the class ..and it proved way slower ..so parallels been removed..lol
            results = process(col, self.sheet, tbl)
            if results[0] is True:
                    return str( results[1] ) +' To '+ str( results[2] )

        return (None, None)

    def findHeaderInfo(self, tbl):
        """
        Find header information from the XL file.
        take the first 2 rows and then to be on the safe side also take the 
        first 2 columns ( in case the col headers are just numbers / % etc and the row contain item name in the first col )
        send it to the LLM for a summary
        ALSO there's no need to take all ROWS and COLS .. some 10-15 elements are more than enough but can be adjustedfor domains that need more
        """

        hdr_row_start_ = self.findHdrRow( tbl )
        row_starter_ = tbl['START_ROW'] if hdr_row_start_ is None else hdr_row_start_

        col_frame_ = ''

        for rw in range( row_starter_ , min( row_starter_ + self.num_rows_to_consider_, tbl['END_ROW'] ) ):
            for col in range( tbl['START_COL'], min( self.max_elements_for_summary_, tbl['END_COL'] + 1 ) ):
                col_frame_ += '\t' + str( self.sheet.cell(row=rw, column=col).value )

            col_frame_ += '\n'

        return col_frame_

    def findHighVarianceColumns(self, start_hdr_row_, sheet, tbl ):
        '''
        iterate through columns that have numeric values and figure out the more important columns
        num of rows - we can restrict it to lets say 1k rows ..should suffice 
        '''
        numeric_frame_, high_var_indices_, hdr_col_names_ = dict(), set(), []
        end_row_ =  min( tbl['START_ROW'] + self.max_rows_variance , tbl['END_ROW'] + 1 )
        ## add 1 to the start row since we dont want to include the header value
        start_row_ = ( tbl['START_ROW'] if start_hdr_row_ is None else start_hdr_row_ ) + 1 

        print('BIGGIE-> start_row_, end_row_ = ', start_row_, end_row_)

        for col_ctr in range( tbl['START_COL'], tbl['END_COL']+1 ):
                hdr_col_names_.append( str( self.sheet.cell(row=start_row_-1, column=col_ctr).value ) )

        try:
            for col_ctr in range( tbl['START_COL'], tbl['END_COL']+1 ):
                col_arr_ = [ 'NA' for x in range( ( end_row_ - start_row_ ) + 1 ) ]

                for idx, row_ctr in enumerate( range( start_row_, end_row_ ) ):
                    col_arr_[ idx ] = str( self.sheet.cell(row=row_ctr, column=col_ctr).value )

                ## standardize the column since PCA better be done on std values
                col_set_ = set( col_arr_ )
                ## convert the variables into unique IDs
                uid = [ list( col_set_ ).index( x ) for x in col_arr_ ]
                max_uid_ = np.max( uid )
                ## normalize the int values
                numeric_frame_[ col_ctr ] = [ x/max_uid_ for x in uid ]

            if len( numeric_frame_.keys() ) > 0:
                ## now transpose the contents of the frame since we want it to retain the shape of a column
                transposed_ = np.transpose( np.asarray( list( numeric_frame_.values() ) ) )
                #print('The val of transposed_->', transposed_)
                ## perform PCA and pick the most high variance columns
                ## the number of components to be picked will be decided by the thresh self.percent_pca_var_
                self.sklearn_pca_object_.fit( transposed_ )
                ## components_loading_ will give you principal component wise contribution of the features
                components_loading_ = self.sklearn_pca_object_.components_
                ## only consider those components that contribute to 90% or whatever threshold level of variance
                relevant_loading_ = components_loading_[0] \
                                    if self.sklearn_pca_object_.explained_variance_ratio_[0] > self.pca_var_min_contrib \
                                    else []

                #print('LOADING AND REL_LOADING->', components_loading_, relevant_loading_)
                key_list_ = list( numeric_frame_.keys() )

                for feat_idx, feat_contribution in enumerate( relevant_loading_ ):
                        if feat_contribution >= self.feature_contribution_per_thresh_: 

                            high_var_indices_.add( hdr_col_names_[ key_list_[ feat_idx ] ] )

                            #print('Adding ', hdr_col_names_[ key_list_[ feat_idx ] ],' As a high variance col')
        except:
            pass

        return list( high_var_indices_ ), hdr_col_names_


    def returnSummary(self, tbl ):
        '''
        take the first few rows to try and generate a coherent summary for the type of the data present
        i am also considering transposing the first few rows to see how different the summary looks
        ALSO maybe limiting the number of columns makes sense
        '''
        frame_, transposed_frame_, start_hdr_row_ = '', '', self.findHdrRow( tbl )

        time_ = time.time()
        high_variance_cols_, hdr_col_names_ = self.findHighVarianceColumns( start_hdr_row_, self.sheet, tbl )
        print('Time taken to find high var cols ->', time.time() - time_)
        print('AND THEY ARE->', high_variance_cols_)

        frame_num_contours_, transposed_frame_contours_ = 0, 0
        ## NATURAL order -> left to right, top to bottom
        for row_ctr in range( tbl['START_ROW'] if start_hdr_row_ is None else start_hdr_row_\
                              , min( tbl['START_ROW']+self.num_rows_to_consider_ , tbl['END_ROW']+1 ) ):

            for col_ctr in range( tbl['START_COL'], min( self.max_elements_for_summary_, tbl['END_COL']+1 ) ):

                frame_ += '\t' + str( self.sheet.cell(row=row_ctr, column=col_ctr).value )
                frame_num_contours_ += 1

            frame_ += '\n'

        return frame_, high_variance_cols_, list( set(hdr_col_names_) )

    def findHdrRow( self, tbl ):

        total_cols_ = tbl['END_COL'] - tbl['START_COL']

        for row_ctr in range( tbl['START_ROW'], \
                              min( tbl['START_ROW']+self.num_rows_to_consider_ , tbl['END_ROW']+1 ) ):
            num_non_blank_ = 0

            for col_ctr in range( tbl['START_COL'], tbl['END_COL'] ):
                if self.sheet.cell(row=row_ctr, column=col_ctr).value is not None and \
                        len( str( self.sheet.cell(row=row_ctr, column=col_ctr).value ) ) > 0: 
                    num_non_blank_ += 1

            ## only if the number of hdr columns is in the ballpark w.r.t. total number of columns
            ## should we start the table ..at times the header table is split across more than 1 row
            if total_cols_ > 1 and (num_non_blank_/total_cols_) > self.col_thresh_:
                return row_ctr

        return None # so default value of row #1 applies to table start

    def createDBRec( self, summary_D, mode='NORM' ):

        insertRec = dict()
        insertRec['docID'] = random.randint( 1000, 100000 )
        ## combine all necessary fields to form vector signature
        ## keys-> 'sample_summary'; 'date_range' ; 'hdr_info'

        hdr_info = summary_D['hdr_info']
        sample_summary_ = summary_D['sample_summary']

        unified_key_ =   'Date Range : '+ str( summary_D['date_range'] ) + '\n' \
                       + 'Column Headers : '+ ' , '.join( summary_D['col_names_'] ).strip() + '\n' \
                       + 'LLM Summary : '+ ( sample_summary_ ) if self.add_ai_summary_to_embedding_ is True else ''

        emb_ = createJsonFeats.returnEmbed( unified_key_ )
        insertRec['docSignature'] = emb_
        insertRec['summary'] = unified_key_
        insertRec['file_path'] = summary_D['file_path']
        insertRec['file_name'] = summary_D['file_path'].split('/')[-1]
        insertRec['sheet_name'] = summary_D['sheet_name']
        insertRec['date_range'] = summary_D['date_range']
        insertRec['hdr_info'] = hdr_info

        print('Inserting RECORD->', insertRec['file_name'], insertRec['sheet_name'], unified_key_ )
        return insertRec

    def mergeAndInsert( self, summary_D ):
        '''
        we shall be inserting 2 records for every table
        a) the normal table structure
        b) the transposed table structure
        along with all meta info
        '''
        ##NORM TBL STRUCT
        rec_ = self.createDBRec( summary_D, 'NORM' )
        db_utils.insertNewSignature( rec_ )

    def returnEntireSheet( self, tbl_, sheet_name ):
        '''
        find if the entire sheet contains mostly textual information. If so, then we should simply
        chunk the whole sheet , after concatenating 
        A simple rule of thumb can be the length of the cell contents in any column.
        If the lenght of the cell contents is greater than some threshold say 10 words
        '''
        use_entire_sheet_, chunks_ = False, []

        for col_ctr in range( tbl_['START_COL'], tbl_['END_COL'] ):
            num_str_, unique_, ignore = 0, set(), False
            for row_ctr in range( tbl_['START_ROW'], tbl_['END_ROW'] ):
                if type( self.sheet.cell(row=row_ctr, column=col_ctr).value ) == str and\
                        len( (self.sheet.cell(row=row_ctr, column=col_ctr).value).split() ) >= self.sz_of_phrase_:
                            num_str_ += 1
                unique_.add( ( self.sheet.cell(row=row_ctr, column=col_ctr).value ) )

            ## if num of unique strings in col is low it means, this value is being repeated
            ## HENCE its mostly observations being selected from a drop down and does NOT need
            ## the entire doc chunked
            if len( unique_ ) < self.unique_str_thresh_*( tbl_['END_ROW'] - tbl_['START_ROW'] ): ignore = True

            print('returnEntireSheet->', sheet_name, tbl_, num_str_, ( tbl_['END_ROW'] - tbl_['START_ROW'] ), ignore)
            if num_str_ >= self.number_str_thresh_*( tbl_['END_ROW'] - tbl_['START_ROW'] ) and ignore is False:

                use_entire_sheet_ = True
                ## aggregate all text and chunk using self.chunk_size_
                frame_ = ''
                for row_ctr in range( tbl_['START_ROW'], tbl_['END_ROW'] ):
                    for col_ctr in range( tbl_['START_COL'], tbl_['END_COL'] ):

                        if len( frame_ ) >= self.chunk_size_:
                            chunks_.append( frame_ )
                            frame_ = ''

                        frame_ += '\t'+ str( self.sheet.cell(row=row_ctr, column=col_ctr).value )
                    frame_ += '\n'

                if len( frame_ ) > 0: chunks_.append( frame_ )

        return chunks_, use_entire_sheet_

    def process_full_frame_( self, full_frame_, summary_D ):

        for chunk in full_frame_:
           summary_D['sample_summary'] = chunk
           self.mergeAndInsert( summary_D )

    def read_excel_file(self):
        # Load the workbook
        main_timer_ = time.time()
        workbook = openpyxl.load_workbook( self.file_path )
        #workbook = openpyxl.load_workbook( self.file_path, read_only=True )
        # Get the specified sheet in the workbook
        summary_D = dict()

        print( ' Time taken to open workbook->', time.time() - main_timer_)
        for sheet_obj in workbook:
            tt_ = time.time()
            self.sheet = sheet_obj
            sheet_name = self.sheet.title
            ## find all tables in the sheet
            #if 'Testing' not in self.sheet.title: continue

            print('Iterating over sheet->', self.sheet.title, self.sheet.max_row)

            all_tables_ = self.find_tables( self.sheet )
            print( 'ALL TABLES in the sheet->', sheet_name, all_tables_)
            print('TIMER: self.find_tables :: ', time.time() - tt_)

            for tblidx, tbl_ in enumerate( all_tables_ ):
                frame_, transposed_frame_ = '', ''
                print('xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx')

                if tbl_['START_ROW'] is None or tbl_['END_ROW'] is None or tbl_['START_COL'] is None\
                        or tbl_['END_COL'] is None:
                            print('The tbl and sheet->', self.sheet.title,' no data!')

                try:
                    frame_, high_variance_cols_, col_names_ = self.returnSummary( tbl_ )
                    print('TIMER: self.returnSummary :: ', time.time() - tt_)
                    full_frame_, is_full_frame_necessary_ = self.returnEntireSheet( tbl_, sheet_name )
                    ## find date range if available
                    summary_D['date_range'] = self.findDateRange( tbl_ )
                    summary_D['file_path'] = self.file_path
                    summary_D['sheet_name'] = sheet_name
                    summary_D['col_names_'] = col_names_
                    ## summarize hdr info
                    hdr_frame_ = self.findHeaderInfo( tbl_ )
                    print('TIMER: self.findHeaderInfo :: ', time.time() - tt_)

                    #summary_D['hdr_info'] = self.query_fn_.augmentHeaderInformation( hdr_frame_ )
                    summary_D['hdr_info'] = self.file_path.split('/')[-1] + ' ' + sheet_name + ' ' + \
                                            hdr_frame_ #+ ' ' + summary_D['hdr_info']

                    print('TIMER: self.findDateRange :: ', time.time() - tt_)

                    if summary_D['date_range'] == ( None, None ):
                        ## just add the timestamp of the file ..backup , BUT better than no time dimension
                        summary_D['date_range'] = \
                            datetime.datetime.fromtimestamp( os.path.getmtime(self.file_path) ).strftime('%B %d, %Y')

                    if is_full_frame_necessary_ == True:
                        self.process_full_frame_( full_frame_, summary_D )
                        print('All TEXT ..hence saving chunks!')
                        continue

                    summary_ = self.query_fn_.returnDocSummary( frame_, high_variance_cols_ )

                    print( tblidx,' :: ', tbl_, '::', '\n', frame_, '\n LLAMA3: ', summary_  )
                    print('TIMER: self.query_fn_.returnDocSummary :: ', time.time() - tt_)
                    time.sleep(1) ## groq APIs while testing this were timing out like crazy

                    #print('Sending to LLM for summary->', summary_, '\n', summary_transposed_)
                    ## append file name, sheet name
                    print('Time taken for first 2 LLM calls->', time.time() - tt_)
                    summary_D['sample_summary'] = self.file_path.split('/')[-1] + ' ' + sheet_name + ' ' + summary_

                    print('Time Taken->', time.time() - tt_)
                    print('Time taken for last LLM calls->', time.time() - tt_)

                    #summary_D['pandas_dataframe'] = self.convertToPandas( tbl_ )
                    #print('Time taken for pandas calls->', time.time() - tt_)
                    ## now MERGE all the info and push into vector DB
                    self.mergeAndInsert( summary_D )
                    print('TIMER: self.mergeAndInsert :: ', time.time() - tt_)
                except:
                    print( 'EXCPN-> '+self.file_path + ' ' + sheet_name + ' ' + traceback.format_exc() )
                    continue

                self.masterInfo_[ sheet_name ] = summary_D

if __name__ == '__main__':
    files_ = os.listdir( './DATA/' )

    for file_ in files_:
        try:
            if 'Indexing Process' not in file_: continue

            get_meta_ = GenerateXLMetaInfo( './DATA/' + file_ )
            get_meta_.read_excel_file()
        except:
            print('EXCPN2-> File Loader FAIL = '+'./DATA/' + file_)
            print( traceback.format_exc() )
            continue

    '''

CODE_SNIP-> import json, math, sys, traceback, copy, multiprocessing, os
from dateutil import parser
import numpy as np
import openpyxl
from openpyxl.utils import column_index_from_string
import time, random, datetime
import pandas as pd
from sklearn.decomposition import PCA

import query_llama3_via_groq as llama3
import query_gpt_via_groq as openai
import createJsonFeats
import db_utils

def is_date( input_str):
        ## first check for INT and FLOAT since parser foolishly accepts ints
        try:
            _ = int( input_str )
            return None
        except:
            pass

        try:
            _ = float( input_str )
            return None
        except:
            pass

        try:
            return parser.parse(input_str)
        except ValueError:
            return None

def process( colNum, sheet, tbl ):
        dt_counts_ = []

        for rw in range( tbl['START_ROW'], tbl['END_ROW'] ):
                dtVal_ = is_date( str( sheet.cell(row=rw, column=colNum).value ) )

                if dtVal_ is not None : 
                    dt_counts_.append( dtVal_ ) 

        if len( dt_counts_ ) >= ( tbl['END_ROW'] - tbl['START_ROW'] )/2: 
                ## defensive chk to ensure dt counts are high
                print('Dt Col found !', colNum)
                ## sort the values to get range
                sorted_dates_ = sorted( dt_counts_ )
                print('Dt range->', sorted_dates_[0], sorted_dates_[-1] )

                return ( True, sorted_dates_[0].strftime('%B %d, %Y'), sorted_dates_[-1].strftime('%B %d, %Y') )

        return ( False, None, None )

class GenerateXLMetaInfo:
    def __init__(self, file_path, llm='LLAMA'):
        """
        Initialize the GenerateXmlMetaInfo class with the XML file.

        Parameters:
        - xml_file (str): The path to the XML file.
        """
        self.file_path = file_path
        self.masterInfo_ = dict()
        self.llm_framework_ = llm
        self.sheet = None
        self.sklearn_pca_object_ = PCA()
        self.add_ai_summary_to_embedding_ = True
        self.chunk_size_ = 500 ## approx 1024 tokens

        self.sz_of_phrase_, self.unique_str_thresh_, self.number_str_thresh_ = 5, 0.5, 0.4
        self.pca_var_min_contrib, self.feature_contribution_per_thresh_ = 0.5, 0.3
        self.max_tables_per_sheet_ = 10 ## at times a single sheet can have multiple tables
        self.num_rows_to_consider_, self.col_thresh_, self.minElemsInTable, self.max_rows_variance = 4, 0.8, 6, 100
        self.default_max_col_ , self.default_max_row_, self.max_cols_for_review_, \
                self.min_num_distinct_values_, self.max_elements_for_summary_ = 50, 50, 10, 3, 15

        if llm == 'OPENAI':
            self.query_fn_ = openai
        else:
            ## default , llama3 inferred via groq
            self.query_fn_ = llama3

    def find_bounds( self, sheet, max_row, max_col, start_row ,end_row ,start_col ,end_col ):

        # Iterate over rows to find the start and end rows
        start_row_idx_ = 1 if start_row is None else start_row
        start_col_idx_ = 1 if start_col is None else start_col

        ## need to add 2 to max rw and col since max_row of sheet returns the last row with data
        ## and range will stop at max_row - 1 
        for row in range( start_row_idx_ , max_row + 2):
            if all( self.sheet.cell(row=row, column=col).value is None for col in range(1, max_col + 2)):
                if start_row is None:
                    continue  # Skip empty rows before the table
                else:
                    end_row = row - 1
                    break
            elif start_row is None:
                start_row = row

        # Iterate over columns to find the start and end columns
        for col in range( start_col_idx_, max_col + 2):
            #for row in range(start_row, end_row):
            #    print('ROW NUM->', col, ' VALUE: ', sheet.cell(row=row, column=col).value )
            if end_row is None: continue

            if all( self.sheet.cell(row=row, column=col).value is None for row in range(start_row, end_row)):
                if start_col is None:
                    continue  # Skip empty columns before the table
                else:
                    end_col = col - 1
                    break
            elif start_col is None:
                start_col = col


        #print('Found tables between-> start_row, end_row, start_col, end_col = ',\
        #        start_row, end_row, start_col, end_col )

        return start_row, end_row, start_col, end_col

    def is_hdr_row_format( self, tbl_bound, sheet ):

        num_str_cols_ = 0
        for col_ctr in range( tbl_bound['START_COL'], tbl_bound['END_COL'] ):
            if type( self.sheet.cell(row=tbl_bound['START_ROW'], column=col_ctr).value ) == str:
                num_str_cols_ += 1

        if num_str_cols_ < ( tbl_bound['END_COL'] - tbl_bound['START_COL'] ): return False

        return True

    def find_tables(self, sheet):
        ## NOTE -> sheet.max_row and sheet.max_column is NOT WORKING !! NEED TO FIX
        ## default is stop gap
        max_row = sheet.max_row if sheet.max_row is not None else self.default_max_row_
        max_col = sheet.max_column if sheet.max_column is not None else self.default_max_col_
        table_bounds_ = []

        print('KKR->', max_row, max_col)
        timer_ = time.time()
        # Initialize variables to track the bounds
        start_row ,end_row ,start_col ,end_col = None, None, 1, sheet.max_column

        ## do a first pass to find the first table
        start_row, end_row, start_col, end_col = self.find_bounds( sheet, max_row, max_col, start_row ,\
                                                                         end_row ,start_col ,end_col )

        print('Time taken->', time.time() - timer_, start_row, end_row, start_col, end_col)    
        init_end_col = copy.copy( end_col )

        table_bounds_.append( { 'START_ROW': start_row, 'END_ROW': end_row,\
                                    'START_COL': start_col, 'END_COL': end_col } ) 

        ## now iterate from end_row to max_row to find all tables row wise
        while end_row is not None:
            end_row += 2 ## increment by 2 since we need to look ahead and see if any more tables exist !
            ##              if u increment by 1 then u will end up on the same blank line that stopped the prev tbl  
            ## start_row is assigned the value of end_row from above and end_row is made None
            if end_row >= max_row: break

            #print('DUM ROW->', end_row)
            start_row, end_row, start_col, end_col = self.find_bounds( sheet, max_row, max_col, end_row ,\
                                                                             None , None , None )

            if ( start_col is None or end_col is None ) or \
                    ( abs( start_row - end_row )*abs( start_col - end_col ) ) <= self.minElemsInTable: continue    

            table_bounds_.append( { 'START_ROW': start_row, 'END_ROW': end_row,\
                                    'START_COL': start_col, 'END_COL': end_col } ) 

        ## now iterate from end_col to max_col to find all tables cols wise
        while init_end_col is not None:
            init_end_col += 2 ## increment by 1 since we need to look ahead and see if any more tables exist !
            ## start_row is assigned the value of end_row from above and end_row is made None
            if init_end_col >= max_col: break

            #print('DUM COL->', init_end_col)
            start_row, end_row, start_col, end_col = self.find_bounds( sheet, max_row, max_col, None ,\
                                                                             None , init_end_col , None )

            if ( start_col >= end_col ): continue    

            table_bounds_.append( { 'START_ROW': start_row, 'END_ROW': end_row,\
                                    'START_COL': start_col, 'END_COL': end_col } ) 

        ## init star and end col to min and max
        for tab in table_bounds_: tab['START_COL'] = 1; tab['END_COL'] = max_col;
        ## remove dupes
        tmp_, dupe = [], set()

        for idx1, tab1 in enumerate( table_bounds_ ):
            for idx2, tab2 in enumerate( table_bounds_ ):
                if idx1 <= idx2: continue
                if tab2['START_ROW'] >= tab1['START_ROW'] and tab2['END_ROW'] <= tab1['END_ROW']:
                    dupe.add( idx2 )

        for idx, tab in enumerate( table_bounds_ ):
            if idx not in list( dupe ):
                tmp_.append( tab )

        ## blend tables - in case the rows are FPs
        final_resp_ = []
        if len( tmp_ ) > 1:
            last_tbl_ = tmp_[0]
            final_resp_.append( last_tbl_ )
            ## check if the first row is not all STR
            for ctr in range( 1, len( tmp_ ) ):
                if self.is_hdr_row_format( tmp_[ctr], sheet ) == False:
                    ## blend with the last table
                    final_resp_[-1]['END_ROW'] = tmp_[ctr]['END_ROW']
                else:
                    final_resp_.append( table_bounds_ )
        else:
            final_resp_ = tmp_

        return final_resp_[ : min( self.max_tables_per_sheet_, len( final_resp_ ) ) ]

    def findDateRange( self, tbl ):

        colRange_ = list( range( tbl['START_COL'], tbl['END_COL'] ) )

        for col in colRange_:
            ## process was taken out of the class only because multi processing map refused to pickle
            ## a method that was part of the class ..and it proved way slower ..so parallels been removed..lol
            results = process(col, self.sheet, tbl)
            if results[0] is True:
                    return str( results[1] ) +' To '+ str( results[2] )

        return (None, None)

    def findHeaderInfo(self, tbl):
        """
        Find header information from the XL file.
        take the first 2 rows and then to be on the safe side also take the 
        first 2 columns ( in case the col headers are just numbers / % etc and the row contain item name in the first col )
        send it to the LLM for a summary
        ALSO there's no need to take all ROWS and COLS .. some 10-15 elements are more than enough but can be adjustedfor domains that need more
        """

        hdr_row_start_ = self.findHdrRow( tbl )
        row_starter_ = tbl['START_ROW'] if hdr_row_start_ is None else hdr_row_start_

        col_frame_ = ''

        for rw in range( row_starter_ , min( row_starter_ + self.num_rows_to_consider_, tbl['END_ROW'] ) ):
            for col in range( tbl['START_COL'], min( self.max_elements_for_summary_, tbl['END_COL'] + 1 ) ):
                col_frame_ += '\t' + str( self.sheet.cell(row=rw, column=col).value )

            col_frame_ += '\n'

        return col_frame_

    def findHighVarianceColumns(self, start_hdr_row_, sheet, tbl ):
        '''
        iterate through columns that have numeric values and figure out the more important columns
        num of rows - we can restrict it to lets say 1k rows ..should suffice 
        '''
        numeric_frame_, high_var_indices_, hdr_col_names_ = dict(), set(), []
        end_row_ =  min( tbl['START_ROW'] + self.max_rows_variance , tbl['END_ROW'] + 1 )
        ## add 1 to the start row since we dont want to include the header value
        start_row_ = ( tbl['START_ROW'] if start_hdr_row_ is None else start_hdr_row_ ) + 1 

        print('BIGGIE-> start_row_, end_row_ = ', start_row_, end_row_)

        for col_ctr in range( tbl['START_COL'], tbl['END_COL']+1 ):
                hdr_col_names_.append( str( self.sheet.cell(row=start_row_-1, column=col_ctr).value ) )

        try:
            for col_ctr in range( tbl['START_COL'], tbl['END_COL']+1 ):
                col_arr_ = [ 'NA' for x in range( ( end_row_ - start_row_ ) + 1 ) ]

                for idx, row_ctr in enumerate( range( start_row_, end_row_ ) ):
                    col_arr_[ idx ] = str( self.sheet.cell(row=row_ctr, column=col_ctr).value )

                ## standardize the column since PCA better be done on std values
                col_set_ = set( col_arr_ )
                ## convert the variables into unique IDs
                uid = [ list( col_set_ ).index( x ) for x in col_arr_ ]
                max_uid_ = np.max( uid )
                ## normalize the int values
                numeric_frame_[ col_ctr ] = [ x/max_uid_ for x in uid ]

            if len( numeric_frame_.keys() ) > 0:
                ## now transpose the contents of the frame since we want it to retain the shape of a column
                transposed_ = np.transpose( np.asarray( list( numeric_frame_.values() ) ) )
                #print('The val of transposed_->', transposed_)
                ## perform PCA and pick the most high variance columns
                ## the number of components to be picked will be decided by the thresh self.percent_pca_var_
                self.sklearn_pca_object_.fit( transposed_ )
                ## components_loading_ will give you principal component wise contribution of the features
                components_loading_ = self.sklearn_pca_object_.components_
                ## only consider those components that contribute to 90% or whatever threshold level of variance
                relevant_loading_ = components_loading_[0] \
                                    if self.sklearn_pca_object_.explained_variance_ratio_[0] > self.pca_var_min_contrib \
                                    else []

                #print('LOADING AND REL_LOADING->', components_loading_, relevant_loading_)
                key_list_ = list( numeric_frame_.keys() )

                for feat_idx, feat_contribution in enumerate( relevant_loading_ ):
                        if feat_contribution >= self.feature_contribution_per_thresh_: 

                            high_var_indices_.add( hdr_col_names_[ key_list_[ feat_idx ] ] )

                            #print('Adding ', hdr_col_names_[ key_list_[ feat_idx ] ],' As a high variance col')
        except:
            pass

        return list( high_var_indices_ ), hdr_col_names_


    def returnSummary(self, tbl ):
        '''
        take the first few rows to try and generate a coherent summary for the type of the data present
        i am also considering transposing the first few rows to see how different the summary looks
        ALSO maybe limiting the number of columns makes sense
        '''
        frame_, transposed_frame_, start_hdr_row_ = '', '', self.findHdrRow( tbl )

        time_ = time.time()
        high_variance_cols_, hdr_col_names_ = self.findHighVarianceColumns( start_hdr_row_, self.sheet, tbl )
        print('Time taken to find high var cols ->', time.time() - time_)
        print('AND THEY ARE->', high_variance_cols_)

        frame_num_contours_, transposed_frame_contours_ = 0, 0
        ## NATURAL order -> left to right, top to bottom
        for row_ctr in range( tbl['START_ROW'] if start_hdr_row_ is None else start_hdr_row_\
                              , min( tbl['START_ROW']+self.num_rows_to_consider_ , tbl['END_ROW']+1 ) ):

            for col_ctr in range( tbl['START_COL'], min( self.max_elements_for_summary_, tbl['END_COL']+1 ) ):

                frame_ += '\t' + str( self.sheet.cell(row=row_ctr, column=col_ctr).value )
                frame_num_contours_ += 1

            frame_ += '\n'

        return frame_, high_variance_cols_, list( set(hdr_col_names_) )

    def findHdrRow( self, tbl ):

        total_cols_ = tbl['END_COL'] - tbl['START_COL']

        for row_ctr in range( tbl['START_ROW'], \
                              min( tbl['START_ROW']+self.num_rows_to_consider_ , tbl['END_ROW']+1 ) ):
            num_non_blank_ = 0

            for col_ctr in range( tbl['START_COL'], tbl['END_COL'] ):
                if self.sheet.cell(row=row_ctr, column=col_ctr).value is not None and \
                        len( str( self.sheet.cell(row=row_ctr, column=col_ctr).value ) ) > 0: 
                    num_non_blank_ += 1

            ## only if the number of hdr columns is in the ballpark w.r.t. total number of columns
            ## should we start the table ..at times the header table is split across more than 1 row
            if total_cols_ > 1 and (num_non_blank_/total_cols_) > self.col_thresh_:
                return row_ctr

        return None # so default value of row #1 applies to table start

    def createDBRec( self, summary_D, mode='NORM' ):

        insertRec = dict()
        insertRec['docID'] = random.randint( 1000, 100000 )
        ## combine all necessary fields to form vector signature
        ## keys-> 'sample_summary'; 'date_range' ; 'hdr_info'

        hdr_info = summary_D['hdr_info']
        sample_summary_ = summary_D['sample_summary']

        unified_key_ =   'Date Range : '+ str( summary_D['date_range'] ) + '\n' \
                       + 'Column Headers : '+ ' , '.join( summary_D['col_names_'] ).strip() + '\n' \
                       + 'LLM Summary : '+ ( sample_summary_ ) if self.add_ai_summary_to_embedding_ is True else ''

        emb_ = createJsonFeats.returnEmbed( unified_key_ )
        insertRec['docSignature'] = emb_
        insertRec['summary'] = unified_key_
        insertRec['file_path'] = summary_D['file_path']
        insertRec['file_name'] = summary_D['file_path'].split('/')[-1]
        insertRec['sheet_name'] = summary_D['sheet_name']
        insertRec['date_range'] = summary_D['date_range']
        insertRec['hdr_info'] = hdr_info

        print('Inserting RECORD->', insertRec['file_name'], insertRec['sheet_name'], unified_key_ )
        return insertRec

    def mergeAndInsert( self, summary_D ):
        '''
        we shall be inserting 2 records for every table
        a) the normal table structure
        b) the transposed table structure
        along with all meta info
        '''
        ##NORM TBL STRUCT
        rec_ = self.createDBRec( summary_D, 'NORM' )
        db_utils.insertNewSignature( rec_ )

    def returnEntireSheet( self, tbl_, sheet_name ):
        '''
        find if the entire sheet contains mostly textual information. If so, then we should simply
        chunk the whole sheet , after concatenating 
        A simple rule of thumb can be the length of the cell contents in any column.
        If the lenght of the cell contents is greater than some threshold say 10 words
        '''
        use_entire_sheet_, chunks_ = False, []

        for col_ctr in range( tbl_['START_COL'], tbl_['END_COL'] ):
            num_str_, unique_, ignore = 0, set(), False
            for row_ctr in range( tbl_['START_ROW'], tbl_['END_ROW'] ):
                if type( self.sheet.cell(row=row_ctr, column=col_ctr).value ) == str and\
                        len( (self.sheet.cell(row=row_ctr, column=col_ctr).value).split() ) >= self.sz_of_phrase_:
                            num_str_ += 1
                unique_.add( ( self.sheet.cell(row=row_ctr, column=col_ctr).value ) )

            ## if num of unique strings in col is low it means, this value is being repeated
            ## HENCE its mostly observations being selected from a drop down and does NOT need
            ## the entire doc chunked
            if len( unique_ ) < self.unique_str_thresh_*( tbl_['END_ROW'] - tbl_['START_ROW'] ): ignore = True

            print('returnEntireSheet->', sheet_name, tbl_, num_str_, ( tbl_['END_ROW'] - tbl_['START_ROW'] ), ignore)
            if num_str_ >= self.number_str_thresh_*( tbl_['END_ROW'] - tbl_['START_ROW'] ) and ignore is False:

                use_entire_sheet_ = True
                ## aggregate all text and chunk using self.chunk_size_
                frame_ = ''
                for row_ctr in range( tbl_['START_ROW'], tbl_['END_ROW'] ):
                    for col_ctr in range( tbl_['START_COL'], tbl_['END_COL'] ):

                        if len( frame_ ) >= self.chunk_size_:
                            chunks_.append( frame_ )
                            frame_ = ''

                        frame_ += '\t'+ str( self.sheet.cell(row=row_ctr, column=col_ctr).value )
                    frame_ += '\n'

                if len( frame_ ) > 0: chunks_.append( frame_ )

        return chunks_, use_entire_sheet_

    def process_full_frame_( self, full_frame_, summary_D ):

        for chunk in full_frame_:
           summary_D['sample_summary'] = chunk
           self.mergeAndInsert( summary_D )

    def read_excel_file(self):
        # Load the workbook
        main_timer_ = time.time()
        workbook = openpyxl.load_workbook( self.file_path )
        #workbook = openpyxl.load_workbook( self.file_path, read_only=True )
        # Get the specified sheet in the workbook
        summary_D = dict()

        print( ' Time taken to open workbook->', time.time() - main_timer_)
        for sheet_obj in workbook:
            tt_ = time.time()
            self.sheet = sheet_obj
            sheet_name = self.sheet.title
            ## find all tables in the sheet
            #if 'Testing' not in self.sheet.title: continue

            print('Iterating over sheet->', self.sheet.title, self.sheet.max_row)

            all_tables_ = self.find_tables( self.sheet )
            print( 'ALL TABLES in the sheet->', sheet_name, all_tables_)
            print('TIMER: self.find_tables :: ', time.time() - tt_)

            for tblidx, tbl_ in enumerate( all_tables_ ):
                frame_, transposed_frame_ = '', ''
                print('xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx')

                if tbl_['START_ROW'] is None or tbl_['END_ROW'] is None or tbl_['START_COL'] is None\
                        or tbl_['END_COL'] is None:
                            print('The tbl and sheet->', self.sheet.title,' no data!')

                try:
                    frame_, high_variance_cols_, col_names_ = self.returnSummary( tbl_ )
                    print('TIMER: self.returnSummary :: ', time.time() - tt_)
                    full_frame_, is_full_frame_necessary_ = self.returnEntireSheet( tbl_, sheet_name )
                    ## find date range if available
                    summary_D['date_range'] = self.findDateRange( tbl_ )
                    summary_D['file_path'] = self.file_path
                    summary_D['sheet_name'] = sheet_name
                    summary_D['col_names_'] = col_names_
                    ## summarize hdr info
                    hdr_frame_ = self.findHeaderInfo( tbl_ )
                    print('TIMER: self.findHeaderInfo :: ', time.time() - tt_)

                    #summary_D['hdr_info'] = self.query_fn_.augmentHeaderInformation( hdr_frame_ )
                    summary_D['hdr_info'] = self.file_path.split('/')[-1] + ' ' + sheet_name + ' ' + \
                                            hdr_frame_ #+ ' ' + summary_D['hdr_info']

                    print('TIMER: self.findDateRange :: ', time.time() - tt_)

                    if summary_D['date_range'] == ( None, None ):
                        ## just add the timestamp of the file ..backup , BUT better than no time dimension
                        summary_D['date_range'] = \
                            datetime.datetime.fromtimestamp( os.path.getmtime(self.file_path) ).strftime('%B %d, %Y')

                    if is_full_frame_necessary_ == True:
                        self.process_full_frame_( full_frame_, summary_D )
                        print('All TEXT ..hence saving chunks!')
                        continue

                    summary_ = self.query_fn_.returnDocSummary( frame_, high_variance_cols_ )

                    print( tblidx,' :: ', tbl_, '::', '\n', frame_, '\n LLAMA3: ', summary_  )
                    print('TIMER: self.query_fn_.returnDocSummary :: ', time.time() - tt_)
                    time.sleep(1) ## groq APIs while testing this were timing out like crazy

                    #print('Sending to LLM for summary->', summary_, '\n', summary_transposed_)
                    ## append file name, sheet name
                    print('Time taken for first 2 LLM calls->', time.time() - tt_)
                    summary_D['sample_summary'] = self.file_path.split('/')[-1] + ' ' + sheet_name + ' ' + summary_

                    print('Time Taken->', time.time() - tt_)
                    print('Time taken for last LLM calls->', time.time() - tt_)

                    #summary_D['pandas_dataframe'] = self.convertToPandas( tbl_ )
                    #print('Time taken for pandas calls->', time.time() - tt_)
                    ## now MERGE all the info and push into vector DB
                    self.mergeAndInsert( summary_D )
                    print('TIMER: self.mergeAndInsert :: ', time.time() - tt_)
                except:
                    print( 'EXCPN-> '+self.file_path + ' ' + sheet_name + ' ' + traceback.format_exc() )
                    continue

                self.masterInfo_[ sheet_name ] = summary_D

if __name__ == '__main__':
    files_ = os.listdir( './DATA/' )

    for file_ in files_:
        try:
            if 'Indexing Process' not in file_: continue

            get_meta_ = GenerateXLMetaInfo( './DATA/' + file_ )
            get_meta_.read_excel_file()
        except:
            print('EXCPN2-> File Loader FAIL = '+'./DATA/' + file_)
            print( traceback.format_exc() )
            continue


CODE_SNIP-> import numpy as np
import json, sys, os

def chunking_test(a, b):
    x = a*b
    y = call_func( a, b )
    zz = x**2
    y = zz*x
    abc = 123
    def_ = 456
    final_ = call_def( y )
    ghi = final_

    for st_, nm_ in common_wds_1:
        locdist_ = []
        for st_1, nm_1 in common_wds_1:
            locdist_.append( distance.euclidean( nm_, nm_1 ) )

        distm1.append( locdist_ )

    ffg_ = ghi
    for st_, nm_ in common_wds_2:
        locdist_ = []
        for st_1, nm_1 in common_wds_2:
            locdist_.append( distance.euclidean( nm_, nm_1 ) )

        distm2.append( locdist_ )

    ## now calc prime eigenvectors
    eigenvalues1, eigenvectors1 = eig( distm1 )
    idx = np.argsort(eigenvalues1)[::-1]
    print( eigenvalues1[idx][:5] )
    print( eigenvectors1[:, idx][:, :1] )

CODE_SNIP-> import testChunking as tc

def downstream_antics( x, y ):
    ## search the db now

    dbRec_ = db_utils.returnBlankDBRec()
    dbRec_['docID'] = fnm
    dbRec_['docSignature'] = encoded_
    dbRec_['tupArr'] = key_coord_tup_
    global debug_sign_

    results_ = db_utils.searchSignature( dbRec_ )['searchRes_']
    matching_recs_, closest_match, self_rec, all_matches = [], None, None, dict()
    #print('DREDD->', docs_used_)
    #print('Whats the hit ?-?', results_)
    highest_match_score_ = 0
    x = tc.chunking_test( 10, 20 )

    insertD = dict()
    insertD[ 'config_field_nm' ] = keyNm
    insertD[ 'local_field' ] = feedback_local_key_dict
    insertD[ 'feedback_value' ] = feedback_value_
    insertD[ 'feedback_co_ords' ] = feedback_co_ords
    insertD[ 'comments' ] = comments
    y = x**3
    insertD[ 'config_field_nm' ] = keyNm
    insertD[ 'local_field' ] = feedback_local_key_dict
    insertD[ 'feedback_value' ] = feedback_value_
    insertD[ 'feedback_co_ords' ] = feedback_co_ords
    fgh_ = doSomething( y )

    if ( len( txt1.split() ) == 1 and fuzzr is not None and fuzzr >= 80 ) or \
            ( len( txt2.split() ) == 1 and fuzzr is not None and fuzzr >= 80 ): 
        #print('ALTHOUGHT SIZE 1, high fuzz ration->', txt1, txt2, fuzzr) 
        return True
    if ( len( txt1.split() ) == 1 and fuzzr is not None and fuzzr >= 80 ) or \
            ( len( txt2.split() ) == 1 and fuzzr is not None and fuzzr >= 80 ): 
        #print('ALTHOUGHT SIZE 1, high fuzz ration->', txt1, txt2, fuzzr) 
        return True


CODE_SNIP-> import json
import numpy as np
import findKeys
from scipy.spatial import distance
from scipy.linalg import eig
import urllib.request

import sys, os

# Disable
def blockPrint():
    sys.stdout = open(os.devnull, 'w')

# Restore
def enablePrint():
    sys.stdout = sys.__stdout__


#from sentence_transformers import SentenceTransformer

#encoder = SentenceTransformer('all-MiniLM-L6-v2')

#url_encode = 'http://20.235.122.20:5000/encodeSentence'
url_encode = 'http://0.0.0.0:5200/encodeSentence'

#def testCodeQl():
#    return None

def returnEmbed( sent ):

    rec_ = { 'sentence': sent }

    data = json.dumps( rec_ ).encode('utf-8')
    _request = urllib.request.Request( url_encode, data=data, method='POST', \
                                        headers={'Content-Type': 'application/json'} )

    response = urllib.request.urlopen( _request )
    string = response.read().decode('utf-8')
    json_obj = json.loads(string)

    return json_obj['encoded_'], True

def returnJsonFeat( src_0, src_raw ):

    with open( src_0, 'r' ) as fp:
    #with open( src_0 + file_, 'r' ) as fp:
        json_ = json.load( fp )

    with open( src_raw, 'r' ) as fp:
    #with open( src_raw + file_, 'r' ) as fp:
        json_raw = json.load( fp )

    blockPrint()    

    ## dummy
    file_ = ''

    key_tuple_ = findKeys.processNeighbours( json_, json_raw, file_ )    

    enablePrint()

    doc_str_, dist_matrix_, xymatrix = '', [], []

    for str_, norm_coords in key_tuple_:
        doc_str_ += ' ' + str_

    #print( doc_str_ )
    #print( np.asarray( dist_matrix_ ).shape, dist_matrix_[0] )
    #return doc_str_, xymatrix
    rec_ = { 'sentence': doc_str_ }

    data = json.dumps( rec_ ).encode('utf-8')
    _request = urllib.request.Request( url_encode, data=data, method='POST', headers={'Content-Type': 'application/json'} )
    response = urllib.request.urlopen( _request )
    string = response.read().decode('utf-8')
    json_obj = json.loads(string)

    return json_obj['encoded_'], key_tuple_

if __name__ == '__main__':

    import sys
    from scipy import linalg


    '''
    fnm_, fnm2 = sys.argv[1], sys.argv[2]

    doc_str_, key_tup1 = returnJsonFeat( fnm_ )

    doc_str_2, key_tup2 = returnJsonFeat( fnm2 )

    print( 'Euclide->', distance.cosine( doc_str_, doc_str_2 ) )
    common_wds_1, common_wds_2, distm1, distm2 = [], [], [], []

    for str1, norm1 in key_tup1:
        for str2, norm2 in key_tup2:
            if str1 == str2:# and distance.cosine( norm1, norm2 ) <= 0.01:
                common_wds_1.append( (str1, norm1) )
                common_wds_2.append( (str2, norm2) )

    for st_, nm_ in common_wds_1:
        locdist_ = []
        for st_1, nm_1 in common_wds_1:
            locdist_.append( distance.euclidean( nm_, nm_1 ) )

        distm1.append( locdist_ )    

    for st_, nm_ in common_wds_2:
        locdist_ = []
        for st_1, nm_1 in common_wds_2:
            locdist_.append( distance.euclidean( nm_, nm_1 ) )

        distm2.append( locdist_ )  

    ## now calc prime eigenvectors    
    eigenvalues1, eigenvectors1 = eig( distm1 )
    idx = np.argsort(eigenvalues1)[::-1]
    print( eigenvalues1[idx][:5] )
    print( eigenvectors1[:, idx][:, :1] )

    eigenvalues2, eigenvectors2 = eig( distm2 )
    idx = np.argsort(eigenvalues2)[::-1]
    print( eigenvalues2[idx][:5] )
    print( eigenvectors2[:, idx][:, :1] )

    print( distance.cosine( eigenvectors1[:, idx][:, 0], eigenvectors2[:, idx][:, 0] ) )
    print( distance.cosine( eigenvalues1, eigenvalues2 ) )
    '''

Graph INput data generated!!
