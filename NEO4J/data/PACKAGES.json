{
    "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/searchDB.py": [
        {
            "numpy": 3
        },
        {
            "np": {
                "def": 3,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/searchDB.py",
                        "method_nm": "def pos( res_ ):",
                        "usage": [
                            "        print('--------', keyid, np.asarray( sorted_doc_score_ )[:3, :1])    \n",
                            "        if [keyid] not in np.asarray( sorted_doc_score_ )[:3, :1]: continue\n",
                            "        print('--------', keyid, np.asarray( sorted_doc_score_1 )[:3, :1])    \n",
                            "        if [keyid] not in np.asarray( sorted_doc_score_1 )[:3, :1]: continue\n",
                            "        print('--------', keyid, np.asarray( sorted_doc_score_2 )[:3, :1])    \n",
                            "        if [keyid] not in np.asarray( sorted_doc_score_2 )[:3, :1]: continue\n",
                            "        print('--------', keyid, np.asarray( sorted_doc_score_3 )[:3, :1])    \n",
                            "        if [keyid] not in np.asarray( sorted_doc_score_3 )[:3, :1]: continue\n",
                            "        print('--------', keyid, np.asarray( sorted_doc_score_4 )[:3, :1])    \n",
                            "        if [keyid] not in np.asarray( sorted_doc_score_4 )[:3, :1]: continue\n"
                        ]
                    }
                ]
            }
        }
    ],
    "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/addtoDB.py": [
        {
            "createJsonFeats": {
                "def": 0,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/addtoDB.py",
                        "method_nm": "def addToDB():",
                        "usage": [
                            "            emb_ = createJsonFeats.returnEmbed( txt )\n"
                        ]
                    }
                ]
            }
        },
        {
            "os": 1
        },
        {
            "json": 1
        },
        {
            "sys": 1
        },
        {
            "traceback": 1
        },
        {
            "db_utils": {
                "def": 1,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/addtoDB.py",
                        "method_nm": "def addToDB():",
                        "usage": [
                            "            db_utils.insertNewSignature( dd_ )\n"
                        ]
                    }
                ]
            }
        }
    ],
    "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py": [
        {
            "json": 0
        },
        {
            "math": 0
        },
        {
            "sys": 0
        },
        {
            "traceback": 0
        },
        {
            "copy": {
                "def": 0,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def find_tables(self, sheet)",
                        "usage": [
                            "        init_end_col = copy.copy( end_col )\n"
                        ]
                    }
                ]
            }
        },
        {
            "multiprocessing": 0
        },
        {
            "os": {
                "def": 0,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def findHighVarianceColumns( self, start_hdr_row_, sheet, tbl )",
                        "usage": [
                            "                ## now transpose the contents of the frame since we want it to retain the shape of a column\n",
                            "                transposed_ = np.transpose( np.asarray( list( numeric_frame_.values() ) ) )\n",
                            "                #print('The val of transposed_->', transposed_)\n",
                            "                ## perform PCA and pick the most high variance columns\n",
                            "                self.sklearn_pca_object_.fit( transposed_ )\n",
                            "                ## only consider those components that contribute to 90% or whatever threshold level of variance\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def returnSummary( self, tbl_ ):",
                        "usage": [
                            "        i am also considering transposing the first few rows to see how different the summary looks\n",
                            "        frame_, transposed_frame_, start_hdr_row_ = '', '', self.findHdrRow( tbl )\n",
                            "        frame_num_contours_, transposed_frame_contours_ = 0, 0\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def findHdrRow( self, tbl )",
                        "usage": [
                            "            ## should we start the table ..at times the header table is split across more than 1 row\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def mergeAndInsert( self, summary_D ):",
                        "usage": [
                            "        b) the transposed table structure\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def returnEntireSheet( self, tbl_, sheet_name ):",
                        "usage": [
                            "        find if the entire sheet contains mostly textual information. If so, then we should simply\n",
                            "            ## HENCE its mostly observations being selected from a drop down and does NOT need\n"
                        ]
                    }
                ]
            }
        },
        {
            "dateutil.parser": 18
        },
        {
            "parser": {
                "def": 18,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def is_date( input_str):",
                        "usage": [
                            "        ## first check for INT and FLOAT since parser foolishly accepts ints\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def findHeaderInfo( self, tbl_ ):",
                        "usage": [
                            "            return parser.parse(input_str)\n"
                        ]
                    }
                ]
            }
        },
        {
            "numpy": 2
        },
        {
            "np": {
                "def": 2,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def is_date( input_str):",
                        "usage": [
                            "def is_date( input_str):\n",
                            "            _ = int( input_str )\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def findHeaderInfo( self, tbl_ ):",
                        "usage": [
                            "            _ = int( input_str )\n",
                            "            _ = float( input_str )\n",
                            "            return parser.parse(input_str)\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def findHighVarianceColumns( self, start_hdr_row_, sheet, tbl )",
                        "usage": [
                            "                max_uid_ = np.max( uid )\n",
                            "                transposed_ = np.transpose( np.asarray( list( numeric_frame_.values() ) ) )\n"
                        ]
                    }
                ]
            }
        },
        {
            "openpyxl": 3
        },
        {
            "openpyxl.utils": 4
        },
        {
            "column_index_from_string": 4
        },
        {
            "time": {
                "def": 5,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def process( colNum, sheet, tbl ):",
                        "usage": [
                            "                return ( True, sorted_dates_[0].strftime('%B %d, %Y'), sorted_dates_[-1].strftime('%B %d, %Y') )\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def __init__(self, file_path, llm='LLAMA'):",
                        "usage": [
                            "        self.max_tables_per_sheet_ = 10 ## at times a single sheet can have multiple tables\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def find_tables(self, sheet)",
                        "usage": [
                            "        timer_ = time.time()\n",
                            "        print('Time taken->', time.time() - timer_, start_row, end_row, start_col, end_col)    \n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def returnSummary( self, tbl_ ):",
                        "usage": [
                            "        time_ = time.time()\n",
                            "        print('Time taken to find high var cols ->', time.time() - time_)\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def findHdrRow( self, tbl )",
                        "usage": [
                            "            ## should we start the table ..at times the header table is split across more than 1 row\n"
                        ]
                    }
                ]
            }
        },
        {
            "random": {
                "def": 5,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def createDBRec( self, summary_D, mode='NORM' ):",
                        "usage": [
                            "        insertRec['docID'] = random.randint( 1000, 100000 )\n"
                        ]
                    }
                ]
            }
        },
        {
            "datetime": 5
        },
        {
            "pandas": 6
        },
        {
            "pd": 6
        },
        {
            "sklearn.decomposition": 7
        },
        {
            "PCA": {
                "def": 7,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def __init__(self, file_path, llm='LLAMA'):",
                        "usage": [
                            "        self.sklearn_pca_object_ = PCA()\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def findHighVarianceColumns( self, start_hdr_row_, sheet, tbl )",
                        "usage": [
                            "                ## standardize the column since PCA better be done on std values\n",
                            "                ## perform PCA and pick the most high variance columns\n"
                        ]
                    }
                ]
            }
        },
        {
            "query_llama3_via_groq": 9
        },
        {
            "llama3": {
                "def": 9,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def __init__(self, file_path, llm='LLAMA'):",
                        "usage": [
                            "            ## default , llama3 inferred via groq\n",
                            "            self.query_fn_ = llama3\n"
                        ]
                    }
                ]
            }
        },
        {
            "query_gpt_via_groq": 10
        },
        {
            "openai": {
                "def": 10,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def __init__(self, file_path, llm='LLAMA'):",
                        "usage": [
                            "            self.query_fn_ = openai\n"
                        ]
                    }
                ]
            }
        },
        {
            "createJsonFeats": {
                "def": 11,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def createDBRec( self, summary_D, mode='NORM' ):",
                        "usage": [
                            "        emb_ = createJsonFeats.returnEmbed( unified_key_ )\n"
                        ]
                    }
                ]
            }
        },
        {
            "db_utils": {
                "def": 12,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def mergeAndInsert( self, summary_D ):",
                        "usage": [
                            "        db_utils.insertNewSignature( rec_ )\n"
                        ]
                    }
                ]
            }
        },
        {
            "timer_": {
                "def": 5,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def find_tables(self, sheet)",
                        "usage": [
                            "        timer_ = time.time()\n",
                            "        print('Time taken->', time.time() - timer_, start_row, end_row, start_col, end_col)    \n"
                        ]
                    }
                ]
            }
        },
        {
            "": {
                "def": 3,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def is_date( input_str):",
                        "usage": [
                            "\n",
                            "def is_date( input_str):\n",
                            "        ## first check for INT and FLOAT since parser foolishly accepts ints\n",
                            "        try:\n",
                            "            _ = int( input_str )\n",
                            "            return None\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def findHeaderInfo( self, tbl_ ):",
                        "usage": [
                            "            _ = int( input_str )\n",
                            "            return None\n",
                            "        except:\n",
                            "            pass\n",
                            "\n",
                            "        try:\n",
                            "            _ = float( input_str )\n",
                            "            return None\n",
                            "        except:\n",
                            "            pass\n",
                            "\n",
                            "        try:\n",
                            "            return parser.parse(input_str)\n",
                            "        except ValueError:\n",
                            "            return None\n",
                            "\n",
                            "def process( colNum, sheet, tbl ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def process( colNum, sheet, tbl ):",
                        "usage": [
                            "\n",
                            "def process( colNum, sheet, tbl ):\n",
                            "        dt_counts_ = []\n",
                            "\n",
                            "        for rw in range( tbl['START_ROW'], tbl['END_ROW'] ):\n",
                            "                dtVal_ = is_date( str( sheet.cell(row=rw, column=colNum).value ) )\n",
                            "\n",
                            "                if dtVal_ is not None : \n",
                            "                    dt_counts_.append( dtVal_ ) \n",
                            "\n",
                            "        if len( dt_counts_ ) >= ( tbl['END_ROW'] - tbl['START_ROW'] )/2: \n",
                            "                ## defensive chk to ensure dt counts are high\n",
                            "                print('Dt Col found !', colNum)\n",
                            "                ## sort the values to get range\n",
                            "                sorted_dates_ = sorted( dt_counts_ )\n",
                            "                print('Dt range->', sorted_dates_[0], sorted_dates_[-1] )\n",
                            "\n",
                            "                return ( True, sorted_dates_[0].strftime('%B %d, %Y'), sorted_dates_[-1].strftime('%B %d, %Y') )\n",
                            "\n",
                            "        return ( False, None, None )\n",
                            "\n",
                            "class GenerateXLMetaInfo:\n",
                            "    def __init__(self, file_path, llm='LLAMA'):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def __init__(self, file_path, llm='LLAMA'):",
                        "usage": [
                            "class GenerateXLMetaInfo:\n",
                            "    def __init__(self, file_path, llm='LLAMA'):\n",
                            "        \"\"\"\n",
                            "        Initialize the GenerateXmlMetaInfo class with the XML file.\n",
                            "\n",
                            "        Parameters:\n",
                            "        - xml_file (str): The path to the XML file.\n",
                            "        \"\"\"\n",
                            "        self.file_path = file_path\n",
                            "        self.masterInfo_ = dict()\n",
                            "        self.llm_framework_ = llm\n",
                            "        self.sheet = None\n",
                            "        self.sklearn_pca_object_ = PCA()\n",
                            "        self.add_ai_summary_to_embedding_ = True\n",
                            "        self.chunk_size_ = 500 ## approx 1024 tokens\n",
                            "\n",
                            "        self.sz_of_phrase_, self.unique_str_thresh_, self.number_str_thresh_ = 5, 0.5, 0.4\n",
                            "        self.pca_var_min_contrib, self.feature_contribution_per_thresh_ = 0.5, 0.3\n",
                            "        self.max_tables_per_sheet_ = 10 ## at times a single sheet can have multiple tables\n",
                            "        self.num_rows_to_consider_, self.col_thresh_, self.minElemsInTable, self.max_rows_variance = 4, 0.8, 6, 100\n",
                            "        self.default_max_col_ , self.default_max_row_, self.max_cols_for_review_, \\\n",
                            "                self.min_num_distinct_values_, self.max_elements_for_summary_ = 50, 50, 10, 3, 15\n",
                            "\n",
                            "        if llm == 'OPENAI':\n",
                            "            self.query_fn_ = openai\n",
                            "        else:\n",
                            "            ## default , llama3 inferred via groq\n",
                            "            self.query_fn_ = llama3\n",
                            "   \n",
                            "    def find_bounds( self, sheet, max_row, max_col, start_row ,end_row ,start_col ,end_col ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def find_bounds( self, sheet, max_row, max_col, start_row , end_row ,start_col ,end_col )",
                        "usage": [
                            "   \n",
                            "    def find_bounds( self, sheet, max_row, max_col, start_row ,end_row ,start_col ,end_col ):\n",
                            "\n",
                            "        # Iterate over rows to find the start and end rows\n",
                            "        start_row_idx_ = 1 if start_row is None else start_row\n",
                            "        start_col_idx_ = 1 if start_col is None else start_col\n",
                            "        \n",
                            "        ## need to add 2 to max rw and col since max_row of sheet returns the last row with data\n",
                            "        ## and range will stop at max_row - 1 \n",
                            "        for row in range( start_row_idx_ , max_row + 2):\n",
                            "            if all( self.sheet.cell(row=row, column=col).value is None for col in range(1, max_col + 2)):\n",
                            "                if start_row is None:\n",
                            "                    continue  # Skip empty rows before the table\n",
                            "                else:\n",
                            "                    end_row = row - 1\n",
                            "                    break\n",
                            "            elif start_row is None:\n",
                            "                start_row = row\n",
                            "           \n",
                            "        # Iterate over columns to find the start and end columns\n",
                            "        for col in range( start_col_idx_, max_col + 2):\n",
                            "            #for row in range(start_row, end_row):\n",
                            "            #    print('ROW NUM->', col, ' VALUE: ', sheet.cell(row=row, column=col).value )\n",
                            "            if end_row is None: continue\n",
                            "\n",
                            "            if all( self.sheet.cell(row=row, column=col).value is None for row in range(start_row, end_row)):\n",
                            "                if start_col is None:\n",
                            "                    continue  # Skip empty columns before the table\n",
                            "                else:\n",
                            "                    end_col = col - 1\n",
                            "                    break\n",
                            "            elif start_col is None:\n",
                            "                start_col = col\n",
                            "            \n",
                            "\n",
                            "        #print('Found tables between-> start_row, end_row, start_col, end_col = ',\\\n",
                            "        #        start_row, end_row, start_col, end_col )\n",
                            "\n",
                            "        return start_row, end_row, start_col, end_col\n",
                            "\n",
                            "    def is_hdr_row_format( self, tbl_bound, sheet ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def is_hdr_row_format( self, tbl_bound, sheet )",
                        "usage": [
                            "\n",
                            "    def is_hdr_row_format( self, tbl_bound, sheet ):\n",
                            "        \n",
                            "        num_str_cols_ = 0\n",
                            "        for col_ctr in range( tbl_bound['START_COL'], tbl_bound['END_COL'] ):\n",
                            "            if type( self.sheet.cell(row=tbl_bound['START_ROW'], column=col_ctr).value ) == str:\n",
                            "                num_str_cols_ += 1\n",
                            "\n",
                            "        if num_str_cols_ < ( tbl_bound['END_COL'] - tbl_bound['START_COL'] ): return False\n",
                            "\n",
                            "        return True\n",
                            "\n",
                            "    def find_tables(self, sheet):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def find_tables(self, sheet)",
                        "usage": [
                            "\n",
                            "    def find_tables(self, sheet):\n",
                            "        ## NOTE -> sheet.max_row and sheet.max_column is NOT WORKING !! NEED TO FIX\n",
                            "        ## default is stop gap\n",
                            "        max_row = sheet.max_row if sheet.max_row is not None else self.default_max_row_\n",
                            "        max_col = sheet.max_column if sheet.max_column is not None else self.default_max_col_\n",
                            "        table_bounds_ = []\n",
                            "\n",
                            "        print('KKR->', max_row, max_col)\n",
                            "        timer_ = time.time()\n",
                            "        # Initialize variables to track the bounds\n",
                            "        start_row ,end_row ,start_col ,end_col = None, None, 1, sheet.max_column\n",
                            "\n",
                            "        ## do a first pass to find the first table\n",
                            "        start_row, end_row, start_col, end_col = self.find_bounds( sheet, max_row, max_col, start_row ,\\\n",
                            "                                                                         end_row ,start_col ,end_col )\n",
                            "\n",
                            "        print('Time taken->', time.time() - timer_, start_row, end_row, start_col, end_col)    \n",
                            "        init_end_col = copy.copy( end_col )\n",
                            "\n",
                            "        table_bounds_.append( { 'START_ROW': start_row, 'END_ROW': end_row,\\\n",
                            "                                    'START_COL': start_col, 'END_COL': end_col } ) \n",
                            "\n",
                            "        ## now iterate from end_row to max_row to find all tables row wise\n",
                            "        while end_row is not None:\n",
                            "            end_row += 2 ## increment by 2 since we need to look ahead and see if any more tables exist !\n",
                            "            ##              if u increment by 1 then u will end up on the same blank line that stopped the prev tbl  \n",
                            "            ## start_row is assigned the value of end_row from above and end_row is made None\n",
                            "            if end_row >= max_row: break\n",
                            "\n",
                            "            #print('DUM ROW->', end_row)\n",
                            "            start_row, end_row, start_col, end_col = self.find_bounds( sheet, max_row, max_col, end_row ,\\\n",
                            "                                                                             None , None , None )\n",
                            "\n",
                            "            if ( start_col is None or end_col is None ) or \\\n",
                            "                    ( abs( start_row - end_row )*abs( start_col - end_col ) ) <= self.minElemsInTable: continue    \n",
                            "\n",
                            "            table_bounds_.append( { 'START_ROW': start_row, 'END_ROW': end_row,\\\n",
                            "                                    'START_COL': start_col, 'END_COL': end_col } ) \n",
                            "        \n",
                            "        ## now iterate from end_col to max_col to find all tables cols wise\n",
                            "        while init_end_col is not None:\n",
                            "            init_end_col += 2 ## increment by 1 since we need to look ahead and see if any more tables exist !\n",
                            "            ## start_row is assigned the value of end_row from above and end_row is made None\n",
                            "            if init_end_col >= max_col: break\n",
                            "\n",
                            "            #print('DUM COL->', init_end_col)\n",
                            "            start_row, end_row, start_col, end_col = self.find_bounds( sheet, max_row, max_col, None ,\\\n",
                            "                                                                             None , init_end_col , None )\n",
                            "\n",
                            "            if ( start_col >= end_col ): continue    \n",
                            "\n",
                            "            table_bounds_.append( { 'START_ROW': start_row, 'END_ROW': end_row,\\\n",
                            "                                    'START_COL': start_col, 'END_COL': end_col } ) \n",
                            "\n",
                            "        ## init star and end col to min and max\n",
                            "        for tab in table_bounds_: tab['START_COL'] = 1; tab['END_COL'] = max_col;\n",
                            "        ## remove dupes\n",
                            "        tmp_, dupe = [], set()\n",
                            "\n",
                            "        for idx1, tab1 in enumerate( table_bounds_ ):\n",
                            "            for idx2, tab2 in enumerate( table_bounds_ ):\n",
                            "                if idx1 <= idx2: continue\n",
                            "                if tab2['START_ROW'] >= tab1['START_ROW'] and tab2['END_ROW'] <= tab1['END_ROW']:\n",
                            "                    dupe.add( idx2 )\n",
                            "\n",
                            "        for idx, tab in enumerate( table_bounds_ ):\n",
                            "            if idx not in list( dupe ):\n",
                            "                tmp_.append( tab )\n",
                            "\n",
                            "        ## blend tables - in case the rows are FPs\n",
                            "        final_resp_ = []\n",
                            "        if len( tmp_ ) > 1:\n",
                            "            last_tbl_ = tmp_[0]\n",
                            "            final_resp_.append( last_tbl_ )\n",
                            "            ## check if the first row is not all STR\n",
                            "            for ctr in range( 1, len( tmp_ ) ):\n",
                            "                if self.is_hdr_row_format( tmp_[ctr], sheet ) == False:\n",
                            "                    ## blend with the last table\n",
                            "                    final_resp_[-1]['END_ROW'] = tmp_[ctr]['END_ROW']\n",
                            "                else:\n",
                            "                    final_resp_.append( table_bounds_ )\n",
                            "        else:\n",
                            "            final_resp_ = tmp_\n",
                            "\n",
                            "        return final_resp_[ : min( self.max_tables_per_sheet_, len( final_resp_ ) ) ]\n",
                            "\n",
                            "    def findDateRange( self, tbl ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def findDateRange( self, tbl_ ):",
                        "usage": [
                            "\n",
                            "    def findDateRange( self, tbl ):\n",
                            "\n",
                            "        colRange_ = list( range( tbl['START_COL'], tbl['END_COL'] ) )\n",
                            "\n",
                            "        for col in colRange_:\n",
                            "            ## process was taken out of the class only because multi processing map refused to pickle\n",
                            "            ## a method that was part of the class ..and it proved way slower ..so parallels been removed..lol\n",
                            "            results = process(col, self.sheet, tbl)\n",
                            "            if results[0] is True:\n",
                            "                    return str( results[1] ) +' To '+ str( results[2] )\n",
                            "\n",
                            "        return (None, None)\n",
                            "\n",
                            "    def findHeaderInfo(self, tbl):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def findHeaderInfo(self, tbl):",
                        "usage": [
                            "\n",
                            "    def findHeaderInfo(self, tbl):\n",
                            "        \"\"\"\n",
                            "        Find header information from the XL file.\n",
                            "        take the first 2 rows and then to be on the safe side also take the \n",
                            "        first 2 columns ( in case the col headers are just numbers / % etc and the row contain item name in the first col )\n",
                            "        send it to the LLM for a summary\n",
                            "        ALSO there's no need to take all ROWS and COLS .. some 10-15 elements are more than enough but can be adjustedfor domains that need more\n",
                            "        \"\"\"\n",
                            "\n",
                            "        hdr_row_start_ = self.findHdrRow( tbl )\n",
                            "        row_starter_ = tbl['START_ROW'] if hdr_row_start_ is None else hdr_row_start_\n",
                            "\n",
                            "        col_frame_ = ''\n",
                            "\n",
                            "        for rw in range( row_starter_ , min( row_starter_ + self.num_rows_to_consider_, tbl['END_ROW'] ) ):\n",
                            "            for col in range( tbl['START_COL'], min( self.max_elements_for_summary_, tbl['END_COL'] + 1 ) ):\n",
                            "                col_frame_ += '\\t' + str( self.sheet.cell(row=rw, column=col).value )\n",
                            "\n",
                            "            col_frame_ += '\\n'\n",
                            "\n",
                            "        return col_frame_\n",
                            "\n",
                            "    def findHighVarianceColumns(self, start_hdr_row_, sheet, tbl ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def findHighVarianceColumns( self, start_hdr_row_, sheet, tbl )",
                        "usage": [
                            "\n",
                            "    def findHighVarianceColumns(self, start_hdr_row_, sheet, tbl ):\n",
                            "        '''\n",
                            "        iterate through columns that have numeric values and figure out the more important columns\n",
                            "        num of rows - we can restrict it to lets say 1k rows ..should suffice \n",
                            "        '''\n",
                            "        numeric_frame_, high_var_indices_, hdr_col_names_ = dict(), set(), []\n",
                            "        end_row_ =  min( tbl['START_ROW'] + self.max_rows_variance , tbl['END_ROW'] + 1 )\n",
                            "        ## add 1 to the start row since we dont want to include the header value\n",
                            "        start_row_ = ( tbl['START_ROW'] if start_hdr_row_ is None else start_hdr_row_ ) + 1 \n",
                            "        \n",
                            "        print('BIGGIE-> start_row_, end_row_ = ', start_row_, end_row_)\n",
                            "\n",
                            "        for col_ctr in range( tbl['START_COL'], tbl['END_COL']+1 ):\n",
                            "                hdr_col_names_.append( str( self.sheet.cell(row=start_row_-1, column=col_ctr).value ) )\n",
                            "\n",
                            "        try:\n",
                            "            for col_ctr in range( tbl['START_COL'], tbl['END_COL']+1 ):\n",
                            "                col_arr_ = [ 'NA' for x in range( ( end_row_ - start_row_ ) + 1 ) ]\n",
                            "\n",
                            "                for idx, row_ctr in enumerate( range( start_row_, end_row_ ) ):\n",
                            "                    col_arr_[ idx ] = str( self.sheet.cell(row=row_ctr, column=col_ctr).value )\n",
                            "\n",
                            "                ## standardize the column since PCA better be done on std values\n",
                            "                col_set_ = set( col_arr_ )\n",
                            "                ## convert the variables into unique IDs\n",
                            "                uid = [ list( col_set_ ).index( x ) for x in col_arr_ ]\n",
                            "                max_uid_ = np.max( uid )\n",
                            "                ## normalize the int values\n",
                            "                numeric_frame_[ col_ctr ] = [ x/max_uid_ for x in uid ]\n",
                            "\n",
                            "            if len( numeric_frame_.keys() ) > 0:\n",
                            "                ## now transpose the contents of the frame since we want it to retain the shape of a column\n",
                            "                transposed_ = np.transpose( np.asarray( list( numeric_frame_.values() ) ) )\n",
                            "                #print('The val of transposed_->', transposed_)\n",
                            "                ## perform PCA and pick the most high variance columns\n",
                            "                ## the number of components to be picked will be decided by the thresh self.percent_pca_var_\n",
                            "                self.sklearn_pca_object_.fit( transposed_ )\n",
                            "                ## components_loading_ will give you principal component wise contribution of the features\n",
                            "                components_loading_ = self.sklearn_pca_object_.components_\n",
                            "                ## only consider those components that contribute to 90% or whatever threshold level of variance\n",
                            "                relevant_loading_ = components_loading_[0] \\\n",
                            "                                    if self.sklearn_pca_object_.explained_variance_ratio_[0] > self.pca_var_min_contrib \\\n",
                            "                                    else []\n",
                            "\n",
                            "                #print('LOADING AND REL_LOADING->', components_loading_, relevant_loading_)\n",
                            "                key_list_ = list( numeric_frame_.keys() )\n",
                            "\n",
                            "                for feat_idx, feat_contribution in enumerate( relevant_loading_ ):\n",
                            "                        if feat_contribution >= self.feature_contribution_per_thresh_: \n",
                            "\n",
                            "                            high_var_indices_.add( hdr_col_names_[ key_list_[ feat_idx ] ] )\n",
                            "\n",
                            "                            #print('Adding ', hdr_col_names_[ key_list_[ feat_idx ] ],' As a high variance col')\n",
                            "        except:\n",
                            "            pass\n",
                            "\n",
                            "        return list( high_var_indices_ ), hdr_col_names_\n",
                            "\n",
                            "\n",
                            "    def returnSummary(self, tbl ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def returnSummary( self, tbl_ ):",
                        "usage": [
                            "\n",
                            "    def returnSummary(self, tbl ):\n",
                            "        '''\n",
                            "        take the first few rows to try and generate a coherent summary for the type of the data present\n",
                            "        i am also considering transposing the first few rows to see how different the summary looks\n",
                            "        ALSO maybe limiting the number of columns makes sense\n",
                            "        '''\n",
                            "        frame_, transposed_frame_, start_hdr_row_ = '', '', self.findHdrRow( tbl )\n",
                            "        \n",
                            "        time_ = time.time()\n",
                            "        high_variance_cols_, hdr_col_names_ = self.findHighVarianceColumns( start_hdr_row_, self.sheet, tbl )\n",
                            "        print('Time taken to find high var cols ->', time.time() - time_)\n",
                            "        print('AND THEY ARE->', high_variance_cols_)\n",
                            "\n",
                            "        frame_num_contours_, transposed_frame_contours_ = 0, 0\n",
                            "        ## NATURAL order -> left to right, top to bottom\n",
                            "        for row_ctr in range( tbl['START_ROW'] if start_hdr_row_ is None else start_hdr_row_\\\n",
                            "                              , min( tbl['START_ROW']+self.num_rows_to_consider_ , tbl['END_ROW']+1 ) ):\n",
                            "\n",
                            "            for col_ctr in range( tbl['START_COL'], min( self.max_elements_for_summary_, tbl['END_COL']+1 ) ):\n",
                            "\n",
                            "                frame_ += '\\t' + str( self.sheet.cell(row=row_ctr, column=col_ctr).value )\n",
                            "                frame_num_contours_ += 1\n",
                            "\n",
                            "            frame_ += '\\n'\n",
                            "\n",
                            "        return frame_, high_variance_cols_, list( set(hdr_col_names_) )\n",
                            "\n",
                            "    def findHdrRow( self, tbl ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def findHdrRow( self, tbl )",
                        "usage": [
                            "\n",
                            "    def findHdrRow( self, tbl ):\n",
                            "\n",
                            "        total_cols_ = tbl['END_COL'] - tbl['START_COL']\n",
                            "\n",
                            "        for row_ctr in range( tbl['START_ROW'], \\\n",
                            "                              min( tbl['START_ROW']+self.num_rows_to_consider_ , tbl['END_ROW']+1 ) ):\n",
                            "            num_non_blank_ = 0\n",
                            "            \n",
                            "            for col_ctr in range( tbl['START_COL'], tbl['END_COL'] ):\n",
                            "                if self.sheet.cell(row=row_ctr, column=col_ctr).value is not None and \\\n",
                            "                        len( str( self.sheet.cell(row=row_ctr, column=col_ctr).value ) ) > 0: \n",
                            "                    num_non_blank_ += 1\n",
                            "\n",
                            "            ## only if the number of hdr columns is in the ballpark w.r.t. total number of columns\n",
                            "            ## should we start the table ..at times the header table is split across more than 1 row\n",
                            "            if total_cols_ > 1 and (num_non_blank_/total_cols_) > self.col_thresh_:\n",
                            "                return row_ctr\n",
                            "\n",
                            "        return None # so default value of row #1 applies to table start\n",
                            "    \n",
                            "    def createDBRec( self, summary_D, mode='NORM' ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def createDBRec( self, summary_D, mode='NORM' ):",
                        "usage": [
                            "    \n",
                            "    def createDBRec( self, summary_D, mode='NORM' ):\n",
                            "\n",
                            "        insertRec = dict()\n",
                            "        insertRec['docID'] = random.randint( 1000, 100000 )\n",
                            "        ## combine all necessary fields to form vector signature\n",
                            "        ## keys-> 'sample_summary'; 'date_range' ; 'hdr_info'\n",
                            "\n",
                            "        hdr_info = summary_D['hdr_info']\n",
                            "        sample_summary_ = summary_D['sample_summary']\n",
                            "\n",
                            "        unified_key_ =   'Date Range : '+ str( summary_D['date_range'] ) + '\\n' \\\n",
                            "                       + 'Column Headers : '+ ' , '.join( summary_D['col_names_'] ).strip() + '\\n' \\\n",
                            "                       + 'LLM Summary : '+ ( sample_summary_ ) if self.add_ai_summary_to_embedding_ is True else ''\n",
                            "\n",
                            "        emb_ = createJsonFeats.returnEmbed( unified_key_ )\n",
                            "        insertRec['docSignature'] = emb_\n",
                            "        insertRec['summary'] = unified_key_\n",
                            "        insertRec['file_path'] = summary_D['file_path']\n",
                            "        insertRec['file_name'] = summary_D['file_path'].split('/')[-1]\n",
                            "        insertRec['sheet_name'] = summary_D['sheet_name']\n",
                            "        insertRec['date_range'] = summary_D['date_range']\n",
                            "        insertRec['hdr_info'] = hdr_info\n",
                            "\n",
                            "        print('Inserting RECORD->', insertRec['file_name'], insertRec['sheet_name'], unified_key_ )\n",
                            "        return insertRec\n",
                            "\n",
                            "    def mergeAndInsert( self, summary_D ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def mergeAndInsert( self, summary_D ):",
                        "usage": [
                            "\n",
                            "    def mergeAndInsert( self, summary_D ):\n",
                            "        '''\n",
                            "        we shall be inserting 2 records for every table\n",
                            "        a) the normal table structure\n",
                            "        b) the transposed table structure\n",
                            "        along with all meta info\n",
                            "        '''\n",
                            "        ##NORM TBL STRUCT\n",
                            "        rec_ = self.createDBRec( summary_D, 'NORM' )\n",
                            "        db_utils.insertNewSignature( rec_ )\n",
                            "\n",
                            "    def returnEntireSheet( self, tbl_, sheet_name ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def returnEntireSheet( self, tbl_, sheet_name ):",
                        "usage": [
                            "\n",
                            "    def returnEntireSheet( self, tbl_, sheet_name ):\n",
                            "        '''\n",
                            "        find if the entire sheet contains mostly textual information. If so, then we should simply\n",
                            "        chunk the whole sheet , after concatenating \n",
                            "        A simple rule of thumb can be the length of the cell contents in any column.\n",
                            "        If the lenght of the cell contents is greater than some threshold say 10 words\n",
                            "        '''\n",
                            "        use_entire_sheet_, chunks_ = False, []\n",
                            "\n",
                            "        for col_ctr in range( tbl_['START_COL'], tbl_['END_COL'] ):\n",
                            "            num_str_, unique_, ignore = 0, set(), False\n",
                            "            for row_ctr in range( tbl_['START_ROW'], tbl_['END_ROW'] ):\n",
                            "                if type( self.sheet.cell(row=row_ctr, column=col_ctr).value ) == str and\\\n",
                            "                        len( (self.sheet.cell(row=row_ctr, column=col_ctr).value).split() ) >= self.sz_of_phrase_:\n",
                            "                            num_str_ += 1\n",
                            "                unique_.add( ( self.sheet.cell(row=row_ctr, column=col_ctr).value ) )\n",
                            "\n",
                            "            ## if num of unique strings in col is low it means, this value is being repeated\n",
                            "            ## HENCE its mostly observations being selected from a drop down and does NOT need\n",
                            "            ## the entire doc chunked\n",
                            "            if len( unique_ ) < self.unique_str_thresh_*( tbl_['END_ROW'] - tbl_['START_ROW'] ): ignore = True\n",
                            "            \n",
                            "            print('returnEntireSheet->', sheet_name, tbl_, num_str_, ( tbl_['END_ROW'] - tbl_['START_ROW'] ), ignore)\n",
                            "            if num_str_ >= self.number_str_thresh_*( tbl_['END_ROW'] - tbl_['START_ROW'] ) and ignore is False:\n",
                            "\n",
                            "                use_entire_sheet_ = True\n",
                            "                ## aggregate all text and chunk using self.chunk_size_\n",
                            "                frame_ = ''\n",
                            "                for row_ctr in range( tbl_['START_ROW'], tbl_['END_ROW'] ):\n",
                            "                    for col_ctr in range( tbl_['START_COL'], tbl_['END_COL'] ):\n",
                            "                        \n",
                            "                        if len( frame_ ) >= self.chunk_size_:\n",
                            "                            chunks_.append( frame_ )\n",
                            "                            frame_ = ''\n",
                            "\n",
                            "                        frame_ += '\\t'+ str( self.sheet.cell(row=row_ctr, column=col_ctr).value )\n",
                            "                    frame_ += '\\n'\n",
                            "\n",
                            "                if len( frame_ ) > 0: chunks_.append( frame_ )\n",
                            "\n",
                            "        return chunks_, use_entire_sheet_\n",
                            "\n",
                            "    def process_full_frame_( self, full_frame_, summary_D ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def process_full_frame_( self, full_frame_, summary_D ):",
                        "usage": [
                            "\n",
                            "    def process_full_frame_( self, full_frame_, summary_D ):\n",
                            "\n",
                            "        for chunk in full_frame_:\n",
                            "           summary_D['sample_summary'] = chunk\n",
                            "           self.mergeAndInsert( summary_D )\n",
                            "        \n",
                            "    def read_excel_file(self):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def process_full_frame_( self, full_frame_, summary_D ):",
                        "usage": [
                            "\n",
                            "    def process_full_frame_( self, full_frame_, summary_D ):\n",
                            "\n",
                            "        for chunk in full_frame_:\n",
                            "           summary_D['sample_summary'] = chunk\n",
                            "           self.mergeAndInsert( summary_D )\n",
                            "        \n",
                            "    def read_excel_file(self):\n"
                        ]
                    }
                ]
            }
        },
        {
            "": {
                "def": 5,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def is_date( input_str):",
                        "usage": [
                            "\n",
                            "def is_date( input_str):\n",
                            "        ## first check for INT and FLOAT since parser foolishly accepts ints\n",
                            "        try:\n",
                            "            _ = int( input_str )\n",
                            "            return None\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def findHeaderInfo( self, tbl_ ):",
                        "usage": [
                            "            _ = int( input_str )\n",
                            "            return None\n",
                            "        except:\n",
                            "            pass\n",
                            "\n",
                            "        try:\n",
                            "            _ = float( input_str )\n",
                            "            return None\n",
                            "        except:\n",
                            "            pass\n",
                            "\n",
                            "        try:\n",
                            "            return parser.parse(input_str)\n",
                            "        except ValueError:\n",
                            "            return None\n",
                            "\n",
                            "def process( colNum, sheet, tbl ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def process( colNum, sheet, tbl ):",
                        "usage": [
                            "\n",
                            "def process( colNum, sheet, tbl ):\n",
                            "        dt_counts_ = []\n",
                            "\n",
                            "        for rw in range( tbl['START_ROW'], tbl['END_ROW'] ):\n",
                            "                dtVal_ = is_date( str( sheet.cell(row=rw, column=colNum).value ) )\n",
                            "\n",
                            "                if dtVal_ is not None : \n",
                            "                    dt_counts_.append( dtVal_ ) \n",
                            "\n",
                            "        if len( dt_counts_ ) >= ( tbl['END_ROW'] - tbl['START_ROW'] )/2: \n",
                            "                ## defensive chk to ensure dt counts are high\n",
                            "                print('Dt Col found !', colNum)\n",
                            "                ## sort the values to get range\n",
                            "                sorted_dates_ = sorted( dt_counts_ )\n",
                            "                print('Dt range->', sorted_dates_[0], sorted_dates_[-1] )\n",
                            "\n",
                            "                return ( True, sorted_dates_[0].strftime('%B %d, %Y'), sorted_dates_[-1].strftime('%B %d, %Y') )\n",
                            "\n",
                            "        return ( False, None, None )\n",
                            "\n",
                            "class GenerateXLMetaInfo:\n",
                            "    def __init__(self, file_path, llm='LLAMA'):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def __init__(self, file_path, llm='LLAMA'):",
                        "usage": [
                            "class GenerateXLMetaInfo:\n",
                            "    def __init__(self, file_path, llm='LLAMA'):\n",
                            "        \"\"\"\n",
                            "        Initialize the GenerateXmlMetaInfo class with the XML file.\n",
                            "\n",
                            "        Parameters:\n",
                            "        - xml_file (str): The path to the XML file.\n",
                            "        \"\"\"\n",
                            "        self.file_path = file_path\n",
                            "        self.masterInfo_ = dict()\n",
                            "        self.llm_framework_ = llm\n",
                            "        self.sheet = None\n",
                            "        self.sklearn_pca_object_ = PCA()\n",
                            "        self.add_ai_summary_to_embedding_ = True\n",
                            "        self.chunk_size_ = 500 ## approx 1024 tokens\n",
                            "\n",
                            "        self.sz_of_phrase_, self.unique_str_thresh_, self.number_str_thresh_ = 5, 0.5, 0.4\n",
                            "        self.pca_var_min_contrib, self.feature_contribution_per_thresh_ = 0.5, 0.3\n",
                            "        self.max_tables_per_sheet_ = 10 ## at times a single sheet can have multiple tables\n",
                            "        self.num_rows_to_consider_, self.col_thresh_, self.minElemsInTable, self.max_rows_variance = 4, 0.8, 6, 100\n",
                            "        self.default_max_col_ , self.default_max_row_, self.max_cols_for_review_, \\\n",
                            "                self.min_num_distinct_values_, self.max_elements_for_summary_ = 50, 50, 10, 3, 15\n",
                            "\n",
                            "        if llm == 'OPENAI':\n",
                            "            self.query_fn_ = openai\n",
                            "        else:\n",
                            "            ## default , llama3 inferred via groq\n",
                            "            self.query_fn_ = llama3\n",
                            "   \n",
                            "    def find_bounds( self, sheet, max_row, max_col, start_row ,end_row ,start_col ,end_col ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def find_bounds( self, sheet, max_row, max_col, start_row , end_row ,start_col ,end_col )",
                        "usage": [
                            "   \n",
                            "    def find_bounds( self, sheet, max_row, max_col, start_row ,end_row ,start_col ,end_col ):\n",
                            "\n",
                            "        # Iterate over rows to find the start and end rows\n",
                            "        start_row_idx_ = 1 if start_row is None else start_row\n",
                            "        start_col_idx_ = 1 if start_col is None else start_col\n",
                            "        \n",
                            "        ## need to add 2 to max rw and col since max_row of sheet returns the last row with data\n",
                            "        ## and range will stop at max_row - 1 \n",
                            "        for row in range( start_row_idx_ , max_row + 2):\n",
                            "            if all( self.sheet.cell(row=row, column=col).value is None for col in range(1, max_col + 2)):\n",
                            "                if start_row is None:\n",
                            "                    continue  # Skip empty rows before the table\n",
                            "                else:\n",
                            "                    end_row = row - 1\n",
                            "                    break\n",
                            "            elif start_row is None:\n",
                            "                start_row = row\n",
                            "           \n",
                            "        # Iterate over columns to find the start and end columns\n",
                            "        for col in range( start_col_idx_, max_col + 2):\n",
                            "            #for row in range(start_row, end_row):\n",
                            "            #    print('ROW NUM->', col, ' VALUE: ', sheet.cell(row=row, column=col).value )\n",
                            "            if end_row is None: continue\n",
                            "\n",
                            "            if all( self.sheet.cell(row=row, column=col).value is None for row in range(start_row, end_row)):\n",
                            "                if start_col is None:\n",
                            "                    continue  # Skip empty columns before the table\n",
                            "                else:\n",
                            "                    end_col = col - 1\n",
                            "                    break\n",
                            "            elif start_col is None:\n",
                            "                start_col = col\n",
                            "            \n",
                            "\n",
                            "        #print('Found tables between-> start_row, end_row, start_col, end_col = ',\\\n",
                            "        #        start_row, end_row, start_col, end_col )\n",
                            "\n",
                            "        return start_row, end_row, start_col, end_col\n",
                            "\n",
                            "    def is_hdr_row_format( self, tbl_bound, sheet ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def is_hdr_row_format( self, tbl_bound, sheet )",
                        "usage": [
                            "\n",
                            "    def is_hdr_row_format( self, tbl_bound, sheet ):\n",
                            "        \n",
                            "        num_str_cols_ = 0\n",
                            "        for col_ctr in range( tbl_bound['START_COL'], tbl_bound['END_COL'] ):\n",
                            "            if type( self.sheet.cell(row=tbl_bound['START_ROW'], column=col_ctr).value ) == str:\n",
                            "                num_str_cols_ += 1\n",
                            "\n",
                            "        if num_str_cols_ < ( tbl_bound['END_COL'] - tbl_bound['START_COL'] ): return False\n",
                            "\n",
                            "        return True\n",
                            "\n",
                            "    def find_tables(self, sheet):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def find_tables(self, sheet)",
                        "usage": [
                            "\n",
                            "    def find_tables(self, sheet):\n",
                            "        ## NOTE -> sheet.max_row and sheet.max_column is NOT WORKING !! NEED TO FIX\n",
                            "        ## default is stop gap\n",
                            "        max_row = sheet.max_row if sheet.max_row is not None else self.default_max_row_\n",
                            "        max_col = sheet.max_column if sheet.max_column is not None else self.default_max_col_\n",
                            "        table_bounds_ = []\n",
                            "\n",
                            "        print('KKR->', max_row, max_col)\n",
                            "        timer_ = time.time()\n",
                            "        # Initialize variables to track the bounds\n",
                            "        start_row ,end_row ,start_col ,end_col = None, None, 1, sheet.max_column\n",
                            "\n",
                            "        ## do a first pass to find the first table\n",
                            "        start_row, end_row, start_col, end_col = self.find_bounds( sheet, max_row, max_col, start_row ,\\\n",
                            "                                                                         end_row ,start_col ,end_col )\n",
                            "\n",
                            "        print('Time taken->', time.time() - timer_, start_row, end_row, start_col, end_col)    \n",
                            "        init_end_col = copy.copy( end_col )\n",
                            "\n",
                            "        table_bounds_.append( { 'START_ROW': start_row, 'END_ROW': end_row,\\\n",
                            "                                    'START_COL': start_col, 'END_COL': end_col } ) \n",
                            "\n",
                            "        ## now iterate from end_row to max_row to find all tables row wise\n",
                            "        while end_row is not None:\n",
                            "            end_row += 2 ## increment by 2 since we need to look ahead and see if any more tables exist !\n",
                            "            ##              if u increment by 1 then u will end up on the same blank line that stopped the prev tbl  \n",
                            "            ## start_row is assigned the value of end_row from above and end_row is made None\n",
                            "            if end_row >= max_row: break\n",
                            "\n",
                            "            #print('DUM ROW->', end_row)\n",
                            "            start_row, end_row, start_col, end_col = self.find_bounds( sheet, max_row, max_col, end_row ,\\\n",
                            "                                                                             None , None , None )\n",
                            "\n",
                            "            if ( start_col is None or end_col is None ) or \\\n",
                            "                    ( abs( start_row - end_row )*abs( start_col - end_col ) ) <= self.minElemsInTable: continue    \n",
                            "\n",
                            "            table_bounds_.append( { 'START_ROW': start_row, 'END_ROW': end_row,\\\n",
                            "                                    'START_COL': start_col, 'END_COL': end_col } ) \n",
                            "        \n",
                            "        ## now iterate from end_col to max_col to find all tables cols wise\n",
                            "        while init_end_col is not None:\n",
                            "            init_end_col += 2 ## increment by 1 since we need to look ahead and see if any more tables exist !\n",
                            "            ## start_row is assigned the value of end_row from above and end_row is made None\n",
                            "            if init_end_col >= max_col: break\n",
                            "\n",
                            "            #print('DUM COL->', init_end_col)\n",
                            "            start_row, end_row, start_col, end_col = self.find_bounds( sheet, max_row, max_col, None ,\\\n",
                            "                                                                             None , init_end_col , None )\n",
                            "\n",
                            "            if ( start_col >= end_col ): continue    \n",
                            "\n",
                            "            table_bounds_.append( { 'START_ROW': start_row, 'END_ROW': end_row,\\\n",
                            "                                    'START_COL': start_col, 'END_COL': end_col } ) \n",
                            "\n",
                            "        ## init star and end col to min and max\n",
                            "        for tab in table_bounds_: tab['START_COL'] = 1; tab['END_COL'] = max_col;\n",
                            "        ## remove dupes\n",
                            "        tmp_, dupe = [], set()\n",
                            "\n",
                            "        for idx1, tab1 in enumerate( table_bounds_ ):\n",
                            "            for idx2, tab2 in enumerate( table_bounds_ ):\n",
                            "                if idx1 <= idx2: continue\n",
                            "                if tab2['START_ROW'] >= tab1['START_ROW'] and tab2['END_ROW'] <= tab1['END_ROW']:\n",
                            "                    dupe.add( idx2 )\n",
                            "\n",
                            "        for idx, tab in enumerate( table_bounds_ ):\n",
                            "            if idx not in list( dupe ):\n",
                            "                tmp_.append( tab )\n",
                            "\n",
                            "        ## blend tables - in case the rows are FPs\n",
                            "        final_resp_ = []\n",
                            "        if len( tmp_ ) > 1:\n",
                            "            last_tbl_ = tmp_[0]\n",
                            "            final_resp_.append( last_tbl_ )\n",
                            "            ## check if the first row is not all STR\n",
                            "            for ctr in range( 1, len( tmp_ ) ):\n",
                            "                if self.is_hdr_row_format( tmp_[ctr], sheet ) == False:\n",
                            "                    ## blend with the last table\n",
                            "                    final_resp_[-1]['END_ROW'] = tmp_[ctr]['END_ROW']\n",
                            "                else:\n",
                            "                    final_resp_.append( table_bounds_ )\n",
                            "        else:\n",
                            "            final_resp_ = tmp_\n",
                            "\n",
                            "        return final_resp_[ : min( self.max_tables_per_sheet_, len( final_resp_ ) ) ]\n",
                            "\n",
                            "    def findDateRange( self, tbl ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def findDateRange( self, tbl_ ):",
                        "usage": [
                            "\n",
                            "    def findDateRange( self, tbl ):\n",
                            "\n",
                            "        colRange_ = list( range( tbl['START_COL'], tbl['END_COL'] ) )\n",
                            "\n",
                            "        for col in colRange_:\n",
                            "            ## process was taken out of the class only because multi processing map refused to pickle\n",
                            "            ## a method that was part of the class ..and it proved way slower ..so parallels been removed..lol\n",
                            "            results = process(col, self.sheet, tbl)\n",
                            "            if results[0] is True:\n",
                            "                    return str( results[1] ) +' To '+ str( results[2] )\n",
                            "\n",
                            "        return (None, None)\n",
                            "\n",
                            "    def findHeaderInfo(self, tbl):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def findHeaderInfo(self, tbl):",
                        "usage": [
                            "\n",
                            "    def findHeaderInfo(self, tbl):\n",
                            "        \"\"\"\n",
                            "        Find header information from the XL file.\n",
                            "        take the first 2 rows and then to be on the safe side also take the \n",
                            "        first 2 columns ( in case the col headers are just numbers / % etc and the row contain item name in the first col )\n",
                            "        send it to the LLM for a summary\n",
                            "        ALSO there's no need to take all ROWS and COLS .. some 10-15 elements are more than enough but can be adjustedfor domains that need more\n",
                            "        \"\"\"\n",
                            "\n",
                            "        hdr_row_start_ = self.findHdrRow( tbl )\n",
                            "        row_starter_ = tbl['START_ROW'] if hdr_row_start_ is None else hdr_row_start_\n",
                            "\n",
                            "        col_frame_ = ''\n",
                            "\n",
                            "        for rw in range( row_starter_ , min( row_starter_ + self.num_rows_to_consider_, tbl['END_ROW'] ) ):\n",
                            "            for col in range( tbl['START_COL'], min( self.max_elements_for_summary_, tbl['END_COL'] + 1 ) ):\n",
                            "                col_frame_ += '\\t' + str( self.sheet.cell(row=rw, column=col).value )\n",
                            "\n",
                            "            col_frame_ += '\\n'\n",
                            "\n",
                            "        return col_frame_\n",
                            "\n",
                            "    def findHighVarianceColumns(self, start_hdr_row_, sheet, tbl ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def findHighVarianceColumns( self, start_hdr_row_, sheet, tbl )",
                        "usage": [
                            "\n",
                            "    def findHighVarianceColumns(self, start_hdr_row_, sheet, tbl ):\n",
                            "        '''\n",
                            "        iterate through columns that have numeric values and figure out the more important columns\n",
                            "        num of rows - we can restrict it to lets say 1k rows ..should suffice \n",
                            "        '''\n",
                            "        numeric_frame_, high_var_indices_, hdr_col_names_ = dict(), set(), []\n",
                            "        end_row_ =  min( tbl['START_ROW'] + self.max_rows_variance , tbl['END_ROW'] + 1 )\n",
                            "        ## add 1 to the start row since we dont want to include the header value\n",
                            "        start_row_ = ( tbl['START_ROW'] if start_hdr_row_ is None else start_hdr_row_ ) + 1 \n",
                            "        \n",
                            "        print('BIGGIE-> start_row_, end_row_ = ', start_row_, end_row_)\n",
                            "\n",
                            "        for col_ctr in range( tbl['START_COL'], tbl['END_COL']+1 ):\n",
                            "                hdr_col_names_.append( str( self.sheet.cell(row=start_row_-1, column=col_ctr).value ) )\n",
                            "\n",
                            "        try:\n",
                            "            for col_ctr in range( tbl['START_COL'], tbl['END_COL']+1 ):\n",
                            "                col_arr_ = [ 'NA' for x in range( ( end_row_ - start_row_ ) + 1 ) ]\n",
                            "\n",
                            "                for idx, row_ctr in enumerate( range( start_row_, end_row_ ) ):\n",
                            "                    col_arr_[ idx ] = str( self.sheet.cell(row=row_ctr, column=col_ctr).value )\n",
                            "\n",
                            "                ## standardize the column since PCA better be done on std values\n",
                            "                col_set_ = set( col_arr_ )\n",
                            "                ## convert the variables into unique IDs\n",
                            "                uid = [ list( col_set_ ).index( x ) for x in col_arr_ ]\n",
                            "                max_uid_ = np.max( uid )\n",
                            "                ## normalize the int values\n",
                            "                numeric_frame_[ col_ctr ] = [ x/max_uid_ for x in uid ]\n",
                            "\n",
                            "            if len( numeric_frame_.keys() ) > 0:\n",
                            "                ## now transpose the contents of the frame since we want it to retain the shape of a column\n",
                            "                transposed_ = np.transpose( np.asarray( list( numeric_frame_.values() ) ) )\n",
                            "                #print('The val of transposed_->', transposed_)\n",
                            "                ## perform PCA and pick the most high variance columns\n",
                            "                ## the number of components to be picked will be decided by the thresh self.percent_pca_var_\n",
                            "                self.sklearn_pca_object_.fit( transposed_ )\n",
                            "                ## components_loading_ will give you principal component wise contribution of the features\n",
                            "                components_loading_ = self.sklearn_pca_object_.components_\n",
                            "                ## only consider those components that contribute to 90% or whatever threshold level of variance\n",
                            "                relevant_loading_ = components_loading_[0] \\\n",
                            "                                    if self.sklearn_pca_object_.explained_variance_ratio_[0] > self.pca_var_min_contrib \\\n",
                            "                                    else []\n",
                            "\n",
                            "                #print('LOADING AND REL_LOADING->', components_loading_, relevant_loading_)\n",
                            "                key_list_ = list( numeric_frame_.keys() )\n",
                            "\n",
                            "                for feat_idx, feat_contribution in enumerate( relevant_loading_ ):\n",
                            "                        if feat_contribution >= self.feature_contribution_per_thresh_: \n",
                            "\n",
                            "                            high_var_indices_.add( hdr_col_names_[ key_list_[ feat_idx ] ] )\n",
                            "\n",
                            "                            #print('Adding ', hdr_col_names_[ key_list_[ feat_idx ] ],' As a high variance col')\n",
                            "        except:\n",
                            "            pass\n",
                            "\n",
                            "        return list( high_var_indices_ ), hdr_col_names_\n",
                            "\n",
                            "\n",
                            "    def returnSummary(self, tbl ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def returnSummary( self, tbl_ ):",
                        "usage": [
                            "\n",
                            "    def returnSummary(self, tbl ):\n",
                            "        '''\n",
                            "        take the first few rows to try and generate a coherent summary for the type of the data present\n",
                            "        i am also considering transposing the first few rows to see how different the summary looks\n",
                            "        ALSO maybe limiting the number of columns makes sense\n",
                            "        '''\n",
                            "        frame_, transposed_frame_, start_hdr_row_ = '', '', self.findHdrRow( tbl )\n",
                            "        \n",
                            "        time_ = time.time()\n",
                            "        high_variance_cols_, hdr_col_names_ = self.findHighVarianceColumns( start_hdr_row_, self.sheet, tbl )\n",
                            "        print('Time taken to find high var cols ->', time.time() - time_)\n",
                            "        print('AND THEY ARE->', high_variance_cols_)\n",
                            "\n",
                            "        frame_num_contours_, transposed_frame_contours_ = 0, 0\n",
                            "        ## NATURAL order -> left to right, top to bottom\n",
                            "        for row_ctr in range( tbl['START_ROW'] if start_hdr_row_ is None else start_hdr_row_\\\n",
                            "                              , min( tbl['START_ROW']+self.num_rows_to_consider_ , tbl['END_ROW']+1 ) ):\n",
                            "\n",
                            "            for col_ctr in range( tbl['START_COL'], min( self.max_elements_for_summary_, tbl['END_COL']+1 ) ):\n",
                            "\n",
                            "                frame_ += '\\t' + str( self.sheet.cell(row=row_ctr, column=col_ctr).value )\n",
                            "                frame_num_contours_ += 1\n",
                            "\n",
                            "            frame_ += '\\n'\n",
                            "\n",
                            "        return frame_, high_variance_cols_, list( set(hdr_col_names_) )\n",
                            "\n",
                            "    def findHdrRow( self, tbl ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def findHdrRow( self, tbl )",
                        "usage": [
                            "\n",
                            "    def findHdrRow( self, tbl ):\n",
                            "\n",
                            "        total_cols_ = tbl['END_COL'] - tbl['START_COL']\n",
                            "\n",
                            "        for row_ctr in range( tbl['START_ROW'], \\\n",
                            "                              min( tbl['START_ROW']+self.num_rows_to_consider_ , tbl['END_ROW']+1 ) ):\n",
                            "            num_non_blank_ = 0\n",
                            "            \n",
                            "            for col_ctr in range( tbl['START_COL'], tbl['END_COL'] ):\n",
                            "                if self.sheet.cell(row=row_ctr, column=col_ctr).value is not None and \\\n",
                            "                        len( str( self.sheet.cell(row=row_ctr, column=col_ctr).value ) ) > 0: \n",
                            "                    num_non_blank_ += 1\n",
                            "\n",
                            "            ## only if the number of hdr columns is in the ballpark w.r.t. total number of columns\n",
                            "            ## should we start the table ..at times the header table is split across more than 1 row\n",
                            "            if total_cols_ > 1 and (num_non_blank_/total_cols_) > self.col_thresh_:\n",
                            "                return row_ctr\n",
                            "\n",
                            "        return None # so default value of row #1 applies to table start\n",
                            "    \n",
                            "    def createDBRec( self, summary_D, mode='NORM' ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def createDBRec( self, summary_D, mode='NORM' ):",
                        "usage": [
                            "    \n",
                            "    def createDBRec( self, summary_D, mode='NORM' ):\n",
                            "\n",
                            "        insertRec = dict()\n",
                            "        insertRec['docID'] = random.randint( 1000, 100000 )\n",
                            "        ## combine all necessary fields to form vector signature\n",
                            "        ## keys-> 'sample_summary'; 'date_range' ; 'hdr_info'\n",
                            "\n",
                            "        hdr_info = summary_D['hdr_info']\n",
                            "        sample_summary_ = summary_D['sample_summary']\n",
                            "\n",
                            "        unified_key_ =   'Date Range : '+ str( summary_D['date_range'] ) + '\\n' \\\n",
                            "                       + 'Column Headers : '+ ' , '.join( summary_D['col_names_'] ).strip() + '\\n' \\\n",
                            "                       + 'LLM Summary : '+ ( sample_summary_ ) if self.add_ai_summary_to_embedding_ is True else ''\n",
                            "\n",
                            "        emb_ = createJsonFeats.returnEmbed( unified_key_ )\n",
                            "        insertRec['docSignature'] = emb_\n",
                            "        insertRec['summary'] = unified_key_\n",
                            "        insertRec['file_path'] = summary_D['file_path']\n",
                            "        insertRec['file_name'] = summary_D['file_path'].split('/')[-1]\n",
                            "        insertRec['sheet_name'] = summary_D['sheet_name']\n",
                            "        insertRec['date_range'] = summary_D['date_range']\n",
                            "        insertRec['hdr_info'] = hdr_info\n",
                            "\n",
                            "        print('Inserting RECORD->', insertRec['file_name'], insertRec['sheet_name'], unified_key_ )\n",
                            "        return insertRec\n",
                            "\n",
                            "    def mergeAndInsert( self, summary_D ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def mergeAndInsert( self, summary_D ):",
                        "usage": [
                            "\n",
                            "    def mergeAndInsert( self, summary_D ):\n",
                            "        '''\n",
                            "        we shall be inserting 2 records for every table\n",
                            "        a) the normal table structure\n",
                            "        b) the transposed table structure\n",
                            "        along with all meta info\n",
                            "        '''\n",
                            "        ##NORM TBL STRUCT\n",
                            "        rec_ = self.createDBRec( summary_D, 'NORM' )\n",
                            "        db_utils.insertNewSignature( rec_ )\n",
                            "\n",
                            "    def returnEntireSheet( self, tbl_, sheet_name ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def returnEntireSheet( self, tbl_, sheet_name ):",
                        "usage": [
                            "\n",
                            "    def returnEntireSheet( self, tbl_, sheet_name ):\n",
                            "        '''\n",
                            "        find if the entire sheet contains mostly textual information. If so, then we should simply\n",
                            "        chunk the whole sheet , after concatenating \n",
                            "        A simple rule of thumb can be the length of the cell contents in any column.\n",
                            "        If the lenght of the cell contents is greater than some threshold say 10 words\n",
                            "        '''\n",
                            "        use_entire_sheet_, chunks_ = False, []\n",
                            "\n",
                            "        for col_ctr in range( tbl_['START_COL'], tbl_['END_COL'] ):\n",
                            "            num_str_, unique_, ignore = 0, set(), False\n",
                            "            for row_ctr in range( tbl_['START_ROW'], tbl_['END_ROW'] ):\n",
                            "                if type( self.sheet.cell(row=row_ctr, column=col_ctr).value ) == str and\\\n",
                            "                        len( (self.sheet.cell(row=row_ctr, column=col_ctr).value).split() ) >= self.sz_of_phrase_:\n",
                            "                            num_str_ += 1\n",
                            "                unique_.add( ( self.sheet.cell(row=row_ctr, column=col_ctr).value ) )\n",
                            "\n",
                            "            ## if num of unique strings in col is low it means, this value is being repeated\n",
                            "            ## HENCE its mostly observations being selected from a drop down and does NOT need\n",
                            "            ## the entire doc chunked\n",
                            "            if len( unique_ ) < self.unique_str_thresh_*( tbl_['END_ROW'] - tbl_['START_ROW'] ): ignore = True\n",
                            "            \n",
                            "            print('returnEntireSheet->', sheet_name, tbl_, num_str_, ( tbl_['END_ROW'] - tbl_['START_ROW'] ), ignore)\n",
                            "            if num_str_ >= self.number_str_thresh_*( tbl_['END_ROW'] - tbl_['START_ROW'] ) and ignore is False:\n",
                            "\n",
                            "                use_entire_sheet_ = True\n",
                            "                ## aggregate all text and chunk using self.chunk_size_\n",
                            "                frame_ = ''\n",
                            "                for row_ctr in range( tbl_['START_ROW'], tbl_['END_ROW'] ):\n",
                            "                    for col_ctr in range( tbl_['START_COL'], tbl_['END_COL'] ):\n",
                            "                        \n",
                            "                        if len( frame_ ) >= self.chunk_size_:\n",
                            "                            chunks_.append( frame_ )\n",
                            "                            frame_ = ''\n",
                            "\n",
                            "                        frame_ += '\\t'+ str( self.sheet.cell(row=row_ctr, column=col_ctr).value )\n",
                            "                    frame_ += '\\n'\n",
                            "\n",
                            "                if len( frame_ ) > 0: chunks_.append( frame_ )\n",
                            "\n",
                            "        return chunks_, use_entire_sheet_\n",
                            "\n",
                            "    def process_full_frame_( self, full_frame_, summary_D ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def process_full_frame_( self, full_frame_, summary_D ):",
                        "usage": [
                            "\n",
                            "    def process_full_frame_( self, full_frame_, summary_D ):\n",
                            "\n",
                            "        for chunk in full_frame_:\n",
                            "           summary_D['sample_summary'] = chunk\n",
                            "           self.mergeAndInsert( summary_D )\n",
                            "        \n",
                            "    def read_excel_file(self):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def process_full_frame_( self, full_frame_, summary_D ):",
                        "usage": [
                            "\n",
                            "    def process_full_frame_( self, full_frame_, summary_D ):\n",
                            "\n",
                            "        for chunk in full_frame_:\n",
                            "           summary_D['sample_summary'] = chunk\n",
                            "           self.mergeAndInsert( summary_D )\n",
                            "        \n",
                            "    def read_excel_file(self):\n"
                        ]
                    }
                ]
            }
        },
        {
            "": {
                "def": 0,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def is_date( input_str):",
                        "usage": [
                            "\n",
                            "def is_date( input_str):\n",
                            "        ## first check for INT and FLOAT since parser foolishly accepts ints\n",
                            "        try:\n",
                            "            _ = int( input_str )\n",
                            "            return None\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def findHeaderInfo( self, tbl_ ):",
                        "usage": [
                            "            _ = int( input_str )\n",
                            "            return None\n",
                            "        except:\n",
                            "            pass\n",
                            "\n",
                            "        try:\n",
                            "            _ = float( input_str )\n",
                            "            return None\n",
                            "        except:\n",
                            "            pass\n",
                            "\n",
                            "        try:\n",
                            "            return parser.parse(input_str)\n",
                            "        except ValueError:\n",
                            "            return None\n",
                            "\n",
                            "def process( colNum, sheet, tbl ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def process( colNum, sheet, tbl ):",
                        "usage": [
                            "\n",
                            "def process( colNum, sheet, tbl ):\n",
                            "        dt_counts_ = []\n",
                            "\n",
                            "        for rw in range( tbl['START_ROW'], tbl['END_ROW'] ):\n",
                            "                dtVal_ = is_date( str( sheet.cell(row=rw, column=colNum).value ) )\n",
                            "\n",
                            "                if dtVal_ is not None : \n",
                            "                    dt_counts_.append( dtVal_ ) \n",
                            "\n",
                            "        if len( dt_counts_ ) >= ( tbl['END_ROW'] - tbl['START_ROW'] )/2: \n",
                            "                ## defensive chk to ensure dt counts are high\n",
                            "                print('Dt Col found !', colNum)\n",
                            "                ## sort the values to get range\n",
                            "                sorted_dates_ = sorted( dt_counts_ )\n",
                            "                print('Dt range->', sorted_dates_[0], sorted_dates_[-1] )\n",
                            "\n",
                            "                return ( True, sorted_dates_[0].strftime('%B %d, %Y'), sorted_dates_[-1].strftime('%B %d, %Y') )\n",
                            "\n",
                            "        return ( False, None, None )\n",
                            "\n",
                            "class GenerateXLMetaInfo:\n",
                            "    def __init__(self, file_path, llm='LLAMA'):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def __init__(self, file_path, llm='LLAMA'):",
                        "usage": [
                            "class GenerateXLMetaInfo:\n",
                            "    def __init__(self, file_path, llm='LLAMA'):\n",
                            "        \"\"\"\n",
                            "        Initialize the GenerateXmlMetaInfo class with the XML file.\n",
                            "\n",
                            "        Parameters:\n",
                            "        - xml_file (str): The path to the XML file.\n",
                            "        \"\"\"\n",
                            "        self.file_path = file_path\n",
                            "        self.masterInfo_ = dict()\n",
                            "        self.llm_framework_ = llm\n",
                            "        self.sheet = None\n",
                            "        self.sklearn_pca_object_ = PCA()\n",
                            "        self.add_ai_summary_to_embedding_ = True\n",
                            "        self.chunk_size_ = 500 ## approx 1024 tokens\n",
                            "\n",
                            "        self.sz_of_phrase_, self.unique_str_thresh_, self.number_str_thresh_ = 5, 0.5, 0.4\n",
                            "        self.pca_var_min_contrib, self.feature_contribution_per_thresh_ = 0.5, 0.3\n",
                            "        self.max_tables_per_sheet_ = 10 ## at times a single sheet can have multiple tables\n",
                            "        self.num_rows_to_consider_, self.col_thresh_, self.minElemsInTable, self.max_rows_variance = 4, 0.8, 6, 100\n",
                            "        self.default_max_col_ , self.default_max_row_, self.max_cols_for_review_, \\\n",
                            "                self.min_num_distinct_values_, self.max_elements_for_summary_ = 50, 50, 10, 3, 15\n",
                            "\n",
                            "        if llm == 'OPENAI':\n",
                            "            self.query_fn_ = openai\n",
                            "        else:\n",
                            "            ## default , llama3 inferred via groq\n",
                            "            self.query_fn_ = llama3\n",
                            "   \n",
                            "    def find_bounds( self, sheet, max_row, max_col, start_row ,end_row ,start_col ,end_col ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def find_bounds( self, sheet, max_row, max_col, start_row , end_row ,start_col ,end_col )",
                        "usage": [
                            "   \n",
                            "    def find_bounds( self, sheet, max_row, max_col, start_row ,end_row ,start_col ,end_col ):\n",
                            "\n",
                            "        # Iterate over rows to find the start and end rows\n",
                            "        start_row_idx_ = 1 if start_row is None else start_row\n",
                            "        start_col_idx_ = 1 if start_col is None else start_col\n",
                            "        \n",
                            "        ## need to add 2 to max rw and col since max_row of sheet returns the last row with data\n",
                            "        ## and range will stop at max_row - 1 \n",
                            "        for row in range( start_row_idx_ , max_row + 2):\n",
                            "            if all( self.sheet.cell(row=row, column=col).value is None for col in range(1, max_col + 2)):\n",
                            "                if start_row is None:\n",
                            "                    continue  # Skip empty rows before the table\n",
                            "                else:\n",
                            "                    end_row = row - 1\n",
                            "                    break\n",
                            "            elif start_row is None:\n",
                            "                start_row = row\n",
                            "           \n",
                            "        # Iterate over columns to find the start and end columns\n",
                            "        for col in range( start_col_idx_, max_col + 2):\n",
                            "            #for row in range(start_row, end_row):\n",
                            "            #    print('ROW NUM->', col, ' VALUE: ', sheet.cell(row=row, column=col).value )\n",
                            "            if end_row is None: continue\n",
                            "\n",
                            "            if all( self.sheet.cell(row=row, column=col).value is None for row in range(start_row, end_row)):\n",
                            "                if start_col is None:\n",
                            "                    continue  # Skip empty columns before the table\n",
                            "                else:\n",
                            "                    end_col = col - 1\n",
                            "                    break\n",
                            "            elif start_col is None:\n",
                            "                start_col = col\n",
                            "            \n",
                            "\n",
                            "        #print('Found tables between-> start_row, end_row, start_col, end_col = ',\\\n",
                            "        #        start_row, end_row, start_col, end_col )\n",
                            "\n",
                            "        return start_row, end_row, start_col, end_col\n",
                            "\n",
                            "    def is_hdr_row_format( self, tbl_bound, sheet ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def is_hdr_row_format( self, tbl_bound, sheet )",
                        "usage": [
                            "\n",
                            "    def is_hdr_row_format( self, tbl_bound, sheet ):\n",
                            "        \n",
                            "        num_str_cols_ = 0\n",
                            "        for col_ctr in range( tbl_bound['START_COL'], tbl_bound['END_COL'] ):\n",
                            "            if type( self.sheet.cell(row=tbl_bound['START_ROW'], column=col_ctr).value ) == str:\n",
                            "                num_str_cols_ += 1\n",
                            "\n",
                            "        if num_str_cols_ < ( tbl_bound['END_COL'] - tbl_bound['START_COL'] ): return False\n",
                            "\n",
                            "        return True\n",
                            "\n",
                            "    def find_tables(self, sheet):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def find_tables(self, sheet)",
                        "usage": [
                            "\n",
                            "    def find_tables(self, sheet):\n",
                            "        ## NOTE -> sheet.max_row and sheet.max_column is NOT WORKING !! NEED TO FIX\n",
                            "        ## default is stop gap\n",
                            "        max_row = sheet.max_row if sheet.max_row is not None else self.default_max_row_\n",
                            "        max_col = sheet.max_column if sheet.max_column is not None else self.default_max_col_\n",
                            "        table_bounds_ = []\n",
                            "\n",
                            "        print('KKR->', max_row, max_col)\n",
                            "        timer_ = time.time()\n",
                            "        # Initialize variables to track the bounds\n",
                            "        start_row ,end_row ,start_col ,end_col = None, None, 1, sheet.max_column\n",
                            "\n",
                            "        ## do a first pass to find the first table\n",
                            "        start_row, end_row, start_col, end_col = self.find_bounds( sheet, max_row, max_col, start_row ,\\\n",
                            "                                                                         end_row ,start_col ,end_col )\n",
                            "\n",
                            "        print('Time taken->', time.time() - timer_, start_row, end_row, start_col, end_col)    \n",
                            "        init_end_col = copy.copy( end_col )\n",
                            "\n",
                            "        table_bounds_.append( { 'START_ROW': start_row, 'END_ROW': end_row,\\\n",
                            "                                    'START_COL': start_col, 'END_COL': end_col } ) \n",
                            "\n",
                            "        ## now iterate from end_row to max_row to find all tables row wise\n",
                            "        while end_row is not None:\n",
                            "            end_row += 2 ## increment by 2 since we need to look ahead and see if any more tables exist !\n",
                            "            ##              if u increment by 1 then u will end up on the same blank line that stopped the prev tbl  \n",
                            "            ## start_row is assigned the value of end_row from above and end_row is made None\n",
                            "            if end_row >= max_row: break\n",
                            "\n",
                            "            #print('DUM ROW->', end_row)\n",
                            "            start_row, end_row, start_col, end_col = self.find_bounds( sheet, max_row, max_col, end_row ,\\\n",
                            "                                                                             None , None , None )\n",
                            "\n",
                            "            if ( start_col is None or end_col is None ) or \\\n",
                            "                    ( abs( start_row - end_row )*abs( start_col - end_col ) ) <= self.minElemsInTable: continue    \n",
                            "\n",
                            "            table_bounds_.append( { 'START_ROW': start_row, 'END_ROW': end_row,\\\n",
                            "                                    'START_COL': start_col, 'END_COL': end_col } ) \n",
                            "        \n",
                            "        ## now iterate from end_col to max_col to find all tables cols wise\n",
                            "        while init_end_col is not None:\n",
                            "            init_end_col += 2 ## increment by 1 since we need to look ahead and see if any more tables exist !\n",
                            "            ## start_row is assigned the value of end_row from above and end_row is made None\n",
                            "            if init_end_col >= max_col: break\n",
                            "\n",
                            "            #print('DUM COL->', init_end_col)\n",
                            "            start_row, end_row, start_col, end_col = self.find_bounds( sheet, max_row, max_col, None ,\\\n",
                            "                                                                             None , init_end_col , None )\n",
                            "\n",
                            "            if ( start_col >= end_col ): continue    \n",
                            "\n",
                            "            table_bounds_.append( { 'START_ROW': start_row, 'END_ROW': end_row,\\\n",
                            "                                    'START_COL': start_col, 'END_COL': end_col } ) \n",
                            "\n",
                            "        ## init star and end col to min and max\n",
                            "        for tab in table_bounds_: tab['START_COL'] = 1; tab['END_COL'] = max_col;\n",
                            "        ## remove dupes\n",
                            "        tmp_, dupe = [], set()\n",
                            "\n",
                            "        for idx1, tab1 in enumerate( table_bounds_ ):\n",
                            "            for idx2, tab2 in enumerate( table_bounds_ ):\n",
                            "                if idx1 <= idx2: continue\n",
                            "                if tab2['START_ROW'] >= tab1['START_ROW'] and tab2['END_ROW'] <= tab1['END_ROW']:\n",
                            "                    dupe.add( idx2 )\n",
                            "\n",
                            "        for idx, tab in enumerate( table_bounds_ ):\n",
                            "            if idx not in list( dupe ):\n",
                            "                tmp_.append( tab )\n",
                            "\n",
                            "        ## blend tables - in case the rows are FPs\n",
                            "        final_resp_ = []\n",
                            "        if len( tmp_ ) > 1:\n",
                            "            last_tbl_ = tmp_[0]\n",
                            "            final_resp_.append( last_tbl_ )\n",
                            "            ## check if the first row is not all STR\n",
                            "            for ctr in range( 1, len( tmp_ ) ):\n",
                            "                if self.is_hdr_row_format( tmp_[ctr], sheet ) == False:\n",
                            "                    ## blend with the last table\n",
                            "                    final_resp_[-1]['END_ROW'] = tmp_[ctr]['END_ROW']\n",
                            "                else:\n",
                            "                    final_resp_.append( table_bounds_ )\n",
                            "        else:\n",
                            "            final_resp_ = tmp_\n",
                            "\n",
                            "        return final_resp_[ : min( self.max_tables_per_sheet_, len( final_resp_ ) ) ]\n",
                            "\n",
                            "    def findDateRange( self, tbl ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def findDateRange( self, tbl_ ):",
                        "usage": [
                            "\n",
                            "    def findDateRange( self, tbl ):\n",
                            "\n",
                            "        colRange_ = list( range( tbl['START_COL'], tbl['END_COL'] ) )\n",
                            "\n",
                            "        for col in colRange_:\n",
                            "            ## process was taken out of the class only because multi processing map refused to pickle\n",
                            "            ## a method that was part of the class ..and it proved way slower ..so parallels been removed..lol\n",
                            "            results = process(col, self.sheet, tbl)\n",
                            "            if results[0] is True:\n",
                            "                    return str( results[1] ) +' To '+ str( results[2] )\n",
                            "\n",
                            "        return (None, None)\n",
                            "\n",
                            "    def findHeaderInfo(self, tbl):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def findHeaderInfo(self, tbl):",
                        "usage": [
                            "\n",
                            "    def findHeaderInfo(self, tbl):\n",
                            "        \"\"\"\n",
                            "        Find header information from the XL file.\n",
                            "        take the first 2 rows and then to be on the safe side also take the \n",
                            "        first 2 columns ( in case the col headers are just numbers / % etc and the row contain item name in the first col )\n",
                            "        send it to the LLM for a summary\n",
                            "        ALSO there's no need to take all ROWS and COLS .. some 10-15 elements are more than enough but can be adjustedfor domains that need more\n",
                            "        \"\"\"\n",
                            "\n",
                            "        hdr_row_start_ = self.findHdrRow( tbl )\n",
                            "        row_starter_ = tbl['START_ROW'] if hdr_row_start_ is None else hdr_row_start_\n",
                            "\n",
                            "        col_frame_ = ''\n",
                            "\n",
                            "        for rw in range( row_starter_ , min( row_starter_ + self.num_rows_to_consider_, tbl['END_ROW'] ) ):\n",
                            "            for col in range( tbl['START_COL'], min( self.max_elements_for_summary_, tbl['END_COL'] + 1 ) ):\n",
                            "                col_frame_ += '\\t' + str( self.sheet.cell(row=rw, column=col).value )\n",
                            "\n",
                            "            col_frame_ += '\\n'\n",
                            "\n",
                            "        return col_frame_\n",
                            "\n",
                            "    def findHighVarianceColumns(self, start_hdr_row_, sheet, tbl ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def findHighVarianceColumns( self, start_hdr_row_, sheet, tbl )",
                        "usage": [
                            "\n",
                            "    def findHighVarianceColumns(self, start_hdr_row_, sheet, tbl ):\n",
                            "        '''\n",
                            "        iterate through columns that have numeric values and figure out the more important columns\n",
                            "        num of rows - we can restrict it to lets say 1k rows ..should suffice \n",
                            "        '''\n",
                            "        numeric_frame_, high_var_indices_, hdr_col_names_ = dict(), set(), []\n",
                            "        end_row_ =  min( tbl['START_ROW'] + self.max_rows_variance , tbl['END_ROW'] + 1 )\n",
                            "        ## add 1 to the start row since we dont want to include the header value\n",
                            "        start_row_ = ( tbl['START_ROW'] if start_hdr_row_ is None else start_hdr_row_ ) + 1 \n",
                            "        \n",
                            "        print('BIGGIE-> start_row_, end_row_ = ', start_row_, end_row_)\n",
                            "\n",
                            "        for col_ctr in range( tbl['START_COL'], tbl['END_COL']+1 ):\n",
                            "                hdr_col_names_.append( str( self.sheet.cell(row=start_row_-1, column=col_ctr).value ) )\n",
                            "\n",
                            "        try:\n",
                            "            for col_ctr in range( tbl['START_COL'], tbl['END_COL']+1 ):\n",
                            "                col_arr_ = [ 'NA' for x in range( ( end_row_ - start_row_ ) + 1 ) ]\n",
                            "\n",
                            "                for idx, row_ctr in enumerate( range( start_row_, end_row_ ) ):\n",
                            "                    col_arr_[ idx ] = str( self.sheet.cell(row=row_ctr, column=col_ctr).value )\n",
                            "\n",
                            "                ## standardize the column since PCA better be done on std values\n",
                            "                col_set_ = set( col_arr_ )\n",
                            "                ## convert the variables into unique IDs\n",
                            "                uid = [ list( col_set_ ).index( x ) for x in col_arr_ ]\n",
                            "                max_uid_ = np.max( uid )\n",
                            "                ## normalize the int values\n",
                            "                numeric_frame_[ col_ctr ] = [ x/max_uid_ for x in uid ]\n",
                            "\n",
                            "            if len( numeric_frame_.keys() ) > 0:\n",
                            "                ## now transpose the contents of the frame since we want it to retain the shape of a column\n",
                            "                transposed_ = np.transpose( np.asarray( list( numeric_frame_.values() ) ) )\n",
                            "                #print('The val of transposed_->', transposed_)\n",
                            "                ## perform PCA and pick the most high variance columns\n",
                            "                ## the number of components to be picked will be decided by the thresh self.percent_pca_var_\n",
                            "                self.sklearn_pca_object_.fit( transposed_ )\n",
                            "                ## components_loading_ will give you principal component wise contribution of the features\n",
                            "                components_loading_ = self.sklearn_pca_object_.components_\n",
                            "                ## only consider those components that contribute to 90% or whatever threshold level of variance\n",
                            "                relevant_loading_ = components_loading_[0] \\\n",
                            "                                    if self.sklearn_pca_object_.explained_variance_ratio_[0] > self.pca_var_min_contrib \\\n",
                            "                                    else []\n",
                            "\n",
                            "                #print('LOADING AND REL_LOADING->', components_loading_, relevant_loading_)\n",
                            "                key_list_ = list( numeric_frame_.keys() )\n",
                            "\n",
                            "                for feat_idx, feat_contribution in enumerate( relevant_loading_ ):\n",
                            "                        if feat_contribution >= self.feature_contribution_per_thresh_: \n",
                            "\n",
                            "                            high_var_indices_.add( hdr_col_names_[ key_list_[ feat_idx ] ] )\n",
                            "\n",
                            "                            #print('Adding ', hdr_col_names_[ key_list_[ feat_idx ] ],' As a high variance col')\n",
                            "        except:\n",
                            "            pass\n",
                            "\n",
                            "        return list( high_var_indices_ ), hdr_col_names_\n",
                            "\n",
                            "\n",
                            "    def returnSummary(self, tbl ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def returnSummary( self, tbl_ ):",
                        "usage": [
                            "\n",
                            "    def returnSummary(self, tbl ):\n",
                            "        '''\n",
                            "        take the first few rows to try and generate a coherent summary for the type of the data present\n",
                            "        i am also considering transposing the first few rows to see how different the summary looks\n",
                            "        ALSO maybe limiting the number of columns makes sense\n",
                            "        '''\n",
                            "        frame_, transposed_frame_, start_hdr_row_ = '', '', self.findHdrRow( tbl )\n",
                            "        \n",
                            "        time_ = time.time()\n",
                            "        high_variance_cols_, hdr_col_names_ = self.findHighVarianceColumns( start_hdr_row_, self.sheet, tbl )\n",
                            "        print('Time taken to find high var cols ->', time.time() - time_)\n",
                            "        print('AND THEY ARE->', high_variance_cols_)\n",
                            "\n",
                            "        frame_num_contours_, transposed_frame_contours_ = 0, 0\n",
                            "        ## NATURAL order -> left to right, top to bottom\n",
                            "        for row_ctr in range( tbl['START_ROW'] if start_hdr_row_ is None else start_hdr_row_\\\n",
                            "                              , min( tbl['START_ROW']+self.num_rows_to_consider_ , tbl['END_ROW']+1 ) ):\n",
                            "\n",
                            "            for col_ctr in range( tbl['START_COL'], min( self.max_elements_for_summary_, tbl['END_COL']+1 ) ):\n",
                            "\n",
                            "                frame_ += '\\t' + str( self.sheet.cell(row=row_ctr, column=col_ctr).value )\n",
                            "                frame_num_contours_ += 1\n",
                            "\n",
                            "            frame_ += '\\n'\n",
                            "\n",
                            "        return frame_, high_variance_cols_, list( set(hdr_col_names_) )\n",
                            "\n",
                            "    def findHdrRow( self, tbl ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def findHdrRow( self, tbl )",
                        "usage": [
                            "\n",
                            "    def findHdrRow( self, tbl ):\n",
                            "\n",
                            "        total_cols_ = tbl['END_COL'] - tbl['START_COL']\n",
                            "\n",
                            "        for row_ctr in range( tbl['START_ROW'], \\\n",
                            "                              min( tbl['START_ROW']+self.num_rows_to_consider_ , tbl['END_ROW']+1 ) ):\n",
                            "            num_non_blank_ = 0\n",
                            "            \n",
                            "            for col_ctr in range( tbl['START_COL'], tbl['END_COL'] ):\n",
                            "                if self.sheet.cell(row=row_ctr, column=col_ctr).value is not None and \\\n",
                            "                        len( str( self.sheet.cell(row=row_ctr, column=col_ctr).value ) ) > 0: \n",
                            "                    num_non_blank_ += 1\n",
                            "\n",
                            "            ## only if the number of hdr columns is in the ballpark w.r.t. total number of columns\n",
                            "            ## should we start the table ..at times the header table is split across more than 1 row\n",
                            "            if total_cols_ > 1 and (num_non_blank_/total_cols_) > self.col_thresh_:\n",
                            "                return row_ctr\n",
                            "\n",
                            "        return None # so default value of row #1 applies to table start\n",
                            "    \n",
                            "    def createDBRec( self, summary_D, mode='NORM' ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def createDBRec( self, summary_D, mode='NORM' ):",
                        "usage": [
                            "    \n",
                            "    def createDBRec( self, summary_D, mode='NORM' ):\n",
                            "\n",
                            "        insertRec = dict()\n",
                            "        insertRec['docID'] = random.randint( 1000, 100000 )\n",
                            "        ## combine all necessary fields to form vector signature\n",
                            "        ## keys-> 'sample_summary'; 'date_range' ; 'hdr_info'\n",
                            "\n",
                            "        hdr_info = summary_D['hdr_info']\n",
                            "        sample_summary_ = summary_D['sample_summary']\n",
                            "\n",
                            "        unified_key_ =   'Date Range : '+ str( summary_D['date_range'] ) + '\\n' \\\n",
                            "                       + 'Column Headers : '+ ' , '.join( summary_D['col_names_'] ).strip() + '\\n' \\\n",
                            "                       + 'LLM Summary : '+ ( sample_summary_ ) if self.add_ai_summary_to_embedding_ is True else ''\n",
                            "\n",
                            "        emb_ = createJsonFeats.returnEmbed( unified_key_ )\n",
                            "        insertRec['docSignature'] = emb_\n",
                            "        insertRec['summary'] = unified_key_\n",
                            "        insertRec['file_path'] = summary_D['file_path']\n",
                            "        insertRec['file_name'] = summary_D['file_path'].split('/')[-1]\n",
                            "        insertRec['sheet_name'] = summary_D['sheet_name']\n",
                            "        insertRec['date_range'] = summary_D['date_range']\n",
                            "        insertRec['hdr_info'] = hdr_info\n",
                            "\n",
                            "        print('Inserting RECORD->', insertRec['file_name'], insertRec['sheet_name'], unified_key_ )\n",
                            "        return insertRec\n",
                            "\n",
                            "    def mergeAndInsert( self, summary_D ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def mergeAndInsert( self, summary_D ):",
                        "usage": [
                            "\n",
                            "    def mergeAndInsert( self, summary_D ):\n",
                            "        '''\n",
                            "        we shall be inserting 2 records for every table\n",
                            "        a) the normal table structure\n",
                            "        b) the transposed table structure\n",
                            "        along with all meta info\n",
                            "        '''\n",
                            "        ##NORM TBL STRUCT\n",
                            "        rec_ = self.createDBRec( summary_D, 'NORM' )\n",
                            "        db_utils.insertNewSignature( rec_ )\n",
                            "\n",
                            "    def returnEntireSheet( self, tbl_, sheet_name ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def returnEntireSheet( self, tbl_, sheet_name ):",
                        "usage": [
                            "\n",
                            "    def returnEntireSheet( self, tbl_, sheet_name ):\n",
                            "        '''\n",
                            "        find if the entire sheet contains mostly textual information. If so, then we should simply\n",
                            "        chunk the whole sheet , after concatenating \n",
                            "        A simple rule of thumb can be the length of the cell contents in any column.\n",
                            "        If the lenght of the cell contents is greater than some threshold say 10 words\n",
                            "        '''\n",
                            "        use_entire_sheet_, chunks_ = False, []\n",
                            "\n",
                            "        for col_ctr in range( tbl_['START_COL'], tbl_['END_COL'] ):\n",
                            "            num_str_, unique_, ignore = 0, set(), False\n",
                            "            for row_ctr in range( tbl_['START_ROW'], tbl_['END_ROW'] ):\n",
                            "                if type( self.sheet.cell(row=row_ctr, column=col_ctr).value ) == str and\\\n",
                            "                        len( (self.sheet.cell(row=row_ctr, column=col_ctr).value).split() ) >= self.sz_of_phrase_:\n",
                            "                            num_str_ += 1\n",
                            "                unique_.add( ( self.sheet.cell(row=row_ctr, column=col_ctr).value ) )\n",
                            "\n",
                            "            ## if num of unique strings in col is low it means, this value is being repeated\n",
                            "            ## HENCE its mostly observations being selected from a drop down and does NOT need\n",
                            "            ## the entire doc chunked\n",
                            "            if len( unique_ ) < self.unique_str_thresh_*( tbl_['END_ROW'] - tbl_['START_ROW'] ): ignore = True\n",
                            "            \n",
                            "            print('returnEntireSheet->', sheet_name, tbl_, num_str_, ( tbl_['END_ROW'] - tbl_['START_ROW'] ), ignore)\n",
                            "            if num_str_ >= self.number_str_thresh_*( tbl_['END_ROW'] - tbl_['START_ROW'] ) and ignore is False:\n",
                            "\n",
                            "                use_entire_sheet_ = True\n",
                            "                ## aggregate all text and chunk using self.chunk_size_\n",
                            "                frame_ = ''\n",
                            "                for row_ctr in range( tbl_['START_ROW'], tbl_['END_ROW'] ):\n",
                            "                    for col_ctr in range( tbl_['START_COL'], tbl_['END_COL'] ):\n",
                            "                        \n",
                            "                        if len( frame_ ) >= self.chunk_size_:\n",
                            "                            chunks_.append( frame_ )\n",
                            "                            frame_ = ''\n",
                            "\n",
                            "                        frame_ += '\\t'+ str( self.sheet.cell(row=row_ctr, column=col_ctr).value )\n",
                            "                    frame_ += '\\n'\n",
                            "\n",
                            "                if len( frame_ ) > 0: chunks_.append( frame_ )\n",
                            "\n",
                            "        return chunks_, use_entire_sheet_\n",
                            "\n",
                            "    def process_full_frame_( self, full_frame_, summary_D ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def process_full_frame_( self, full_frame_, summary_D ):",
                        "usage": [
                            "\n",
                            "    def process_full_frame_( self, full_frame_, summary_D ):\n",
                            "\n",
                            "        for chunk in full_frame_:\n",
                            "           summary_D['sample_summary'] = chunk\n",
                            "           self.mergeAndInsert( summary_D )\n",
                            "        \n",
                            "    def read_excel_file(self):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/basic_generateXLMetaData.py",
                        "method_nm": "def process_full_frame_( self, full_frame_, summary_D ):",
                        "usage": [
                            "\n",
                            "    def process_full_frame_( self, full_frame_, summary_D ):\n",
                            "\n",
                            "        for chunk in full_frame_:\n",
                            "           summary_D['sample_summary'] = chunk\n",
                            "           self.mergeAndInsert( summary_D )\n",
                            "        \n",
                            "    def read_excel_file(self):\n"
                        ]
                    }
                ]
            }
        }
    ],
    "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/createJsonFeats.py": [
        {
            "json": {
                "def": 0,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/createJsonFeats.py",
                        "method_nm": "def returnEmbed( sent ):",
                        "usage": [
                            "    data = json.dumps( rec_ ).encode('utf-8')\n",
                            "                                        headers={'Content-Type': 'application/json'} )\n",
                            "    json_obj = json.loads(string)\n",
                            "    return json_obj['encoded_'], True\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/createJsonFeats.py",
                        "method_nm": "def returnEmbed( sent ):",
                        "usage": [
                            "    data = json.dumps( rec_ ).encode('utf-8')\n",
                            "                                        headers={'Content-Type': 'application/json'} )\n",
                            "    json_obj = json.loads(string)\n",
                            "    return json_obj['encoded_'], True\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/createJsonFeats.py",
                        "method_nm": "def returnJsonFeat( src_0, src_raw ):",
                        "usage": [
                            "        json_ = json.load( fp )\n",
                            "        json_raw = json.load( fp )\n",
                            "    key_tuple_ = findKeys.processNeighbours( json_, json_raw, file_ )    \n",
                            "    data = json.dumps( rec_ ).encode('utf-8')\n",
                            "    _request = urllib.request.Request( url_encode, data=data, method='POST', headers={'Content-Type': 'application/json'} )\n",
                            "    json_obj = json.loads(string)\n",
                            "    return json_obj['encoded_'], key_tuple_\n"
                        ]
                    }
                ]
            }
        },
        {
            "numpy": 1
        },
        {
            "np": {
                "def": 1,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/createJsonFeats.py",
                        "method_nm": "def returnJsonFeat( src_0, src_raw ):",
                        "usage": [
                            "    #print( np.asarray( dist_matrix_ ).shape, dist_matrix_[0] )\n"
                        ]
                    }
                ]
            }
        },
        {
            "scipy": 3
        },
        {
            "": {
                "def": 3,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/createJsonFeats.py",
                        "method_nm": "def blockPrint():",
                        "usage": [
                            "# Disable\n",
                            "def blockPrint():\n",
                            "    sys.stdout = open(os.devnull, 'w')\n",
                            "\n",
                            "# Restore\n",
                            "def enablePrint():\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/createJsonFeats.py",
                        "method_nm": "def enablePrint():",
                        "usage": [
                            "# Restore\n",
                            "def enablePrint():\n",
                            "    sys.stdout = sys.__stdout__\n",
                            "\n",
                            "\n",
                            "#from sentence_transformers import SentenceTransformer\n",
                            "\n",
                            "#encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
                            "\n",
                            "#url_encode = 'http://20.235.122.20:5000/encodeSentence'\n",
                            "url_encode = 'http://0.0.0.0:5200/encodeSentence'\n",
                            "\n",
                            "#def testCodeQl():\n",
                            "#    return None\n",
                            "\n",
                            "def returnEmbed( sent ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/createJsonFeats.py",
                        "method_nm": "def returnEmbed( sent ):",
                        "usage": [
                            "\n",
                            "def returnEmbed( sent ):\n",
                            "\n",
                            "    rec_ = { 'sentence': sent }\n",
                            "\n",
                            "    data = json.dumps( rec_ ).encode('utf-8')\n",
                            "    _request = urllib.request.Request( url_encode, data=data, method='POST', \\\n",
                            "                                        headers={'Content-Type': 'application/json'} )\n",
                            "\n",
                            "    response = urllib.request.urlopen( _request )\n",
                            "    string = response.read().decode('utf-8')\n",
                            "    json_obj = json.loads(string)\n",
                            "    \n",
                            "    return json_obj['encoded_'], True\n",
                            "\n",
                            "def returnJsonFeat( src_0, src_raw ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/createJsonFeats.py",
                        "method_nm": "def returnEmbed( sent ):",
                        "usage": [
                            "\n",
                            "def returnEmbed( sent ):\n",
                            "\n",
                            "    rec_ = { 'sentence': sent }\n",
                            "\n",
                            "    data = json.dumps( rec_ ).encode('utf-8')\n",
                            "    _request = urllib.request.Request( url_encode, data=data, method='POST', \\\n",
                            "                                        headers={'Content-Type': 'application/json'} )\n",
                            "\n",
                            "    response = urllib.request.urlopen( _request )\n",
                            "    string = response.read().decode('utf-8')\n",
                            "    json_obj = json.loads(string)\n",
                            "    \n",
                            "    return json_obj['encoded_'], True\n",
                            "\n",
                            "def returnJsonFeat( src_0, src_raw ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/createJsonFeats.py",
                        "method_nm": "def returnJsonFeat( src_0, src_raw ):",
                        "usage": [
                            "\n",
                            "def returnJsonFeat( src_0, src_raw ):\n",
                            "\n",
                            "    with open( src_0, 'r' ) as fp:\n",
                            "    #with open( src_0 + file_, 'r' ) as fp:\n",
                            "        json_ = json.load( fp )\n",
                            "\n",
                            "    with open( src_raw, 'r' ) as fp:\n",
                            "    #with open( src_raw + file_, 'r' ) as fp:\n",
                            "        json_raw = json.load( fp )\n",
                            "\n",
                            "    blockPrint()    \n",
                            "\n",
                            "    ## dummy\n",
                            "    file_ = ''\n",
                            "\n",
                            "    key_tuple_ = findKeys.processNeighbours( json_, json_raw, file_ )    \n",
                            "\n",
                            "    enablePrint()\n",
                            "\n",
                            "    doc_str_, dist_matrix_, xymatrix = '', [], []\n",
                            "\n",
                            "    for str_, norm_coords in key_tuple_:\n",
                            "        doc_str_ += ' ' + str_\n",
                            "\n",
                            "    #print( doc_str_ )\n",
                            "    #print( np.asarray( dist_matrix_ ).shape, dist_matrix_[0] )\n",
                            "    #return doc_str_, xymatrix\n",
                            "    rec_ = { 'sentence': doc_str_ }\n",
                            "\n",
                            "    data = json.dumps( rec_ ).encode('utf-8')\n",
                            "    _request = urllib.request.Request( url_encode, data=data, method='POST', headers={'Content-Type': 'application/json'} )\n",
                            "    response = urllib.request.urlopen( _request )\n",
                            "    string = response.read().decode('utf-8')\n",
                            "    json_obj = json.loads(string)\n",
                            "    \n",
                            "    return json_obj['encoded_'], key_tuple_\n"
                        ]
                    }
                ]
            }
        },
        {
            "findKeys": {
                "def": 2,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/createJsonFeats.py",
                        "method_nm": "def returnJsonFeat( src_0, src_raw ):",
                        "usage": [
                            "    key_tuple_ = findKeys.processNeighbours( json_, json_raw, file_ )    \n"
                        ]
                    }
                ]
            }
        },
        {
            "": {
                "def": 2,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/createJsonFeats.py",
                        "method_nm": "def blockPrint():",
                        "usage": [
                            "# Disable\n",
                            "def blockPrint():\n",
                            "    sys.stdout = open(os.devnull, 'w')\n",
                            "\n",
                            "# Restore\n",
                            "def enablePrint():\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/createJsonFeats.py",
                        "method_nm": "def enablePrint():",
                        "usage": [
                            "# Restore\n",
                            "def enablePrint():\n",
                            "    sys.stdout = sys.__stdout__\n",
                            "\n",
                            "\n",
                            "#from sentence_transformers import SentenceTransformer\n",
                            "\n",
                            "#encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
                            "\n",
                            "#url_encode = 'http://20.235.122.20:5000/encodeSentence'\n",
                            "url_encode = 'http://0.0.0.0:5200/encodeSentence'\n",
                            "\n",
                            "#def testCodeQl():\n",
                            "#    return None\n",
                            "\n",
                            "def returnEmbed( sent ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/createJsonFeats.py",
                        "method_nm": "def returnEmbed( sent ):",
                        "usage": [
                            "\n",
                            "def returnEmbed( sent ):\n",
                            "\n",
                            "    rec_ = { 'sentence': sent }\n",
                            "\n",
                            "    data = json.dumps( rec_ ).encode('utf-8')\n",
                            "    _request = urllib.request.Request( url_encode, data=data, method='POST', \\\n",
                            "                                        headers={'Content-Type': 'application/json'} )\n",
                            "\n",
                            "    response = urllib.request.urlopen( _request )\n",
                            "    string = response.read().decode('utf-8')\n",
                            "    json_obj = json.loads(string)\n",
                            "    \n",
                            "    return json_obj['encoded_'], True\n",
                            "\n",
                            "def returnJsonFeat( src_0, src_raw ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/createJsonFeats.py",
                        "method_nm": "def returnEmbed( sent ):",
                        "usage": [
                            "\n",
                            "def returnEmbed( sent ):\n",
                            "\n",
                            "    rec_ = { 'sentence': sent }\n",
                            "\n",
                            "    data = json.dumps( rec_ ).encode('utf-8')\n",
                            "    _request = urllib.request.Request( url_encode, data=data, method='POST', \\\n",
                            "                                        headers={'Content-Type': 'application/json'} )\n",
                            "\n",
                            "    response = urllib.request.urlopen( _request )\n",
                            "    string = response.read().decode('utf-8')\n",
                            "    json_obj = json.loads(string)\n",
                            "    \n",
                            "    return json_obj['encoded_'], True\n",
                            "\n",
                            "def returnJsonFeat( src_0, src_raw ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/createJsonFeats.py",
                        "method_nm": "def returnJsonFeat( src_0, src_raw ):",
                        "usage": [
                            "\n",
                            "def returnJsonFeat( src_0, src_raw ):\n",
                            "\n",
                            "    with open( src_0, 'r' ) as fp:\n",
                            "    #with open( src_0 + file_, 'r' ) as fp:\n",
                            "        json_ = json.load( fp )\n",
                            "\n",
                            "    with open( src_raw, 'r' ) as fp:\n",
                            "    #with open( src_raw + file_, 'r' ) as fp:\n",
                            "        json_raw = json.load( fp )\n",
                            "\n",
                            "    blockPrint()    \n",
                            "\n",
                            "    ## dummy\n",
                            "    file_ = ''\n",
                            "\n",
                            "    key_tuple_ = findKeys.processNeighbours( json_, json_raw, file_ )    \n",
                            "\n",
                            "    enablePrint()\n",
                            "\n",
                            "    doc_str_, dist_matrix_, xymatrix = '', [], []\n",
                            "\n",
                            "    for str_, norm_coords in key_tuple_:\n",
                            "        doc_str_ += ' ' + str_\n",
                            "\n",
                            "    #print( doc_str_ )\n",
                            "    #print( np.asarray( dist_matrix_ ).shape, dist_matrix_[0] )\n",
                            "    #return doc_str_, xymatrix\n",
                            "    rec_ = { 'sentence': doc_str_ }\n",
                            "\n",
                            "    data = json.dumps( rec_ ).encode('utf-8')\n",
                            "    _request = urllib.request.Request( url_encode, data=data, method='POST', headers={'Content-Type': 'application/json'} )\n",
                            "    response = urllib.request.urlopen( _request )\n",
                            "    string = response.read().decode('utf-8')\n",
                            "    json_obj = json.loads(string)\n",
                            "    \n",
                            "    return json_obj['encoded_'], key_tuple_\n"
                        ]
                    }
                ]
            }
        },
        {
            "urllib.request": {
                "def": 5,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/createJsonFeats.py",
                        "method_nm": "def returnEmbed( sent ):",
                        "usage": [
                            "    _request = urllib.request.Request( url_encode, data=data, method='POST', \\\n",
                            "    response = urllib.request.urlopen( _request )\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/createJsonFeats.py",
                        "method_nm": "def returnEmbed( sent ):",
                        "usage": [
                            "    _request = urllib.request.Request( url_encode, data=data, method='POST', \\\n",
                            "    response = urllib.request.urlopen( _request )\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/createJsonFeats.py",
                        "method_nm": "def returnJsonFeat( src_0, src_raw ):",
                        "usage": [
                            "    _request = urllib.request.Request( url_encode, data=data, method='POST', headers={'Content-Type': 'application/json'} )\n",
                            "    response = urllib.request.urlopen( _request )\n"
                        ]
                    }
                ]
            }
        },
        {
            "": {
                "def": 5,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/createJsonFeats.py",
                        "method_nm": "def blockPrint():",
                        "usage": [
                            "# Disable\n",
                            "def blockPrint():\n",
                            "    sys.stdout = open(os.devnull, 'w')\n",
                            "\n",
                            "# Restore\n",
                            "def enablePrint():\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/createJsonFeats.py",
                        "method_nm": "def enablePrint():",
                        "usage": [
                            "# Restore\n",
                            "def enablePrint():\n",
                            "    sys.stdout = sys.__stdout__\n",
                            "\n",
                            "\n",
                            "#from sentence_transformers import SentenceTransformer\n",
                            "\n",
                            "#encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
                            "\n",
                            "#url_encode = 'http://20.235.122.20:5000/encodeSentence'\n",
                            "url_encode = 'http://0.0.0.0:5200/encodeSentence'\n",
                            "\n",
                            "#def testCodeQl():\n",
                            "#    return None\n",
                            "\n",
                            "def returnEmbed( sent ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/createJsonFeats.py",
                        "method_nm": "def returnEmbed( sent ):",
                        "usage": [
                            "\n",
                            "def returnEmbed( sent ):\n",
                            "\n",
                            "    rec_ = { 'sentence': sent }\n",
                            "\n",
                            "    data = json.dumps( rec_ ).encode('utf-8')\n",
                            "    _request = urllib.request.Request( url_encode, data=data, method='POST', \\\n",
                            "                                        headers={'Content-Type': 'application/json'} )\n",
                            "\n",
                            "    response = urllib.request.urlopen( _request )\n",
                            "    string = response.read().decode('utf-8')\n",
                            "    json_obj = json.loads(string)\n",
                            "    \n",
                            "    return json_obj['encoded_'], True\n",
                            "\n",
                            "def returnJsonFeat( src_0, src_raw ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/createJsonFeats.py",
                        "method_nm": "def returnEmbed( sent ):",
                        "usage": [
                            "\n",
                            "def returnEmbed( sent ):\n",
                            "\n",
                            "    rec_ = { 'sentence': sent }\n",
                            "\n",
                            "    data = json.dumps( rec_ ).encode('utf-8')\n",
                            "    _request = urllib.request.Request( url_encode, data=data, method='POST', \\\n",
                            "                                        headers={'Content-Type': 'application/json'} )\n",
                            "\n",
                            "    response = urllib.request.urlopen( _request )\n",
                            "    string = response.read().decode('utf-8')\n",
                            "    json_obj = json.loads(string)\n",
                            "    \n",
                            "    return json_obj['encoded_'], True\n",
                            "\n",
                            "def returnJsonFeat( src_0, src_raw ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/createJsonFeats.py",
                        "method_nm": "def returnJsonFeat( src_0, src_raw ):",
                        "usage": [
                            "\n",
                            "def returnJsonFeat( src_0, src_raw ):\n",
                            "\n",
                            "    with open( src_0, 'r' ) as fp:\n",
                            "    #with open( src_0 + file_, 'r' ) as fp:\n",
                            "        json_ = json.load( fp )\n",
                            "\n",
                            "    with open( src_raw, 'r' ) as fp:\n",
                            "    #with open( src_raw + file_, 'r' ) as fp:\n",
                            "        json_raw = json.load( fp )\n",
                            "\n",
                            "    blockPrint()    \n",
                            "\n",
                            "    ## dummy\n",
                            "    file_ = ''\n",
                            "\n",
                            "    key_tuple_ = findKeys.processNeighbours( json_, json_raw, file_ )    \n",
                            "\n",
                            "    enablePrint()\n",
                            "\n",
                            "    doc_str_, dist_matrix_, xymatrix = '', [], []\n",
                            "\n",
                            "    for str_, norm_coords in key_tuple_:\n",
                            "        doc_str_ += ' ' + str_\n",
                            "\n",
                            "    #print( doc_str_ )\n",
                            "    #print( np.asarray( dist_matrix_ ).shape, dist_matrix_[0] )\n",
                            "    #return doc_str_, xymatrix\n",
                            "    rec_ = { 'sentence': doc_str_ }\n",
                            "\n",
                            "    data = json.dumps( rec_ ).encode('utf-8')\n",
                            "    _request = urllib.request.Request( url_encode, data=data, method='POST', headers={'Content-Type': 'application/json'} )\n",
                            "    response = urllib.request.urlopen( _request )\n",
                            "    string = response.read().decode('utf-8')\n",
                            "    json_obj = json.loads(string)\n",
                            "    \n",
                            "    return json_obj['encoded_'], key_tuple_\n"
                        ]
                    }
                ]
            }
        },
        {
            "sys": {
                "def": 7,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/createJsonFeats.py",
                        "method_nm": "def blockPrint():",
                        "usage": [
                            "    sys.stdout = open(os.devnull, 'w')\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/createJsonFeats.py",
                        "method_nm": "def enablePrint():",
                        "usage": [
                            "    sys.stdout = sys.__stdout__\n"
                        ]
                    }
                ]
            }
        },
        {
            "": {
                "def": 7,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/createJsonFeats.py",
                        "method_nm": "def blockPrint():",
                        "usage": [
                            "# Disable\n",
                            "def blockPrint():\n",
                            "    sys.stdout = open(os.devnull, 'w')\n",
                            "\n",
                            "# Restore\n",
                            "def enablePrint():\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/createJsonFeats.py",
                        "method_nm": "def enablePrint():",
                        "usage": [
                            "# Restore\n",
                            "def enablePrint():\n",
                            "    sys.stdout = sys.__stdout__\n",
                            "\n",
                            "\n",
                            "#from sentence_transformers import SentenceTransformer\n",
                            "\n",
                            "#encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
                            "\n",
                            "#url_encode = 'http://20.235.122.20:5000/encodeSentence'\n",
                            "url_encode = 'http://0.0.0.0:5200/encodeSentence'\n",
                            "\n",
                            "#def testCodeQl():\n",
                            "#    return None\n",
                            "\n",
                            "def returnEmbed( sent ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/createJsonFeats.py",
                        "method_nm": "def returnEmbed( sent ):",
                        "usage": [
                            "\n",
                            "def returnEmbed( sent ):\n",
                            "\n",
                            "    rec_ = { 'sentence': sent }\n",
                            "\n",
                            "    data = json.dumps( rec_ ).encode('utf-8')\n",
                            "    _request = urllib.request.Request( url_encode, data=data, method='POST', \\\n",
                            "                                        headers={'Content-Type': 'application/json'} )\n",
                            "\n",
                            "    response = urllib.request.urlopen( _request )\n",
                            "    string = response.read().decode('utf-8')\n",
                            "    json_obj = json.loads(string)\n",
                            "    \n",
                            "    return json_obj['encoded_'], True\n",
                            "\n",
                            "def returnJsonFeat( src_0, src_raw ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/createJsonFeats.py",
                        "method_nm": "def returnEmbed( sent ):",
                        "usage": [
                            "\n",
                            "def returnEmbed( sent ):\n",
                            "\n",
                            "    rec_ = { 'sentence': sent }\n",
                            "\n",
                            "    data = json.dumps( rec_ ).encode('utf-8')\n",
                            "    _request = urllib.request.Request( url_encode, data=data, method='POST', \\\n",
                            "                                        headers={'Content-Type': 'application/json'} )\n",
                            "\n",
                            "    response = urllib.request.urlopen( _request )\n",
                            "    string = response.read().decode('utf-8')\n",
                            "    json_obj = json.loads(string)\n",
                            "    \n",
                            "    return json_obj['encoded_'], True\n",
                            "\n",
                            "def returnJsonFeat( src_0, src_raw ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/createJsonFeats.py",
                        "method_nm": "def returnJsonFeat( src_0, src_raw ):",
                        "usage": [
                            "\n",
                            "def returnJsonFeat( src_0, src_raw ):\n",
                            "\n",
                            "    with open( src_0, 'r' ) as fp:\n",
                            "    #with open( src_0 + file_, 'r' ) as fp:\n",
                            "        json_ = json.load( fp )\n",
                            "\n",
                            "    with open( src_raw, 'r' ) as fp:\n",
                            "    #with open( src_raw + file_, 'r' ) as fp:\n",
                            "        json_raw = json.load( fp )\n",
                            "\n",
                            "    blockPrint()    \n",
                            "\n",
                            "    ## dummy\n",
                            "    file_ = ''\n",
                            "\n",
                            "    key_tuple_ = findKeys.processNeighbours( json_, json_raw, file_ )    \n",
                            "\n",
                            "    enablePrint()\n",
                            "\n",
                            "    doc_str_, dist_matrix_, xymatrix = '', [], []\n",
                            "\n",
                            "    for str_, norm_coords in key_tuple_:\n",
                            "        doc_str_ += ' ' + str_\n",
                            "\n",
                            "    #print( doc_str_ )\n",
                            "    #print( np.asarray( dist_matrix_ ).shape, dist_matrix_[0] )\n",
                            "    #return doc_str_, xymatrix\n",
                            "    rec_ = { 'sentence': doc_str_ }\n",
                            "\n",
                            "    data = json.dumps( rec_ ).encode('utf-8')\n",
                            "    _request = urllib.request.Request( url_encode, data=data, method='POST', headers={'Content-Type': 'application/json'} )\n",
                            "    response = urllib.request.urlopen( _request )\n",
                            "    string = response.read().decode('utf-8')\n",
                            "    json_obj = json.loads(string)\n",
                            "    \n",
                            "    return json_obj['encoded_'], key_tuple_\n"
                        ]
                    }
                ]
            }
        },
        {
            "os": {
                "def": 7,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/createJsonFeats.py",
                        "method_nm": "def blockPrint():",
                        "usage": [
                            "    sys.stdout = open(os.devnull, 'w')\n"
                        ]
                    }
                ]
            }
        }
    ],
    "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/db_utils.py": [
        {
            "createJsonFeats": 0
        },
        {
            "": {
                "def": 0,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/db_utils.py",
                        "method_nm": "def returnBlankDBRec():",
                        "usage": [
                            "\n",
                            "def returnBlankDBRec():\n",
                            "    dbRec_ = dict()\n",
                            "    dbRec_['docID'] = ''\n",
                            "    dbRec_['docSignature'] = []\n",
                            "    dbRec_['tupArr'] = []\n",
                            "    dbRec_['ocr_op'] = [] ## assing raw ocr op ['lines']\n",
                            "    dbRec_['dimension'] = [] ## assing raw ocr op ht, wd\n",
                            "    dbRec_['tableFeedback'] = dict()\n",
                            "    dbRec_['feedbackDict'] = [ { 'config_field_nm': '',\\\n",
                            "                               'field_co_ords':[],\\\n",
                            "                               'field_datatype': '',\\\n",
                            "                               'feedback_value': '',\\\n",
                            "                               'local_neigh_dict': dict() } ]\n",
                            "    dbRec_['exception_feedback'] = [] ## will contain dicts of fmt -> \n",
                            "            ## { 'docID':, 'failed_fields': [ { 'config_field_nm':, 'feedback_value':, 'feedback_co_ords':, 'comments;' } ]\n",
                            "    dbRec_['success_feedback'] = [] ## array of dicts        \n",
                            "    ## { 'docID':, 'passed_fields': [ { 'config_field_nm':, 'local_field':, 'feedback_value':, 'feedback_co_ords': , 'comments': } ]\n",
                            "\n",
                            "    return dbRec_\n",
                            "\n",
                            "def insertNewSignature( rec_ ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/db_utils.py",
                        "method_nm": "def insertNewSignature( rec_ ):",
                        "usage": [
                            "\n",
                            "def insertNewSignature( rec_ ):\n",
                            "\n",
                            "    data = json.dumps( rec_ ).encode('utf-8')\n",
                            "    insert_request = urllib.request.Request( url_insert, data=data, method='POST', \\\n",
                            "                                              headers={'Content-Type': 'application/json'})\n",
                            "\n",
                            "    response = urllib.request.urlopen( insert_request )\n",
                            "    string = response.read().decode('utf-8')\n",
                            "\n",
                            "    return string\n",
                            "    \n",
                            "def updateSignature( rec_ ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/db_utils.py",
                        "method_nm": "def updateSignature( rec_ ):",
                        "usage": [
                            "    \n",
                            "def updateSignature( rec_ ):\n",
                            "\n",
                            "    data = json.dumps( rec_ ).encode('utf-8')\n",
                            "    insert_request = urllib.request.Request( url_update, data=data, method='POST', \\\n",
                            "                                              headers={'Content-Type': 'application/json'})\n",
                            "\n",
                            "    response = urllib.request.urlopen( insert_request )\n",
                            "    string = response.read().decode('utf-8')\n",
                            "\n",
                            "    return string\n",
                            "\n",
                            "def searchSignature( rec_ ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/db_utils.py",
                        "method_nm": "def updateSignature( rec_ ):",
                        "usage": [
                            "    \n",
                            "def updateSignature( rec_ ):\n",
                            "\n",
                            "    data = json.dumps( rec_ ).encode('utf-8')\n",
                            "    insert_request = urllib.request.Request( url_update, data=data, method='POST', \\\n",
                            "                                              headers={'Content-Type': 'application/json'})\n",
                            "\n",
                            "    response = urllib.request.urlopen( insert_request )\n",
                            "    string = response.read().decode('utf-8')\n",
                            "\n",
                            "    return string\n",
                            "\n",
                            "def searchSignature( rec_ ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/db_utils.py",
                        "method_nm": "def searchSignature( rec_ ):",
                        "usage": [
                            "\n",
                            "def searchSignature( rec_ ):\n",
                            "\n",
                            "    data = json.dumps( rec_ ).encode('utf-8')\n",
                            "    search_request = urllib.request.Request( url_search, data=data, method='POST', \\\n",
                            "                                                headers={'Content-Type': 'application/json'} )\n",
                            "    response = urllib.request.urlopen( search_request )\n",
                            "    string = response.read().decode('utf-8')\n",
                            "    json_obj = json.loads(string)\n",
                            "\n",
                            "    return json_obj\n"
                        ]
                    }
                ]
            }
        },
        {
            "json": {
                "def": 1,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/db_utils.py",
                        "method_nm": "def insertNewSignature( rec_ ):",
                        "usage": [
                            "    data = json.dumps( rec_ ).encode('utf-8')\n",
                            "                                              headers={'Content-Type': 'application/json'})\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/db_utils.py",
                        "method_nm": "def updateSignature( rec_ ):",
                        "usage": [
                            "    data = json.dumps( rec_ ).encode('utf-8')\n",
                            "                                              headers={'Content-Type': 'application/json'})\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/db_utils.py",
                        "method_nm": "def updateSignature( rec_ ):",
                        "usage": [
                            "    data = json.dumps( rec_ ).encode('utf-8')\n",
                            "                                              headers={'Content-Type': 'application/json'})\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/db_utils.py",
                        "method_nm": "def searchSignature( rec_ ):",
                        "usage": [
                            "    data = json.dumps( rec_ ).encode('utf-8')\n",
                            "                                                headers={'Content-Type': 'application/json'} )\n",
                            "    json_obj = json.loads(string)\n",
                            "    return json_obj\n"
                        ]
                    }
                ]
            }
        },
        {
            "": {
                "def": 1,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/db_utils.py",
                        "method_nm": "def returnBlankDBRec():",
                        "usage": [
                            "\n",
                            "def returnBlankDBRec():\n",
                            "    dbRec_ = dict()\n",
                            "    dbRec_['docID'] = ''\n",
                            "    dbRec_['docSignature'] = []\n",
                            "    dbRec_['tupArr'] = []\n",
                            "    dbRec_['ocr_op'] = [] ## assing raw ocr op ['lines']\n",
                            "    dbRec_['dimension'] = [] ## assing raw ocr op ht, wd\n",
                            "    dbRec_['tableFeedback'] = dict()\n",
                            "    dbRec_['feedbackDict'] = [ { 'config_field_nm': '',\\\n",
                            "                               'field_co_ords':[],\\\n",
                            "                               'field_datatype': '',\\\n",
                            "                               'feedback_value': '',\\\n",
                            "                               'local_neigh_dict': dict() } ]\n",
                            "    dbRec_['exception_feedback'] = [] ## will contain dicts of fmt -> \n",
                            "            ## { 'docID':, 'failed_fields': [ { 'config_field_nm':, 'feedback_value':, 'feedback_co_ords':, 'comments;' } ]\n",
                            "    dbRec_['success_feedback'] = [] ## array of dicts        \n",
                            "    ## { 'docID':, 'passed_fields': [ { 'config_field_nm':, 'local_field':, 'feedback_value':, 'feedback_co_ords': , 'comments': } ]\n",
                            "\n",
                            "    return dbRec_\n",
                            "\n",
                            "def insertNewSignature( rec_ ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/db_utils.py",
                        "method_nm": "def insertNewSignature( rec_ ):",
                        "usage": [
                            "\n",
                            "def insertNewSignature( rec_ ):\n",
                            "\n",
                            "    data = json.dumps( rec_ ).encode('utf-8')\n",
                            "    insert_request = urllib.request.Request( url_insert, data=data, method='POST', \\\n",
                            "                                              headers={'Content-Type': 'application/json'})\n",
                            "\n",
                            "    response = urllib.request.urlopen( insert_request )\n",
                            "    string = response.read().decode('utf-8')\n",
                            "\n",
                            "    return string\n",
                            "    \n",
                            "def updateSignature( rec_ ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/db_utils.py",
                        "method_nm": "def updateSignature( rec_ ):",
                        "usage": [
                            "    \n",
                            "def updateSignature( rec_ ):\n",
                            "\n",
                            "    data = json.dumps( rec_ ).encode('utf-8')\n",
                            "    insert_request = urllib.request.Request( url_update, data=data, method='POST', \\\n",
                            "                                              headers={'Content-Type': 'application/json'})\n",
                            "\n",
                            "    response = urllib.request.urlopen( insert_request )\n",
                            "    string = response.read().decode('utf-8')\n",
                            "\n",
                            "    return string\n",
                            "\n",
                            "def searchSignature( rec_ ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/db_utils.py",
                        "method_nm": "def updateSignature( rec_ ):",
                        "usage": [
                            "    \n",
                            "def updateSignature( rec_ ):\n",
                            "\n",
                            "    data = json.dumps( rec_ ).encode('utf-8')\n",
                            "    insert_request = urllib.request.Request( url_update, data=data, method='POST', \\\n",
                            "                                              headers={'Content-Type': 'application/json'})\n",
                            "\n",
                            "    response = urllib.request.urlopen( insert_request )\n",
                            "    string = response.read().decode('utf-8')\n",
                            "\n",
                            "    return string\n",
                            "\n",
                            "def searchSignature( rec_ ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/db_utils.py",
                        "method_nm": "def searchSignature( rec_ ):",
                        "usage": [
                            "\n",
                            "def searchSignature( rec_ ):\n",
                            "\n",
                            "    data = json.dumps( rec_ ).encode('utf-8')\n",
                            "    search_request = urllib.request.Request( url_search, data=data, method='POST', \\\n",
                            "                                                headers={'Content-Type': 'application/json'} )\n",
                            "    response = urllib.request.urlopen( search_request )\n",
                            "    string = response.read().decode('utf-8')\n",
                            "    json_obj = json.loads(string)\n",
                            "\n",
                            "    return json_obj\n"
                        ]
                    }
                ]
            }
        },
        {
            "urllib": {
                "def": 1,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/db_utils.py",
                        "method_nm": "def insertNewSignature( rec_ ):",
                        "usage": [
                            "    insert_request = urllib.request.Request( url_insert, data=data, method='POST', \\\n",
                            "    response = urllib.request.urlopen( insert_request )\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/db_utils.py",
                        "method_nm": "def updateSignature( rec_ ):",
                        "usage": [
                            "    insert_request = urllib.request.Request( url_update, data=data, method='POST', \\\n",
                            "    response = urllib.request.urlopen( insert_request )\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/db_utils.py",
                        "method_nm": "def updateSignature( rec_ ):",
                        "usage": [
                            "    insert_request = urllib.request.Request( url_update, data=data, method='POST', \\\n",
                            "    response = urllib.request.urlopen( insert_request )\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/db_utils.py",
                        "method_nm": "def searchSignature( rec_ ):",
                        "usage": [
                            "    search_request = urllib.request.Request( url_search, data=data, method='POST', \\\n",
                            "    response = urllib.request.urlopen( search_request )\n"
                        ]
                    }
                ]
            }
        }
    ],
    "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/fetchLLMResponse.py": [
        {
            "os": 0
        },
        {
            "openai": 1
        },
        {
            "OpenAI": {
                "def": 1,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/fetchLLMResponse.py",
                        "method_nm": "def returnOpenAI_response( dataframe ):",
                        "usage": [
                            "def returnOpenAI_response( dataframe ):\n"
                        ]
                    }
                ]
            }
        }
    ],
    "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/findKeys.py": [
        {
            "json": {
                "def": 0,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/findKeys.py",
                        "method_nm": "def allNum(",
                        "usage": [
                            "def euclid( refpts, pts, json_ ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/findKeys.py",
                        "method_nm": "def euclid( refpts, pts, json_ ):",
                        "usage": [
                            "def euclid( refpts, pts, json_ ):\n",
                            "    ht, wd = json_['height'], json_['width']\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/findKeys.py",
                        "method_nm": "def findWdFeats(",
                        "usage": [
                            "def findRaw( ids, json_raw ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/findKeys.py",
                        "method_nm": "def findRaw( ids, json_raw ):",
                        "usage": [
                            "def findRaw( ids, json_raw ):\n",
                            "      for line in json_raw['lines']:\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/findKeys.py",
                        "method_nm": "def featNum( txt ):",
                        "usage": [
                            "def findNeighBour( ref_wd_ctr, lctr, json_, ref_txt, ref_pts ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/findKeys.py",
                        "method_nm": "def findNeighBour( ref_wd_ctr, lctr, json_, ref_txt, ref_pts ):",
                        "usage": [
                            "def findNeighBour( ref_wd_ctr, lctr, json_, ref_txt, ref_pts ):\n",
                            "    for rt_wd_ctr in range( ref_wd_ctr, len( json_['lines'][lctr]) ):\n",
                            "      rt_wd = json_['lines'][lctr][rt_wd_ctr]\n",
                            "    for ctr in range( min( lctr + 1, len( json_['lines'] )-1 ), min( lctr + 4, len( json_['lines'] )-1 ) ):\n",
                            "      curr_line ,line_ = json_['lines'][lctr], json_['lines'][ctr]\n",
                            "def findValNeighBour( ref_wd_ctr, lctr, json_ ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/findKeys.py",
                        "method_nm": "def findValNeighBour( ref_wd_ctr, lctr, json_ ): ",
                        "usage": [
                            "def findValNeighBour( ref_wd_ctr, lctr, json_ ):\n",
                            "    curr_wd = json_['lines'][lctr][ref_wd_ctr]\n",
                            "      lt_wd = json_['lines'][lctr][lt_wd_ctr]\n",
                            "      curr_line ,line_ = json_['lines'][lctr], json_['lines'][ctr]\n",
                            "def neighContours( txt_, pts_, json_, conj_lt=None, conj_rt=None ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/findKeys.py",
                        "method_nm": "def neighContours(",
                        "usage": [
                            "def neighContours( txt_, pts_, json_, conj_lt=None, conj_rt=None ):\n",
                            "    for linectr in range( len( json_['lines'] ) ):\n",
                            "      line_ = json_['lines'][linectr]\n",
                            "def neighContours_old( txt_, pts_, json_, conj_lt=None, conj_rt=None ):\n",
                            "    for linectr in range( len( json_['lines'] ) ):\n",
                            "      line_ = json_['lines'][linectr]\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/findKeys.py",
                        "method_nm": "def isNum( txt ):",
                        "usage": [
                            "def processNeighbours( json_, json_raw, fileNm ):\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/findKeys.py",
                        "method_nm": "def processNeighbours( json_, json_raw, fileNm ):",
                        "usage": [
                            "def processNeighbours( json_, json_raw, fileNm ):\n",
                            "    ht_, wd_ = json_['height'], json_['width']\n",
                            "    for linectr in range( len( json_['lines'] ) ):\n",
                            "      line_ = json_['lines'][linectr]\n",
                            "      print( 'TWIN->', line_, json_raw['lines'][linectr] )\n",
                            "        txtarr, ptsarr = findRaw( line_[ wdctr ]['ids'], json_raw )\n",
                            "                                         findNeighBour( wdctr , linectr, json_, _tmp, ptsarr[ ctr ] )\n",
                            "            typeOfNeighForVal = findValNeighBour( wdctr , linectr, json_)\n",
                            "                          neighContours( contour_arr_[ ctr ],pts_arr_[ ctr ], json_, conjoined_lt, conjoined_rt )\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/findKeys.py",
                        "method_nm": "if __name__ == '__main__':",
                        "usage": [
                            "            json_raw = json.load( fp )\n",
                            "            json_ = json.load( fp )\n",
                            "          processNeighbours( json_, json_raw, fnm ) \n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/findKeys.py",
                        "method_nm": "if __name__ == '__main__':",
                        "usage": [
                            "            json_raw = json.load( fp )\n",
                            "            json_ = json.load( fp )\n",
                            "          processNeighbours( json_, json_raw, fnm ) \n"
                        ]
                    }
                ]
            }
        },
        {
            "sys": {
                "def": 0,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/findKeys.py",
                        "method_nm": "if __name__ == '__main__':",
                        "usage": [
                            "    fnm_ = sys.argv[1]\n",
                            "    folder_ = sys.argv[2]\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/findKeys.py",
                        "method_nm": "if __name__ == '__main__':",
                        "usage": [
                            "    fnm_ = sys.argv[1]\n",
                            "    folder_ = sys.argv[2]\n"
                        ]
                    }
                ]
            }
        },
        {
            "random": 0
        },
        {
            "cv2": {
                "def": 0,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/findKeys.py",
                        "method_nm": "isNum( elem[0] )",
                        "usage": [
                            "    #cv2.imwrite( './RES/contoured_'+fileNm, imgFile_ )  \n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/findKeys.py",
                        "method_nm": "if __name__ == '__main__':",
                        "usage": [
                            "          #imgFile_ = cv2.imread( './safe/' + fileNm )\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/findKeys.py",
                        "method_nm": "if __name__ == '__main__':",
                        "usage": [
                            "          #imgFile_ = cv2.imread( './safe/' + fileNm )\n"
                        ]
                    }
                ]
            }
        },
        {
            "numpy": 1
        },
        {
            "np": {
                "def": 1,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/findKeys.py",
                        "method_nm": "def findValFeats( refwd ):",
                        "usage": [
                            "    returnFeats = np.zeros(( numValFeats ))\n",
                            "    returnFeats[6] = np.median( lenwds_ )\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/findKeys.py",
                        "method_nm": "def findWdFeats(",
                        "usage": [
                            "    returnFeats = np.zeros((6))\n",
                            "    #returnFeats = np.zeros((total_feats_))\n",
                            "    #if len( txt.split() ) < 1: return returnFeats.tolist() + np.zeros((4)).tolist()\n",
                            "    returnFeats[5] = np.median( lenwds_ )\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/findKeys.py",
                        "method_nm": "def neighContours(",
                        "usage": [
                            "    x_vertical, y_horizontal = np.zeros((6, ( upper_neigh + lower_neigh ))), np.zeros((6, ( left_neigh + rt_neigh )))\n",
                            "    respFeat_ = np.zeros((6*4)) \n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/findKeys.py",
                        "method_nm": "def processNeighbours( json_, json_raw, fileNm ):",
                        "usage": [
                            "    print('MEDICI->', np.median( np.asarray( medstore ) ) )\n",
                            "    cluster_input_txt, simple_input, cluster_input_num = dict(), dict(), dict()\n",
                            "    ## create input for XFORMER\n",
                            "    finalInp_ = []\n",
                            "          finalInp_.append( ( cont_, pts_, 'KEY-VALUE-RIGHT', neigh_vert, neigh_hor ) )\n",
                            "          finalInp_.append( ( cont_, pts_, 'KEY-VALUE-BELOW', neigh_vert, neigh_hor ) )\n",
                            "          finalInp_.append( ( cont_, pts_, 'VALUE-KEY-LEFT', neigh_vert, neigh_hor ) )\n",
                            "          finalInp_.append( ( cont_, pts_, 'VALUE-KEY-TOP', neigh_vert, neigh_hor ) )\n",
                            "            finalInp_.append( ( keyTxt, keypts, 'KEY-VALUE-RIGHT', neigh_vert, neo_hor ) )\n",
                            "            finalInp_.append( ( valTxt, valpts, 'VALUE-KEY-LEFT', neigh_vert, neo_hor1 ) )\n",
                            "            print('BOOMER->', finalInp_[-2], finalInp_[-1] )\n",
                            "            finalInp_.append( ( keyTxt, keypts, 'KEY-VALUE-RIGHT', neigh_vert, neigh_hor ) )\n",
                            "            finalInp_.append( ( valTxt, valpts, 'VALUE-KEY-LEFT', neigh_vert, neigh_hor ) )\n",
                            "            print('BOOMER VAL FIRST->', finalInp_[-2], finalInp_[-1] )\n",
                            "            finalInp_.append( ( cont_, pts_, 'IRR-NA', neigh_vert, neigh_hor ) )\n",
                            "    print('TRENCH WARFARE', finalInp_)\n",
                            "    resInp_ = []\n",
                            "    for elem in finalInp_:\n",
                            "        for inner in finalInp_:\n",
                            "            resInp_.append( elem )\n",
                            "        if changed_ is False: resInp_.append( elem )\n",
                            "          if np.sum( np.asarray( hor_[2] ) ) == 0:\n",
                            "            for local_ in finalInp_:\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/findKeys.py",
                        "method_nm": "findWdFeats( local_[0] ).tolist()",
                        "usage": [
                            "          if np.sum( np.asarray( hor_[3] ) ) == 0:\n",
                            "            for local_ in finalInp_:\n",
                            "        resInp_.append( elem )\n",
                            "    #print('EFFING RESULT->', resInp_[0])\n",
                            "    for elem in resInp_:\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/findKeys.py",
                        "method_nm": "isNum( elem[0] )",
                        "usage": [
                            "    for elem in resInp_:\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/findKeys.py",
                        "method_nm": "if __name__ == '__main__':",
                        "usage": [
                            "      if 'output' in fnm or 'input' in fnm or 'global' in fnm: continue\n"
                        ]
                    },
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/findKeys.py",
                        "method_nm": "if __name__ == '__main__':",
                        "usage": [
                            "      if 'output' in fnm or 'input' in fnm or 'global' in fnm: continue\n"
                        ]
                    }
                ]
            }
        },
        {
            "scipy.spatial.distance": 19
        },
        {
            "distance": {
                "def": 19,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/findKeys.py",
                        "method_nm": "def euclid( refpts, pts, json_ ):",
                        "usage": [
                            "    return distance.euclidean( refpts, pts )\n"
                        ]
                    }
                ]
            }
        },
        {
            "sklearn.cluster.KMeans": 19
        },
        {
            "KMeans": 19
        },
        {
            "collections": 4
        },
        {
            "Counter": 4
        }
    ],
    "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/firstpass.py": [
        {
            "openpyxl": 0
        },
        {
            "": {
                "def": 0,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/firstpass.py",
                        "method_nm": "def find_table_bounds(sheet):",
                        "usage": [
                            "\n",
                            "def find_table_bounds(sheet):\n",
                            "    max_row = sheet.max_row\n",
                            "    max_col = sheet.max_column\n",
                            "\n",
                            "    # Initialize variables to track the bounds\n",
                            "    start_row ,end_row ,start_col ,end_col = None, None, None, None\n",
                            "\n",
                            "    # Iterate over rows to find the start and end rows\n",
                            "    for row in range(1, max_row + 1):\n",
                            "        if all(sheet.cell(row=row, column=col).value is None for col in range(1, max_col + 1)):\n",
                            "            if start_row is None:\n",
                            "                continue  # Skip empty rows before the table\n",
                            "            else:\n",
                            "                end_row = row - 1\n",
                            "                break\n",
                            "        elif start_row is None:\n",
                            "            start_row = row\n",
                            "    \n",
                            "    # Iterate over columns to find the start and end columns\n",
                            "    for col in range(1, max_col + 1):\n",
                            "        #for row in range(start_row, end_row):\n",
                            "        #    print('ROW NUM->', col, ' VALUE: ', sheet.cell(row=row, column=col).value )\n",
                            "\n",
                            "        if all(sheet.cell(row=row, column=col).value is None for row in range(start_row, end_row)):\n",
                            "            if start_col is None:\n",
                            "                continue  # Skip empty columns before the table\n",
                            "            else:\n",
                            "                end_col = col - 1\n",
                            "                break\n",
                            "        elif start_col is None:\n",
                            "            start_col = col\n",
                            "\n",
                            "    return start_row, end_row, start_col, end_col\n"
                        ]
                    }
                ]
            }
        }
    ],
    "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/pca_test.py": [
        {
            "numpy": 0
        },
        {
            "np": 0
        },
        {
            "sklearn.decomposition": 1
        },
        {
            "PCA": 1
        }
    ],
    "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/query_gpt_via_groq.py": [
        {
            "os": 0
        },
        {
            "ast": 0
        },
        {
            "time": 0
        },
        {
            "math": 0
        },
        {
            "json": 0
        },
        {
            "openai": 5
        },
        {
            "OpenAI": 5
        }
    ],
    "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/query_llama3_via_groq.py": [
        {
            "os": 0
        },
        {
            "ast": 0
        },
        {
            "time": 0
        },
        {
            "math": 0
        },
        {
            "json": 0
        }
    ],
    "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/readSS.py": [
        {
            "openpyxl": {
                "def": 0,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/readSS.py",
                        "method_nm": "def read_excel_file(file_path):",
                        "usage": [
                            "    workbook = openpyxl.load_workbook( file_path, read_only=True )\n"
                        ]
                    }
                ]
            }
        },
        {
            "": {
                "def": 0,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/readSS.py",
                        "method_nm": "def read_excel_file(file_path):",
                        "usage": [
                            "\n",
                            "def read_excel_file(file_path):\n",
                            "    # Load the workbook\n",
                            "    workbook = openpyxl.load_workbook( file_path, read_only=True )\n",
                            "    # Get the specified sheet in the workbook\n",
                            "    localD = dict()\n",
                            "\n",
                            "    for sheet_name in workbook.sheetnames:\n",
                            "        sheet = workbook[sheet_name]\n",
                            "        time.sleep( 1 )\n",
                            "        print('Iterating over sheet->', file_path, sheet_name)\n",
                            "        \n",
                            "        num_rows_to_consider_ , frame_ = 4, ''\n",
                            "        try:\n",
                            "            for rowidx, row in enumerate( sheet.iter_rows(values_only=True) ):\n",
                            "\n",
                            "                if rowidx > num_rows_to_consider_: break\n",
                            "                for cell in row:\n",
                            "                    frame_ += str(cell) + '\\t'\n",
                            "\n",
                            "                frame_ += '\\n'\n",
                            "\n",
                            "            print('Sending to LLM for summary->', frame_)\n",
                            "\n",
                            "            summary_ = groq_first_pass.returnLLMResponse( frame_ )\n",
                            "            ## append file name, sheet name\n",
                            "            localD[ sheet_name ] = file_path.split('/')[-1] + ' ' + sheet_name + ' ' + summary_\n",
                            "        except:\n",
                            "            print( 'EXCPN-> '+file_path + ' ' + sheet_name + ' ' + traceback.format_exc() )\n",
                            "\n",
                            "    finalJson_[ file_path.split('/')[-1] ] = localD\n"
                        ]
                    }
                ]
            }
        },
        {
            "groq_first_pass": {
                "def": 1,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/readSS.py",
                        "method_nm": "def read_excel_file(file_path):",
                        "usage": [
                            "            summary_ = groq_first_pass.returnLLMResponse( frame_ )\n"
                        ]
                    }
                ]
            }
        },
        {
            "": {
                "def": 1,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/readSS.py",
                        "method_nm": "def read_excel_file(file_path):",
                        "usage": [
                            "\n",
                            "def read_excel_file(file_path):\n",
                            "    # Load the workbook\n",
                            "    workbook = openpyxl.load_workbook( file_path, read_only=True )\n",
                            "    # Get the specified sheet in the workbook\n",
                            "    localD = dict()\n",
                            "\n",
                            "    for sheet_name in workbook.sheetnames:\n",
                            "        sheet = workbook[sheet_name]\n",
                            "        time.sleep( 1 )\n",
                            "        print('Iterating over sheet->', file_path, sheet_name)\n",
                            "        \n",
                            "        num_rows_to_consider_ , frame_ = 4, ''\n",
                            "        try:\n",
                            "            for rowidx, row in enumerate( sheet.iter_rows(values_only=True) ):\n",
                            "\n",
                            "                if rowidx > num_rows_to_consider_: break\n",
                            "                for cell in row:\n",
                            "                    frame_ += str(cell) + '\\t'\n",
                            "\n",
                            "                frame_ += '\\n'\n",
                            "\n",
                            "            print('Sending to LLM for summary->', frame_)\n",
                            "\n",
                            "            summary_ = groq_first_pass.returnLLMResponse( frame_ )\n",
                            "            ## append file name, sheet name\n",
                            "            localD[ sheet_name ] = file_path.split('/')[-1] + ' ' + sheet_name + ' ' + summary_\n",
                            "        except:\n",
                            "            print( 'EXCPN-> '+file_path + ' ' + sheet_name + ' ' + traceback.format_exc() )\n",
                            "\n",
                            "    finalJson_[ file_path.split('/')[-1] ] = localD\n"
                        ]
                    }
                ]
            }
        },
        {
            "time": {
                "def": 3,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/readSS.py",
                        "method_nm": "def read_excel_file(file_path):",
                        "usage": [
                            "        time.sleep( 1 )\n"
                        ]
                    }
                ]
            }
        },
        {
            "": {
                "def": 3,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/readSS.py",
                        "method_nm": "def read_excel_file(file_path):",
                        "usage": [
                            "\n",
                            "def read_excel_file(file_path):\n",
                            "    # Load the workbook\n",
                            "    workbook = openpyxl.load_workbook( file_path, read_only=True )\n",
                            "    # Get the specified sheet in the workbook\n",
                            "    localD = dict()\n",
                            "\n",
                            "    for sheet_name in workbook.sheetnames:\n",
                            "        sheet = workbook[sheet_name]\n",
                            "        time.sleep( 1 )\n",
                            "        print('Iterating over sheet->', file_path, sheet_name)\n",
                            "        \n",
                            "        num_rows_to_consider_ , frame_ = 4, ''\n",
                            "        try:\n",
                            "            for rowidx, row in enumerate( sheet.iter_rows(values_only=True) ):\n",
                            "\n",
                            "                if rowidx > num_rows_to_consider_: break\n",
                            "                for cell in row:\n",
                            "                    frame_ += str(cell) + '\\t'\n",
                            "\n",
                            "                frame_ += '\\n'\n",
                            "\n",
                            "            print('Sending to LLM for summary->', frame_)\n",
                            "\n",
                            "            summary_ = groq_first_pass.returnLLMResponse( frame_ )\n",
                            "            ## append file name, sheet name\n",
                            "            localD[ sheet_name ] = file_path.split('/')[-1] + ' ' + sheet_name + ' ' + summary_\n",
                            "        except:\n",
                            "            print( 'EXCPN-> '+file_path + ' ' + sheet_name + ' ' + traceback.format_exc() )\n",
                            "\n",
                            "    finalJson_[ file_path.split('/')[-1] ] = localD\n"
                        ]
                    }
                ]
            }
        },
        {
            "os": 40
        },
        {
            "": {
                "def": 40,
                "local_uses": [
                    {
                        "file_path": "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/readSS.py",
                        "method_nm": "def read_excel_file(file_path):",
                        "usage": [
                            "\n",
                            "def read_excel_file(file_path):\n",
                            "    # Load the workbook\n",
                            "    workbook = openpyxl.load_workbook( file_path, read_only=True )\n",
                            "    # Get the specified sheet in the workbook\n",
                            "    localD = dict()\n",
                            "\n",
                            "    for sheet_name in workbook.sheetnames:\n",
                            "        sheet = workbook[sheet_name]\n",
                            "        time.sleep( 1 )\n",
                            "        print('Iterating over sheet->', file_path, sheet_name)\n",
                            "        \n",
                            "        num_rows_to_consider_ , frame_ = 4, ''\n",
                            "        try:\n",
                            "            for rowidx, row in enumerate( sheet.iter_rows(values_only=True) ):\n",
                            "\n",
                            "                if rowidx > num_rows_to_consider_: break\n",
                            "                for cell in row:\n",
                            "                    frame_ += str(cell) + '\\t'\n",
                            "\n",
                            "                frame_ += '\\n'\n",
                            "\n",
                            "            print('Sending to LLM for summary->', frame_)\n",
                            "\n",
                            "            summary_ = groq_first_pass.returnLLMResponse( frame_ )\n",
                            "            ## append file name, sheet name\n",
                            "            localD[ sheet_name ] = file_path.split('/')[-1] + ' ' + sheet_name + ' ' + summary_\n",
                            "        except:\n",
                            "            print( 'EXCPN-> '+file_path + ' ' + sheet_name + ' ' + traceback.format_exc() )\n",
                            "\n",
                            "    finalJson_[ file_path.split('/')[-1] ] = localD\n"
                        ]
                    }
                ]
            }
        },
        {
            "json": 40
        }
    ],
    "/datadrive/IKG/LLM_INTERFACE/SRC_DIR/validate_search.py": [
        {
            "createJsonFeats": 0
        },
        {
            "cj": 0
        },
        {
            "scipy.spatial": 1
        },
        {
            "distance": 1
        }
    ]
}