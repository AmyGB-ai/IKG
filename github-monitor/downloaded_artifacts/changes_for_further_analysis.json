[{"file": "LLM_INTERFACE/LLM_Interface.py", "old_start": 5, "old_length": 17, "new_start": 5, "new_length": 18, "old_code": ["def __init__(self, src_folder_, dest_folder_, cfg_path_=\"./config.json\", llm_='LLAMA'):", "self.src_folder_ = src_folder_", "self.dest_folder_ = dest_folder_"], "new_code": ["def __init__(self, cfg_path_=\"./config.json\", llm_='LLAMA'):", "self.src_folder_ = self.config_['SRC_FOLDER']", "self.dest_folder_ = self.config_['DEST_FOLDER']", "self.code_ctx_win_ = self.config_['CODE_CONTEXT_WINDOW']"], "method_class_nm_old": {"class_nm": null, "method_nm": null}, "method_class_nm_new": {"class_nm": null, "method_nm": null}}, {"file": "LLM_INTERFACE/LLM_Interface.py", "old_start": 25, "old_length": 14, "new_start": 26, "new_length": 13, "old_code": ["print('Change for testing github actions#14')", "x = 8", "y = 10"], "new_code": ["self.var_json_file, self.pack_json_file, self.method_json_file = \\", "self.config_['VAR_JSON'], self.config_['PACK_JSON'], self.config_['METHOD_JSON']"], "method_class_nm_old": {"class_nm": "LLM_interface", "method_nm": "__init__"}, "method_class_nm_new": {"class_nm": "LLM_interface", "method_nm": "__init__"}}, {"file": "LLM_INTERFACE/LLM_Interface.py", "old_start": 42, "old_length": 7, "new_start": 42, "new_length": 6, "old_code": ["print('Another test for github actions#231')"], "new_code": [], "method_class_nm_old": {"class_nm": "LLM_interface", "method_nm": "method_contains_variable"}, "method_class_nm_new": {"class_nm": "LLM_interface", "method_nm": "method_contains_variable"}}, {"file": "LLM_INTERFACE/LLM_Interface.py", "old_start": 88, "old_length": 7, "new_start": 87, "new_length": 6, "old_code": ["print('Another test for github actions#263')"], "new_code": [], "method_class_nm_old": {"class_nm": "LLM_interface", "method_nm": "findUsage"}, "method_class_nm_new": {"class_nm": "LLM_interface", "method_nm": "findUsage"}}, {"file": "LLM_INTERFACE/LLM_Interface.py", "old_start": 119, "old_length": 7, "new_start": 117, "new_length": 6, "old_code": ["print('Another test for github actions#2301')"], "new_code": [], "method_class_nm_old": {"class_nm": "LLM_interface", "method_nm": "findUsage"}, "method_class_nm_new": {"class_nm": "LLM_interface", "method_nm": "findUsage"}}, {"file": "LLM_INTERFACE/LLM_Interface.py", "old_start": 132, "old_length": 6, "new_start": 129, "new_length": 7, "old_code": [], "new_code": ["print('LOCAL CHECKS->ELE: ',_element,'\\nLOCALMETHOD: ',method_deet,'\\nUSE: ',uses_)"], "method_class_nm_old": {"class_nm": "LLM_interface", "method_nm": "findUsage"}, "method_class_nm_new": {"class_nm": "LLM_interface", "method_nm": "findUsage"}}, {"file": "LLM_INTERFACE/LLM_Interface.py", "old_start": 209, "old_length": 10, "new_start": 207, "new_length": 13, "old_code": ["def returnLnNum( self, fnm, var_method_nm ):", "with open( fnm, 'r' ) as fp:", "ll_ = fp.readlines()"], "new_code": ["def returnLnNum( self, fnm, var_method_nm, pre_processed_ll_=None ):", "if pre_processed_ll_ == None:", "with open( fnm, 'r' ) as fp:", "ll_ = fp.readlines()", "else:", "ll_ = pre_processed_ll_"], "method_class_nm_old": {"class_nm": "LLM_interface", "method_nm": null}, "method_class_nm_new": {"class_nm": "LLM_interface", "method_nm": "chunker"}}, {"file": "LLM_INTERFACE/LLM_Interface.py", "old_start": 247, "old_length": 12, "new_start": 248, "new_length": 8, "old_code": ["legit_resp_ = self.ensure_starts_with_square_bracket( resp_ )", "try:", "ll_ = ast.literal_eval( legit_resp_ )", "except:", "print('No array returned by the model ->', traceback.format_exc())", "return None"], "new_code": ["ll_ = []", "ll_ = self.checkLLMResponseFormat( resp_ )"], "method_class_nm_old": {"class_nm": "LLM_interface", "method_nm": "processModuleVars"}, "method_class_nm_new": {"class_nm": "LLM_interface", "method_nm": "processModuleVars"}}, {"file": "LLM_INTERFACE/LLM_Interface.py", "old_start": 274, "old_length": 12, "new_start": 271, "new_length": 9, "old_code": ["legit_resp_ = self.ensure_starts_with_square_bracket( resp_ )", "try:", "ll_ = ast.literal_eval( legit_resp_ )", "except:", "print('No array returned by the model')", "return None"], "new_code": ["", "ll_ = []", "ll_ = self.checkLLMResponseFormat( resp_ )"], "method_class_nm_old": {"class_nm": "LLM_interface", "method_nm": "processPackageVars"}, "method_class_nm_new": {"class_nm": "LLM_interface", "method_nm": "processPackageVars"}}, {"file": "LLM_INTERFACE/LLM_Interface.py", "old_start": 306, "old_length": 21, "new_start": 300, "new_length": 27, "old_code": ["def processModuleRefs( self, resp_, chunk_, fnm_ ):", "## populate self.method_store_ { 'file_path': [ {'method_nm': ( < start ln #>, < end ln # > ) } ] }", "'''", "sometimes LLMs vomit unnecessary stuff at the beginning ..simple pre processing to ensure we start", "with an array", "'''"], "new_code": ["def checkLLMResponseFormat( self, resp_ ):", "", "return ll_", "def processModuleRefs( self, resp_, chunk_, fnm_ ):", "## populate self.method_store_ { 'file_path': [ {'method_nm': ( < start ln #>, < end ln # > ) } ] }", "'''", "sometimes LLMs vomit unnecessary stuff at the beginning ..simple pre processing to ensure we start", "with an array", "'''", "ll_ = []", "ll_ = self.checkLLMResponseFormat( resp_ )", ""], "method_class_nm_old": {"class_nm": "LLM_interface", "method_nm": "checkLLMResponseFormat"}, "method_class_nm_new": {"class_nm": "LLM_interface", "method_nm": "processPackageVars"}}, {"file": "LLM_INTERFACE/LLM_Interface.py", "old_start": 419, "old_length": 15, "new_start": 419, "new_length": 26, "old_code": ["with open( self.dest_folder_ + '/VARIABLE.json', 'w+' ) as fp:", "with open( self.dest_folder_ + '/PACKAGES.json', 'w+' ) as fp:", "with open( self.dest_folder_ + '/METHODS.json', 'w+' ) as fp:"], "new_code": ["## NOTE-> this is a very basic write method. Ideally we need to consider the following and see if need a", "## mongo DB a) size of the files b) re-writing the entire DB / doing incremental chunks", "## critical to be able to acess the method info and start and end lines", "with open( self.dest_folder_ + self.var_json_file, 'w+' ) as fp:", "with open( self.dest_folder_ + self.pack_json_file, 'w+' ) as fp:", "with open( self.dest_folder_ + self.method_json_file, 'w+' ) as fp:", "def readMethodsDBJson(self):", "'''", "read the method json and return reference ..let the caller decide what to do with the data", "especially since there might be a mongo call also to be made ..no convoluting this space", "'''", "with open( self.dest_folder_ + self.method_json_file, 'r' ) as fp:", "return json.load( fp )", ""], "method_class_nm_old": {"class_nm": "LLM_interface", "method_nm": "writeUpResults"}, "method_class_nm_new": {"class_nm": "LLM_interface", "method_nm": "writeUpResults"}}, {"file": "LLM_INTERFACE/LLM_Interface.py", "old_start": 436, "old_length": 7, "new_start": 447, "new_length": 7, "old_code": ["#if 'basic_' not in file_: continue"], "new_code": ["if 'basic_' not in file_: continue"], "method_class_nm_old": {"class_nm": "LLM_interface", "method_nm": "readMethodsDBJson"}, "method_class_nm_new": {"class_nm": "LLM_interface", "method_nm": "extractGraphElements"}}, {"file": "LLM_INTERFACE/LLM_Interface.py", "old_start": 477, "old_length": 5, "new_start": 488, "new_length": 5, "old_code": ["interface_ = LLM_interface('./SRC_DIR/', './OP_DIR/')", "-- /dev/null"], "new_code": ["interface_ = LLM_interface()", "++ b/LLM_INTERFACE/ast_utils.py"], "method_class_nm_old": {"class_nm": "LLM_interface", "method_nm": "extractGraphElements"}, "method_class_nm_new": {"class_nm": null, "method_nm": null}}, {"file": "LLM_INTERFACE/ast_utils.py", "old_start": 0, "old_length": 0, "new_start": 1, "new_length": 91, "old_code": ["-- /dev/null"], "new_code": ["import ast, sys, time", "", "# Define a NodeVisitor class to traverse the AST", "class CodeAnalyzer(ast.NodeVisitor):", "", "def __init__(self):", "self.ast_linewise_deets_ = dict()", "self.file_ptr_ = None", "", "def parse_ast( self, file_nm_, range_):", "self.file_ptr_ = open( file_nm_, 'r' )", "code = self.file_ptr_.readlines()[ range_[0]: range_[1] ]", "code = '\\n'.join( code )", "# Parse the code into an AST", "return ast.parse(code)", "", "def gc(self):", "if self.file_ptr_ != None:", "self.file_ptr_.close()", "", "def visit_Assign(self, node):", "# Check for direct assignments", "targets = [t.id for t in node.targets if isinstance(t, ast.Name)]", "value = node.value", "value_names = self.get_names(value)", "print(f\"Assignment: Line {node.lineno}, Targets: {targets}, Values: {value_names}\")", "self.ast_linewise_deets_[ node.lineno ] = { 'Type':'Assignment', 'Targets': targets }", "self.generic_visit(node)", "", "def visit_If(self, node):", "# Handle if statements", "test_names = self.get_names(node.test)", "print(f\"If statement: Line {node.lineno}, End {node.body[-1].lineno} ,Condition Variables: {test_names}\")", "self.ast_linewise_deets_[ node.lineno ] = { 'Type':'If Statement', 'Targets': test_names , 'Ending': node.body[-1].lineno }", "self.generic_visit(node)", "", "def visit_For(self, node):", "# Handle for loops", "target = node.target.id if isinstance(node.target, ast.Name) else str(node.target)", "iter_names = self.get_names(node.iter)", "self.ast_linewise_deets_[ node.lineno ] = { 'Type':'For loop',  'Targets': iter_names, 'Ending': node.body[-1].lineno }", "print(f\"For loop: Line {node.lineno}, End {node.body[-1].lineno} ,Loop Variable: {target}, Iterating Over: {iter_names}\")", "self.generic_visit(node)", "", "def get_names(self, node):", "# Helper function to extract variable names from nodes", "if isinstance(node, ast.Name):", "return [node.id]", "elif isinstance(node, ast.BinOp):", "return self.get_names(node.left) + self.get_names(node.right)", "elif isinstance(node, ast.BoolOp):", "names = []", "for value in node.values:", "names.extend(self.get_names(value))", "return names", "elif isinstance(node, ast.Compare):", "names = self.get_names(node.left)", "for comp in node.comparators:", "names.extend(self.get_names(comp))", "return names", "elif isinstance(node, ast.Call):", "names = self.get_names(node.func)", "for arg in node.args:", "names.extend(self.get_names(arg))", "return names", "elif isinstance(node, ast.Attribute):", "return self.get_names(node.value) + [node.attr]", "elif isinstance(node, ast.Subscript):", "return self.get_names(node.value) + self.get_names(node.slice)", "elif isinstance(node, (ast.List, ast.Tuple, ast.Set)):", "names = []", "for elt in node.elts:", "names.extend(self.get_names(elt))", "return names", "elif isinstance(node, ast.Dict):", "names = []", "for key in node.keys:", "names.extend(self.get_names(key))", "for value in node.values:", "names.extend(self.get_names(value))", "return names", "else:", "return []", "", "if __name__ == \"__main__\":", "# Instantiate the analyzer and visit the AST nodes", "analyzer = CodeAnalyzer()", "analyzer.visit(parsed_ast)", "", "print( analyzer.ast_linewise_deets_ )", "print( 'DOMU->', time.time() - start_ )", "++ b/LLM_INTERFACE/chunking_utils.py"], "method_class_nm_old": {"class_nm": null, "method_nm": null}, "method_class_nm_new": {"class_nm": null, "method_nm": null}}, {"file": "LLM_INTERFACE/chunking_utils.py", "old_start": 0, "old_length": 0, "new_start": 1, "new_length": 71, "old_code": ["-- a/NEO4J/insertRecordsIntoKG.py"], "new_code": ["import numpy as np", "import json, sys, os, traceback", "from LLM_Interface import LLM_interface", "from ast_utils import CodeAnalyzer", "", "def findRange( file_, input_method_, method_json_ ):", "", "for file_key, method_deets_ in method_json_.items():", "ref_file_, input_file_ = file_key.split('/')[-1], file_.split('/')[-1]", "if ref_file_ == input_file_:", "for key, content in method_deets_.items():", "if key == \"method_name\" and content == input_method_:", "return method_deets_[ 'range' ]", "", "return []", "", "def createChunkInChangeFile( method_name, file_name, summary_of_changes ):", "'''", "ideally if the size of the method is small we could pass the entire method to the LLM", "but methods / functions are usually complex and there are high chances of LLMs missing critical info", "so its best to chunk the file methodically. What we do here is", "a) find the lines of change and then ask the LLM to identify variables that are impacted by this change", "b) ensure these variables are in the vicinity of the changes made ( by simple file search )", "c) then find the variables that are indirectly impacted by these changes", "d) finally find the max line # of the direct / indirected impactees of change and thence the context", "would be the beginning of the changed line and the max line #", "e) for e.g. if the method / function is a 100 line behemoth instead of feeding in all the 100 lines", "this approach will help u cut out the noise", "f) worst case, the max line # will coincide with the line # of the return statement where we send the", "entire method definition", "'''", "##NOTE-> assumption is that we have access to the latest changed code", "llm_interface_ = LLM_interface()", "ast_utils_ = CodeAnalyzer()", "method_summary_ = llm_interface_.readMethodsDBJson()", "", "## summary_of_changes -> [ { 'file name': <>, 'method name': <>, 'old_code': <>, 'new_code': <> } ]", "## also search the graph to find the ending of the method so we can pass the bounds for ast search and analysis", "for changeD in summary_of_changes:", "file_nm_, method_nm_, changed_code_, old_code_ = changeD['file'], changeD['method_nm'], \\", "changeD['new_code'], changeD['old_code']", "", "begin_ln_, end_ln_ = findRange( file_nm_, method_nm_, method_summary_ )", "", "parsed_ast_ = ast_utils_.parse_ast( file_nm_, ( begin_ln_, end_ln_ ) )", "## the above call , apart from initializing the ast also parses the code for the range defined", "## this sub tree can now be accessed via its predicate parsed_ast", "ast_utils_.visit( parsed_ast_ )", "## this should generate all the details which can be accessed via its predicate ast_linewise_deets_", "ast_details_ = ast_utils_.ast_linewise_deets_", "", "with open( file_name, 'r' ) as fp:", "file_contents_ = fp.readlines()", "", "method_specific_lines_ = file_contents_[ begin_ln_: end_ln_ ]", "", "chunk_range_ = findChunkRange( method_specific_lines_, ast_details_,", "", "def createChunkInDownStreamFile( method_name, file_name ):", "'''", "similar to the description in createChunkInChangeFile .. the only difference being that we dont", "need the LLM to trace the changes since a downstream file accesses another method only via a function call", "and the function call would either update the argument being sent OR return a value. We can use these 2 and", "then trace the dependent variables as well", "'''", "", "def checkIfLegitVariable( method_name, file_name, var_name_ ):", "'''", "whilst calling createChunkInChangeFile, the LLM would return a bunch of variables ..need to ensure these", "varuables are indeed present in the context", "'''", "++ /dev/null"]}, {"file": "NEO4J/insertRecordsIntoKG.py", "old_start": 1, "old_length": 110, "new_start": 0, "new_length": 0, "old_code": ["import json, traceback", "import createJsonFeats", "from neo4j import GraphDatabase", "", "with open('config.json', 'r' ) as fp:", "js_ = json.load( fp )", "", "URI = js_[\"URI\"]", "AUTH = ( js_['uname'] , js_['pwd'] )", "", "def chunk( tokenizer, child_context, seqlen=256, overlap_window=40 ):", "'''", "seqlen -> max size of tokenized vec ( depending on LM used )", "overlap_window -> how many words from the prior passage need to be used", "default 40 since we expect a chunk to have about 200 words and this is 20% ..lil lol", "'''", "", "emb_arr_, chunk_sent_arr_ = [], []", "", "inp_para = child_context.split()", "tokenized_input = tokenizer( child_context, return_tensors=\"pt\" )", "print( 'SEQ LEN->', len( tokenized_input['input_ids'][0] ) )", "", "if len( tokenized_input['input_ids'][0] ) > seqlen:", "scale_ = ( len( tokenized_input['input_ids'][0] ) + overlap_window )/seqlen", "rounded_ = round( ( len( tokenized_input['input_ids'][0] ) + overlap_window )/seqlen )", "## the idea is to divvy the sentence into overlapping chunks", "## so if num chunks is 2 then parts of the first chunk will also show up in the 2nd ..just that", "## we need to ensure we dont lose any part of", "if scale_ < rounded_:", "iters_ = rounded_", "elif scale_ >= rounded_:", "iters_ = rounded_ + 1", "", "chunk_arr_, adjusted_seq_len = [], int( seqlen*0.8 ) ## since the seqlen is the token length, we CANT use that to split the actual text since typical words are shorter than tokens", "## if scale_ is 4.5, then rounded will be 4 and hence we go with 4+1 and vice versa", "for idx in range( iters_ - 1 ):", "if idx == 0:", "chunk_ = ' '.join( inp_para[ idx*adjusted_seq_len: (idx+1)*adjusted_seq_len ] )", "else:", "chunk_ = inp_para[ (idx-1)*adjusted_seq_len: idx*adjusted_seq_len ][ -1*overlap_window: ] + \\", "inp_para[ idx*adjusted_seq_len: (idx+1)*adjusted_seq_len ]", "", "chunk_ = ' '.join( chunk_ )", "", "chunk_arr_.append( chunk_ )", "", "## for the last idx", "lastidx_ = iters_ - 1", "chunk_last_ =  inp_para[ (lastidx_-1)*adjusted_seq_len: lastidx_*adjusted_seq_len ][ -1*overlap_window: ] + \\", "inp_para[ lastidx_*adjusted_seq_len:  ]", "", "chunk_arr_.append( ' '.join( chunk_last_ ) )", "", "for ch in chunk_arr_:", "emb_arr_.append( createJsonFeats.returnEmbed( ch ) )", "chunk_sent_arr_.append( ch )", "", "else:", "emb_arr_.append( createJsonFeats.returnEmbed( child_context ) )", "chunk_sent_arr_.append( child_context )", "", "return emb_arr_, chunk_sent_arr_", "", "def insertRecord( tokenizer, parent, child_context, relation_context, child_entity ):", "", "with GraphDatabase.driver(URI, auth=AUTH) as driver:", "try:", "parent, child_para_, relation, child = parent, child_context, relation_context, child_entity", "", "relation_emb = createJsonFeats.returnEmbed( parent + ' ' + relation )", "parent_title_embd, child_title_embd = createJsonFeats.returnEmbed( parent ), \\", "createJsonFeats.returnEmbed( child )", "", "emb_arr_, ch_arr_ = chunk( tokenizer, child_para_ )", "print('Total # of chunks->', len( emb_arr_ ) )", "", "with driver.session() as session:", "", "for cnt, child_embd in enumerate( emb_arr_ ): ## child name can simply be child1, child2 etc based on num of chunks", "if cnt == 0:", "child_bear_ = child", "else:", "child_bear_ = child +' - '+ str( cnt )", "", "qry_ = ''' MERGE (  parent:Parent { title:\"'''+ parent +'''\" } )\\", "WITH parent  \\", "CALL db.create.setNodeVectorProperty( parent, 'title_embedding',\\", "apoc.convert.fromJsonList( \"'''+ str( parent_title_embd ) +'''\" ) )", "MERGE(   child:Child { title:\"'''+ child_bear_ +'''\", \\", "context:\"'''+ parent +\" \"+ ch_arr_[cnt] +'''\" } )\\", "WITH child, parent \\", "CALL db.create.setNodeVectorProperty( child, 'embedding', \\", "apoc.convert.fromJsonList( \"'''+ str( child_embd ) +'''\" ) ) \\", "CALL db.create.setNodeVectorProperty( child, 'title_embedding', \\", "apoc.convert.fromJsonList( \"'''+ str( child_title_embd ) +'''\" ) ) \\", "MERGE( parent )-[ relation:FIRST_LEVEL_CONNECT { property: \"'''+ parent +' '+relation \\", "+'''\" } ]->( child ) \\", "WITH child, parent, relation CALL db.create.setRelationshipVectorProperty( relation, 'embedding',\\", "apoc.convert.fromJsonList( \"'''+ str( relation_emb ) +'''\" ) )'''", "", "", "print('xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx')", "result = session.run( qry_ )", "print('RAN QRY and inserterd P, R, C ->', '\\n',{'P': parent}, {'R': relation }, {'C': child_bear_}  )", "", "except:", "print('EXCPN->', traceback.format_exc())", "pass", "", "-- /dev/null"], "new_code": ["++ b/github-monitor/github_monitor.py"]}, {"file": "github-monitor/github_monitor.py", "old_start": 0, "old_length": 0, "new_start": 1, "new_length": 133, "old_code": [], "new_code": ["# github_monitor.py", "import os", "import time", "import requests", "import json", "", "import logging", "from logging.handlers import TimedRotatingFileHandler", "import sys", "import os", "", "", "GITHUB_TOKEN = os.getenv('GITHUB_TOKEN')", "REPO_OWNER = \"AmyGB-ai\"", "REPO_NAME = \"IKG\"", "ARTIFACT_NAME = 'generated-files'", "CHECK_INTERVAL = 10  # Check every 2 minutes", "local_path = '/datadrive/IKG/github-monitor/'", "RUN_ID_FILE = local_path + 'last_run_id.txt'", "OUTPUT_DIR = local_path + '/downloaded_artifacts'", "# Define a custom stream handler to redirect print statements", "", "# Set up the logging configuration", "log_dir = '/log_directory/'", "log_file = 'github_monitor.log'", "log_path = os.path.join( local_path + log_dir, log_file)", "", "# Configure the TimedRotatingFileHandler", "logger = logging.getLogger('GitHubMonitor')", "logger.setLevel(logging.INFO)", "handler = TimedRotatingFileHandler(log_path, when='midnight', interval=1, backupCount=7)", "handler.suffix = \"%Y-%m-%d\"", "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')", "handler.setFormatter(formatter)", "logger.addHandler(handler)", "", "class StreamToLogger:", "def __init__(self, logger, log_level=logging.INFO):", "self.logger = logger", "self.log_level = log_level", "self.linebuf = ''", "", "def write(self, buf):", "for line in buf.rstrip().splitlines():", "self.logger.log(self.log_level, line.rstrip())", "", "def flush(self):", "pass", "", "# Redirect stdout and stderr to the logger", "sys.stdout = StreamToLogger(logger, logging.INFO)", "sys.stderr = StreamToLogger(logger, logging.ERROR)", "", "def get_latest_successful_run_id():", "url = f'https://api.github.com/repos/{REPO_OWNER}/{REPO_NAME}/actions/runs'", "headers = {'Authorization': f'token {GITHUB_TOKEN}'}", "response = requests.get(url, headers=headers)", "runs = response.json().get('workflow_runs', [])", "", "for run in runs:", "if run['conclusion'] == 'success':", "return run['id']", "return None", "", "def get_artifact_url(run_id):", "url = f'https://api.github.com/repos/{REPO_OWNER}/{REPO_NAME}/actions/runs/{run_id}/artifacts'", "headers = {'Authorization': f'token {GITHUB_TOKEN}'}", "response = requests.get(url, headers=headers)", "artifacts = response.json().get('artifacts', [])", "", "for artifact in artifacts:", "if artifact['name'] == ARTIFACT_NAME:", "return artifact['archive_download_url']", "return None", "", "def download_artifact(artifact_url):", "headers = {'Authorization': f'token {GITHUB_TOKEN}'}", "response = requests.get(artifact_url, headers=headers)", "artifact_path = os.path.join(OUTPUT_DIR, 'generated-files.zip')", "", "print('GOIN TO WRITE->', artifact_path)", "with open(artifact_path, 'wb') as f:", "print('IN WITH SCOPE->', artifact_path)", "f.write(response.content)", "return artifact_path", "", "def extract_artifact(artifact_path):", "import zipfile", "", "with zipfile.ZipFile(artifact_path, 'r') as zip_ref:", "zip_ref.extractall(OUTPUT_DIR)", "", "def main():", "", "while True:", "try:", "latest_run_id = get_latest_successful_run_id()", "if latest_run_id is None:", "print(\"No successful runs found. Retrying...\")", "time.sleep(CHECK_INTERVAL)", "continue", "elif latest_run_id is not None:", "print('Extracted latest_run_id->', latest_run_id)", "", "if os.path.exists(RUN_ID_FILE):", "with open(RUN_ID_FILE, 'r') as f:", "last_run_id = f.read().strip()", "else:", "last_run_id = None", "", "if str(latest_run_id) != last_run_id:", "artifact_url = get_artifact_url(latest_run_id)", "if artifact_url:", "print(f\"New artifact found. Downloading from {artifact_url}\")", "artifact_path = download_artifact(artifact_url)", "extract_artifact(artifact_path)", "with open(RUN_ID_FILE, 'w') as f:", "f.write(str(latest_run_id))", "print(f\"Artifact downloaded and extracted successfully.\")", "else:", "print(\"No artifact found for the latest successful run.\")", "else:", "print(\"No new successful runs. Retrying...\")", "", "except Exception as e:", "print(f\"An error occurred: {e}\")", "", "time.sleep(CHECK_INTERVAL)", "#break", "", "if __name__ == '__main__':", "main()", ""], "method_class_nm_old": {"class_nm": null, "method_nm": null}, "method_class_nm_new": {"class_nm": null, "method_nm": null}}]