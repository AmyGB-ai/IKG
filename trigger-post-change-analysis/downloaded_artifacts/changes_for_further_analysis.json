[{"file": "LLM_INTERFACE/SRC_DIR/testChunking.py", "old_start": 0, "old_length": 0, "new_start": 1, "new_length": 34, "old_code": ["-- /dev/null\n"], "new_code": ["import numpy as np\n", "import json, sys, os\n", "\n", "def chunking_test(a, b):\n", "    x = a*b\n", "    y = call_func( a, b )\n", "    zz = x**2\n", "    y = zz*x\n", "    abc = 123\n", "    def_ = 456\n", "    final_ = call_def( y )\n", "    ghi = final_\n", "\n", "    for st_, nm_ in common_wds_1:\n", "        locdist_ = []\n", "        for st_1, nm_1 in common_wds_1:\n", "            locdist_.append( distance.euclidean( nm_, nm_1 ) )\n", "\n", "        distm1.append( locdist_ )\n", "    \n", "    ffg_ = ghi\n", "    for st_, nm_ in common_wds_2:\n", "        locdist_ = []\n", "        for st_1, nm_1 in common_wds_2:\n", "            locdist_.append( distance.euclidean( nm_, nm_1 ) )\n", "\n", "        distm2.append( locdist_ )\n", "\n", "    ## now calc prime eigenvectors\n", "    eigenvalues1, eigenvectors1 = eig( distm1 )\n", "    idx = np.argsort(eigenvalues1)[::-1]\n", "    print( eigenvalues1[idx][:5] )\n", "    print( eigenvectors1[:, idx][:, :1] )\n", "    return ffg_\n", "++ b/LLM_INTERFACE/SRC_DIR/testMonkey.py\n"], "method_class_nm_old": {"class_nm": null, "method_nm": null}, "method_class_nm_new": {"class_nm": null, "method_nm": null}}, {"file": "LLM_INTERFACE/SRC_DIR/testMonkey.py", "old_start": 0, "old_length": 0, "new_start": 1, "new_length": 41, "old_code": ["-- a/LLM_INTERFACE/chunking_utils.py\n"], "new_code": ["import testChunking as tc\n", "\n", "def downstream_antics( x, y ):\n", "    ## search the db now\n", "\n", "    dbRec_ = db_utils.returnBlankDBRec()\n", "    dbRec_['docID'] = fnm\n", "    dbRec_['docSignature'] = encoded_\n", "    dbRec_['tupArr'] = key_coord_tup_\n", "    global debug_sign_\n", "\n", "    results_ = db_utils.searchSignature( dbRec_ )['searchRes_']\n", "    matching_recs_, closest_match, self_rec, all_matches = [], None, None, dict()\n", "    #print('DREDD->', docs_used_)\n", "    #print('Whats the hit ?-?', results_)\n", "    highest_match_score_ = 0\n", "    x = tc.chunking_test( 10, 20 )\n", "\n", "    insertD = dict()\n", "    insertD[ 'config_field_nm' ] = keyNm\n", "    insertD[ 'local_field' ] = feedback_local_key_dict\n", "    insertD[ 'feedback_value' ] = feedback_value_\n", "    insertD[ 'feedback_co_ords' ] = feedback_co_ords\n", "    insertD[ 'comments' ] = comments\n", "    y = x**3\n", "    insertD[ 'config_field_nm' ] = keyNm\n", "    insertD[ 'local_field' ] = feedback_local_key_dict\n", "    insertD[ 'feedback_value' ] = feedback_value_\n", "    insertD[ 'feedback_co_ords' ] = feedback_co_ords\n", "    fgh_ = doSomething( y )\n", "\n", "    if ( len( txt1.split() ) == 1 and fuzzr is not None and fuzzr >= 80 ) or \\\n", "            ( len( txt2.split() ) == 1 and fuzzr is not None and fuzzr >= 80 ): \n", "        #print('ALTHOUGHT SIZE 1, high fuzz ration->', txt1, txt2, fuzzr) \n", "        return True\n", "    if ( len( txt1.split() ) == 1 and fuzzr is not None and fuzzr >= 80 ) or \\\n", "            ( len( txt2.split() ) == 1 and fuzzr is not None and fuzzr >= 80 ): \n", "        #print('ALTHOUGHT SIZE 1, high fuzz ration->', txt1, txt2, fuzzr) \n", "        return True\n", "   \n", "    return False    \n", "++ b/LLM_INTERFACE/chunking_utils.py\n"], "method_class_nm_old": {"class_nm": null, "method_nm": null}, "method_class_nm_new": {"class_nm": null, "method_nm": null}}, {"file": "LLM_INTERFACE/chunking_utils.py", "old_start": 1, "old_length": 17, "new_start": 1, "new_length": 17, "old_code": ["from ast_utils import CodeAnalyzer\n", "        print('FINDING RANGE->', ref_file_, input_file_)\n", "            for key, content in method_deets_.items():\n", "                if key == \"method_name\" and content == input_method_:\n", "                    return method_deets_[ 'range' ]\n"], "new_code": ["from python_ast_utils import CodeAnalyzer ## need to replace this based on programming language\n", "        #print('FINDING RANGE->', ref_file_, input_file_)\n", "            for content in method_deets_:\n", "                if \"method_name\" in content and content[\"method_name\"] == input_method_:\n", "                    return content[ 'range' ]\n"], "method_class_nm_old": {"class_nm": null, "method_nm": null}, "method_class_nm_new": {"class_nm": null, "method_nm": null}}, {"file": "LLM_INTERFACE/chunking_utils.py", "old_start": 22, "old_length": 81, "new_start": 22, "new_length": 98, "old_code": ["    for _ , assignment_deets in old_code_vars_.items():\n", "        tgt_of_interest_ = assignment_deets['Targets'][0]\n", "\n", "        if tgt_of_interest_ in target_dict_ and target_dict_[tgt_of_interest_]['MAX'] is not None:\n", "            idx_ = target_dict_[tgt_of_interest_]['MAX']\n", "            while idx_ < len( target_dict_ ) - 1:\n", "                neo_tgt_ = target_dict_[tgt_of_interest_]['MAX_LN_TGT']\n", "                if neo_tgt_ in target_dict_ and target_dict_[neo_tgt_]['MAX'] is not None:\n", "                    idx_ = target_dict_[neo_tgt_]['MAX']\n", "                    if target_dict_[neo_tgt_]['MIN'] < min_ln_ : min_ln_ = target_dict_[neo_tgt_]['MIN'] \n", "                    if target_dict_[neo_tgt_]['MAX'] > max_ln_ : max_ln_ = target_dict_[neo_tgt_]['MAX']\n", "                else:\n", "                    idx_ += 1\n", "\n", "        if tgt_of_interest_ in target_dict_ and target_dict_[tgt_of_interest_]['MAX'] is not None:\n", "            idx_ = target_dict_[tgt_of_interest_]['MAX']\n", "\n", "            while idx_ < len( target_dict_ ) - 1:\n", "                neo_tgt_ = target_dict_[tgt_of_interest_]['MAX_LN_TGT']\n", "\n", "                if neo_tgt_ in target_dict_ and target_dict_[neo_tgt_]['MAX'] is not None:\n", "                    idx_ = target_dict_[neo_tgt_]['MAX']\n", "                    if target_dict_[neo_tgt_]['MIN'] < min_ln_ : min_ln_ = target_dict_[neo_tgt_]['MIN'] \n", "                    if target_dict_[neo_tgt_]['MAX'] > max_ln_ : max_ln_ = target_dict_[neo_tgt_]['MAX']\n", "                else:\n", "                    idx_ += 1\n", "    ast_old_, ast_new_ = CodeAnalyzer(), CodeAnalyzer()\n", "    old_code_ast_, new_code_ast_ = ast_old_.parse_ast_snippet( old_code_ ), \\\n", "                                   ast_new_.parse_ast_snippet( changed_code_ )\n", "    \n", "    ast_old_.visit( old_code_ast_ )\n", "    ast_new_.visit( new_code_ast_ )\n", "    old_code_vars_, new_code_vars_ = ast_old_.ast_linewise_deets_, ast_new_.ast_linewise_deets_\n", "    ## go through ast_details_ and find the begin and ending reference of the variable ( direct and indirect )\n", "    target_dict_ = dict()\n", "    for ln_no, line_ast_ in ast_details_.items():\n", "        for eval_tgt_ in line_ast_['Targets']:\n", "            max_ln_, min_line_ = -1, 10000\n", "            for ln_no, line_ast_ in ast_details_.items():\n", "                if eval_tgt_ in line_ast_['Values'] and max_ln_ < ln_no:\n", "                    max_ln_ = ln_no\n", "                if eval_tgt_ in line_ast_['Values'] and min_line_ > ln_no:\n", "                    min_line_ = ln_no\n", "            print('Furthest assignment of ',eval_tgt_,' is ', max_ln_, max_val_)\n", "            target_dict_[ eval_tgt_ ] = { 'MIN': min_line_ if min_line_ != 10000 else None ,\\\n", "                                          'MAX': max_ln_ if max_ln_ != -1 else None,\\\n", "                                     'MAX_LN_TGT': ast_details_[max_ln_]['Targets'][0] if max_ln_ != -1 else None }\n", "    ## the above will result in a DS like so\n", "    ## key-> target value -> nearest & furthest USAGE of \"target\" as a VALUE .. so that we can follow the bread\n", "    ## crumbs to the last indirect assignment of the target\n", "    ast_old_.gc()\n", "    ast_new_.gc()\n", "    return cmpOldNew( old_code_vars_, new_code_vars_, target_dict_ )\n", "def createChunkInChangeFile( summary_of_changes ):\n"], "new_code": ["    #print('ENTERING cmpOldNew->', old_code_vars_, new_code_vars_)\n", "    #print( target_dict_ )\n", "    #print( len( target_dict_ ) )\n", "    max_line_from_target_ = -1\n", "    for key, val in target_dict_.items():\n", "        if val['MAX'] is not None and val['MAX'] > max_line_from_target_:\n", "            max_line_from_target_ = val['MAX']\n", "    target_dict_keys_ = list( target_dict_.keys() )\n", "    for _ , assignment_deets in old_code_vars_.items():\n", "        tgt_of_interest_ = assignment_deets['Targets'][0]\n", "        \n", "        for target_, tdeets_ in target_dict_.items():\n", "            if target_ == tgt_of_interest_ and \\\n", "                    ( tdeets_['MIN'] is not None and tdeets_['MAX'] is not None ):\n", "                tgt_of_interest_ = tdeets_['MAX_LN_TGT']\n", "                if tdeets_['MIN'] < min_ln_ : min_ln_ = tdeets_['MIN']\n", "                if tdeets_['MAX'] > max_ln_ : max_ln_ = tdeets_['MAX']\n", "                print('NEW TGT->', tgt_of_interest_, min_ln_, max_ln_)\n", "        \n", "        for target_, tdeets_ in target_dict_.items():\n", "            if target_ == tgt_of_interest_ and \\\n", "                    ( tdeets_['MIN'] is not None and tdeets_['MAX'] is not None ):\n", "                tgt_of_interest_ = tdeets_['MAX_LN_TGT']\n", "                if tdeets_['MIN'] < min_ln_ : min_ln_ = tdeets_['MIN']\n", "                if tdeets_['MAX'] > max_ln_ : max_ln_ = tdeets_['MAX']\n", "                print('NEW TGT->', tgt_of_interest_, min_ln_, max_ln_)\n", "    '''\n", "    the idea is to use both the old and new code snippets and find out the \"sphere of influence\" of changes\n", "    so we just follow the variables directly impacted by the code changes and then follow their trail of direct\n", "    and indirect assignments\n", "    NOTE-> a recursive solution here would be best but a little dangerous OR we can create small sub-graphs\n", "    and then it would be a simple matter of graph traversal ..but this current algo is quick so any new soln\n", "    needs to be robust and quick ( obviously )\n", "    '''\n", "    if changed_code_ != None and old_code_ != None:\n", "        ast_old_, ast_new_ = CodeAnalyzer(), CodeAnalyzer()\n", "        old_code_ast_, new_code_ast_ = ast_old_.parse_ast_snippet( old_code_ ), \\\n", "                                       ast_new_.parse_ast_snippet( changed_code_ )\n", "        \n", "        ast_old_.visit( old_code_ast_ )\n", "        ast_new_.visit( new_code_ast_ )\n", "\n", "        old_code_vars_, new_code_vars_ = ast_old_.ast_linewise_deets_, ast_new_.ast_linewise_deets_\n", "        ## go through ast_details_ and find the begin and ending reference of the variable ( direct and indirect )\n", "        target_dict_ = dict()\n", "        for ln_no, line_ast_ in ast_details_.items():\n", "            for eval_tgt_ in line_ast_['Targets']:\n", "                max_ln_, min_line_ = -1, 10000\n", "                for ln_no, line_ast_ in ast_details_.items():\n", "                    if eval_tgt_ in line_ast_['Values'] and max_ln_ < ln_no:\n", "                        max_ln_ = ln_no\n", "                    if eval_tgt_ in line_ast_['Values'] and min_line_ > ln_no:\n", "                        min_line_ = ln_no\n", "                #print('Furthest assignment of ',eval_tgt_,' is ', max_ln_, min_line_)\n", "                #if min_line_ != 10000 and max_ln_ != -1:\n", "                    #print( ast_details_[min_line_], ast_details_[max_ln_] )\n", "                target_dict_[ eval_tgt_ ] = { 'MIN': min_line_ if min_line_ != 10000 else None ,\\\n", "                                              'MAX': max_ln_ if max_ln_ != -1 else None,\\\n", "                                         'MAX_LN_TGT': ast_details_[max_ln_]['Targets'][0] \\\n", "                                         if ( max_ln_ != -1 and 'Targets' in ast_details_[max_ln_] and \\\n", "                                            len( ast_details_[max_ln_]['Targets'] ) > 0 ) \\\n", "                                         else None }\n", "        ## the above will result in a DS like so\n", "        ## key-> target value -> nearest & furthest USAGE of \"target\" as a VALUE .. so that we can follow the bread\n", "        ## crumbs to the last indirect assignment of the target\n", "        ast_old_.gc()\n", "        ast_new_.gc()\n", "        return cmpOldNew( old_code_vars_, new_code_vars_, target_dict_ )\n", "    return ( 10000, -1 )\n", "\n", "def createChunkInChangeFile( home_dir_, summary_of_changes ):\n"], "method_class_nm_old": {"class_nm": null, "method_nm": "cmpOldNew"}, "method_class_nm_new": {"class_nm": null, "method_nm": "cmpOldNew"}}, {"file": "LLM_INTERFACE/chunking_utils.py", "old_start": 120, "old_length": 7, "new_start": 137, "new_length": 8, "old_code": ["        file_nm_, method_nm_, changed_code_, old_code_ = changeD['file'], changeD[\"method_class_nm_old\"]['method_nm'], \\\n"], "new_code": ["        file_nm_, method_nm_, changed_code_, old_code_ = home_dir_ + changeD['file'], \\\n", "                                                         changeD[\"method_class_nm_old\"]['method_nm'], \\\n"], "method_class_nm_old": {"class_nm": null, "method_nm": "createChunkInChangeFile"}, "method_class_nm_new": {"class_nm": null, "method_nm": "createChunkInChangeFile"}}, {"file": "LLM_INTERFACE/chunking_utils.py", "old_start": 131, "old_length": 8, "new_start": 149, "new_length": 13, "old_code": ["        code_review_range_ = getSphereOfInfluence( ast_details_, changed_code_, old_code_ )\n"], "new_code": ["        #print( ast_details_ )\n", "        try:\n", "            code_review_range_ = getSphereOfInfluence( ast_details_, changed_code_, old_code_ )\n", "        except:\n", "            print('CODE CONTEXT EXTRACTION ERROR->', traceback.format_exc())\n", "            code_review_range_ = ( 10000, -1 )\n"], "method_class_nm_old": {"class_nm": null, "method_nm": "createChunkInChangeFile"}, "method_class_nm_new": {"class_nm": null, "method_nm": "createChunkInChangeFile"}}, {"file": "LLM_INTERFACE/chunking_utils.py", "old_start": 147, "old_length": 19, "new_start": 170, "new_length": 94, "old_code": ["def createChunkInDownStreamFile( method_name, file_name ):\n", "    return None\n", "    import json\n", "    createChunkInChangeFile( js_ )\n", "-- a/LLM_INTERFACE/ast_utils.py\n"], "new_code": ["def findPointOfEntry( file_path_, context_method_range_, changeDeets_ ):\n", "    with open( file_path_, 'r' ) as fp:\n", "        downstream_file_ = fp.readlines()\n", "\n", "    begin_, end_ = context_method_range_\n", "    context_ = downstream_file_[ begin_: end_ ]\n", "    upstream_class_nm_, upstream_method_nm_ = changeDeets_[\"class_nm\"],\\\n", "                                                changeDeets_[\"method_nm\"]\n", "    ## find the first occurence of either the imported class / method name's mention\n", "    for idx, line_ in enumerate( context_ ):\n", "        if upstream_class_nm_ in line_ or upstream_method_nm_ in line_:\n", "            return idx, [ line_ ] ## using array nomenclature to maintain std with other similar inputs\n", "\n", "    return None, None\n", "\n", "def createChunkInDownStreamFile( change_details_, downstream_file_details_ ):\n", "\n", "    basically we are trying to trace \"X\" in the code below\n", "    changed method -> classA.methodB \n", "    downstream file -> from upstream import classA\n", "                       def downstream_func( abc ):\n", "                          clsA = classA()\n", "                          X = clsA.methodB( abc )\n", "                          Y = doSomething( X ) ...\n", "    ##NOTE-> input to the method will be the changes and its downstream usage .. this will need to be called in a\n", "    ## for loop since we have to invoke graph traversal to find global and local calls for the method\n", "    ## change_details_ -> keys - class_nm, method_nm, file_nm\n", "    ## downstream_file_details_ -> keys - method_nm, file_nm\n", "    llm_interface_ = LLM_interface()\n", "    ast_utils_ = CodeAnalyzer()\n", "    method_summary_ = llm_interface_.readMethodsDBJson()\n", "    chunks_for_analysis_ = []\n", "\n", "    begin_ln_, end_ln_ = findRange( downstream_file_details_['file_nm'], downstream_file_details_['method_nm'],\\\n", "                                                                         method_summary_ )\n", "\n", "    parsed_ast_ = ast_utils_.parse_ast( downstream_file_details_['file_nm'], ( begin_ln_, end_ln_ ) )\n", "    ast_utils_.visit( parsed_ast_ )\n", "    ast_details_ = ast_utils_.ast_linewise_deets_\n", "\n", "    begin_ln_, downstream_point_of_entry_ = findPointOfEntry( downstream_file_details_['file_nm'], \\\n", "                                                                ( begin_ln_, end_ln_ ),\\\n", "                                                                                change_details_ )\n", "    if downstream_point_of_entry_ == None:\n", "        print('Point of entry not found ..raise EXCPN')\n", "        return None\n", "\n", "    try:\n", "        changed_code_ = old_code_ = downstream_point_of_entry_\n", "        ## since we want to understand how much of the downstream code is actually impacted by teh upstream\n", "        ## method change we would like to ONLY shortlist those lines of code which are actually impacted\n", "        ## both changed and old can contain the same input since the below method only returns the best possible\n", "        ## range considering both\n", "        code_review_range_ = getSphereOfInfluence( ast_details_, changed_code_, old_code_ )\n", "    except:\n", "        print('CODE CONTEXT EXTRACTION ERROR->', traceback.format_exc())\n", "        code_review_range_ = ( 10000, -1 )\n", "\n", "    with open( downstream_file_details_['file_nm'], 'r' ) as fp:\n", "        tmp_contents_ = fp.readlines()\n", "\n", "    if code_review_range_[0] == 10000 or code_review_range_[1] == -1:\n", "        print('Sending the entire code of <', downstream_point_of_entry_,'> for review')\n", "    else:\n", "        _, end_ln_ = code_review_range_ ## begin will be point of entry ..just get the furthest assignment\n", "        print('Found contextual subtext for <', downstream_point_of_entry_,'>', begin_ln_, end_ln_ )\n", "\n", "    ast_utils_.gc()\n", "\n", "    return tmp_contents_[ begin_ln_: end_ln_ ] \n", "    import json, time\n", "    start_ = time.time()\n", "    #createChunkInChangeFile( '/datadrive/IKG/', js_ )\n", "    for elem in js_:\n", "        changed_, downstream_ = { 'class_nm': elem[\"method_class_nm_new\"][\"class_nm\"],'method_nm': elem[\"method_class_nm_new\"][\"method_nm\"] , 'file_nm': '/datadrive/IKG/' + elem[\"file\"] }, \\\n", "                                { 'method_nm':'downstream_antics' , 'file_nm': \"/datadrive/IKG/LLM_INTERFACE/SRC_DIR/testMonkey.py\" }\n", "\n", "        createChunkInDownStreamFile( changed_, downstream_ )\n", "    print('Total time->', time.time() - start_)\n", "++ b/LLM_INTERFACE/python_ast_utils.py\n"], "method_class_nm_old": {"class_nm": null, "method_nm": "createChunkInChangeFile"}, "method_class_nm_new": {"class_nm": null, "method_nm": "createChunkInChangeFile"}}, {"file": "LLM_INTERFACE/python_ast_utils.py", "old_start": 1, "old_length": 4, "new_start": 1, "new_length": 4, "old_code": ["import ast, sys, time\n"], "new_code": ["import ast, sys, time, textwrap\n"], "method_class_nm_old": {"class_nm": null, "method_nm": null}, "method_class_nm_new": {"class_nm": null, "method_nm": null}}, {"file": "LLM_INTERFACE/python_ast_utils.py", "old_start": 10, "old_length": 12, "new_start": 10, "new_length": 17, "old_code": ["        code = '\\n'.join( code )\n", "        local_snippet_ = snippet_arr_[:-1] # the last entry is always some thing like -- a/ ++ b/ \n"], "new_code": ["        code = textwrap.dedent( ''.join( code ) )\n", "        #code = '\\n'.join( code )\n", "        if '--' in snippet_arr_[-1] or '++' in snippet_arr_[-1]:\n", "            local_snippet_ = snippet_arr_[:-1] # the last entry is always some thing like -- a/ ++ b/ \n", "        else:\n", "            local_snippet_ = snippet_arr_\n", "\n"], "method_class_nm_old": {"class_nm": "CodeAnalyzer", "method_nm": "parse_ast"}, "method_class_nm_new": {"class_nm": "CodeAnalyzer", "method_nm": "parse_ast"}}, {"file": "LLM_INTERFACE/python_ast_utils.py", "old_start": 26, "old_length": 19, "new_start": 31, "new_length": 52, "old_code": ["        # Check for direct assignments\n", "        targets = [t.id for t in node.targets if isinstance(t, ast.Name)]\n", "        print(f\"Assignment: Line {node.lineno}, Targets: {targets}, Values: {value_names}\")\n", "                                                    'Ending': 'NA', 'Values': value_names }\n", "        print(f\"If statement: Line {node.lineno}, End {node.body[-1].lineno} ,Condition Variables: {test_names}\")\n"], "new_code": ["        targets = []\n", "        function_name = self.get_function_name( node.value )\n", "\n", "\n", "        for target in node.targets:\n", "            if isinstance(target, ast.Name):\n", "                # Single variable assignment\n", "                #print(f\"Assignment to variable: {target.id}\")\n", "                targets.append( target.id )\n", "            elif isinstance(target, ast.Tuple):\n", "                # Tuple assignment\n", "                for element in target.elts:\n", "                    if isinstance(element, ast.Name):\n", "                        #print(f\"Assignment to variable in tuple: {element.id}\")\n", "                        targets.append( element.id )\n", "                    # Handle nested tuples if necessary\n", "                    elif isinstance(element, ast.Tuple):\n", "                        self._handle_nested_tuple( element, targets )\n", "\n", "                'Ending': 'NA', 'Values': value_names, 'Function': function_name }\n", "    def get_function_name(self, value):\n", "        if isinstance(value, ast.Call):\n", "            # Extract function name\n", "            if isinstance(value.func, ast.Name):\n", "                return value.func.id\n", "            elif isinstance(value.func, ast.Attribute):\n", "                return value.func.attr\n", "        return \"NA\"\n", "    \n", "    def _handle_nested_tuple( self, tuple_node, targets ):\n", "        for element in tuple_node.elts:\n", "            if isinstance(element, ast.Name):\n", "                #print(f\"Assignment to variable in nested tuple: {element.id}\")\n", "                targets.append( element.id )\n", "            elif isinstance(element, ast.Tuple):\n", "                self._handle_nested_tuple(element)        \n", "\n", "        #print(f\"If statement: Line {node.lineno}, End {node.body[-1].lineno} ,Condition Variables: {test_names}\")\n"], "method_class_nm_old": {"class_nm": "CodeAnalyzer", "method_nm": "parse_ast_snippet"}, "method_class_nm_new": {"class_nm": "CodeAnalyzer", "method_nm": "gc"}}, {"file": "LLM_INTERFACE/python_ast_utils.py", "old_start": 49, "old_length": 7, "new_start": 87, "new_length": 7, "old_code": ["        print(f\"For loop: Line {node.lineno}, End {node.body[-1].lineno} ,Loop Variable: {target}, Iterating Over: {iter_names}\")\n"], "new_code": ["        #print(f\"For loop: Line {node.lineno}, End {node.body[-1].lineno} ,Loop Variable: {target}, Iterating Over: {iter_names}\")\n"], "method_class_nm_old": {"class_nm": "CodeAnalyzer", "method_nm": "visit_Assign"}, "method_class_nm_new": {"class_nm": "CodeAnalyzer", "method_nm": "visit_For"}}]