FINDING RANGE-> addtoDB.py createJsonFeats.py
FINDING RANGE-> basic_generateXLMetaData.py createJsonFeats.py
FINDING RANGE-> createJsonFeats.py createJsonFeats.py
DEEPER-> /datadrive/IKG/code_db/python/createJsonFeats.py {'method_name': 'blockPrint', 'method_begin': 'def blockPrint():\n', 'method_end': "    sys.stdout = open(os.devnull, 'w')\n", 'range': [11, 12], 'global_uses': [], 'local_uses': []} returnEmbed
DEEPER-> /datadrive/IKG/code_db/python/createJsonFeats.py {'method_name': 'enablePrint', 'method_begin': 'def enablePrint():\n', 'method_end': '    sys.stdout = sys.__stdout__\n', 'range': [15, 16], 'global_uses': [], 'local_uses': []} returnEmbed
DEEPER-> /datadrive/IKG/code_db/python/createJsonFeats.py {'method_name': 'returnEmbed', 'method_begin': 'def returnEmbed( sent ):\n', 'method_end': "    return json_obj['encoded_'], True\n", 'range': [29, 41], 'global_uses': [{'file_path': '/datadrive/IKG/code_db/python/addtoDB.py', 'method_nm': 'addToDB', 'method_defn': 'def addToDB():\n', 'usage': '            emb_ = createJsonFeats.returnEmbed( txt )\n', 'method_end': '            db_utils.insertNewSignature( dd_ )\n'}, {'file_path': '/datadrive/IKG/code_db/python/basic_generateXLMetaData.py', 'method_nm': 'createDBRec', 'method_defn': "    def createDBRec( self, summary_D, mode='NORM' ):\n", 'usage': '        emb_ = createJsonFeats.returnEmbed( unified_key_ )\n', 'method_end': '        return insertRec\n'}, {'file_path': '/datadrive/IKG/code_db/python/searchDB.py', 'method_nm': 'pos', 'method_defn': 'def pos( res_ ):\n', 'usage': "            hdr_info_D[ ( resD[ 'payload' ][ 'summary' ] ) ] = createJsonFeats.returnEmbed( resD['payload']['hdr_info'] )\n", 'method_end': "                ' BM25 : ', score_title_[keyid] )\n"}], 'local_uses': []} returnEmbed
CODE_SNIP-> 
rec_ = { 'sentence': sent }

data = json.dumps( rec_ ).encode('utf-8')
_request = urllib.request.Request( url_encode, data=data, method='POST', \
                                    headers={'Content-Type': 'application/json'} )

response = urllib.request.urlopen( _request )
string = response.read().decode('utf-8')
json_obj = json.loads(string)

return json_obj['encoded_'], True

CODE_SNIP-> string = response.read().decode('utf-8')

CODE_SNIP-> string = response.read()

BRAM-> ['sent'] rec_ 2 2
BRAM-> ['json', 'dumps', 'rec_', 'encode'] rec_ 2 4
BRAM-> ['urllib', 'request', 'Request', 'url_encode'] rec_ 2 5
BRAM-> ['urllib', 'request', 'urlopen', '_request'] rec_ 2 8
BRAM-> ['response', 'read', 'decode'] rec_ 2 9
BRAM-> ['json', 'loads', 'string'] rec_ 2 10
Furthest assignment of  rec_  is  4 4
{'Type': 'Assignment', 'Targets': ['data'], 'Ending': 'NA', 'Values': ['json', 'dumps', 'rec_', 'encode'], 'Function': 'encode'} {'Type': 'Assignment', 'Targets': ['data'], 'Ending': 'NA', 'Values': ['json', 'dumps', 'rec_', 'encode'], 'Function': 'encode'}
BRAM-> ['sent'] data 4 2
BRAM-> ['json', 'dumps', 'rec_', 'encode'] data 4 4
BRAM-> ['urllib', 'request', 'Request', 'url_encode'] data 4 5
BRAM-> ['urllib', 'request', 'urlopen', '_request'] data 4 8
BRAM-> ['response', 'read', 'decode'] data 4 9
BRAM-> ['json', 'loads', 'string'] data 4 10
Furthest assignment of  data  is  -1 10000
BRAM-> ['sent'] _request 5 2
BRAM-> ['json', 'dumps', 'rec_', 'encode'] _request 5 4
BRAM-> ['urllib', 'request', 'Request', 'url_encode'] _request 5 5
BRAM-> ['urllib', 'request', 'urlopen', '_request'] _request 5 8
BRAM-> ['response', 'read', 'decode'] _request 5 9
BRAM-> ['json', 'loads', 'string'] _request 5 10
Furthest assignment of  _request  is  8 8
{'Type': 'Assignment', 'Targets': ['response'], 'Ending': 'NA', 'Values': ['urllib', 'request', 'urlopen', '_request'], 'Function': 'urlopen'} {'Type': 'Assignment', 'Targets': ['response'], 'Ending': 'NA', 'Values': ['urllib', 'request', 'urlopen', '_request'], 'Function': 'urlopen'}
BRAM-> ['sent'] response 8 2
BRAM-> ['json', 'dumps', 'rec_', 'encode'] response 8 4
BRAM-> ['urllib', 'request', 'Request', 'url_encode'] response 8 5
BRAM-> ['urllib', 'request', 'urlopen', '_request'] response 8 8
BRAM-> ['response', 'read', 'decode'] response 8 9
BRAM-> ['json', 'loads', 'string'] response 8 10
Furthest assignment of  response  is  9 9
{'Type': 'Assignment', 'Targets': ['string'], 'Ending': 'NA', 'Values': ['response', 'read', 'decode'], 'Function': 'decode'} {'Type': 'Assignment', 'Targets': ['string'], 'Ending': 'NA', 'Values': ['response', 'read', 'decode'], 'Function': 'decode'}
BRAM-> ['sent'] string 9 2
BRAM-> ['json', 'dumps', 'rec_', 'encode'] string 9 4
BRAM-> ['urllib', 'request', 'Request', 'url_encode'] string 9 5
BRAM-> ['urllib', 'request', 'urlopen', '_request'] string 9 8
BRAM-> ['response', 'read', 'decode'] string 9 9
BRAM-> ['json', 'loads', 'string'] string 9 10
Furthest assignment of  string  is  10 10
{'Type': 'Assignment', 'Targets': ['json_obj'], 'Ending': 'NA', 'Values': ['json', 'loads', 'string'], 'Function': 'loads'} {'Type': 'Assignment', 'Targets': ['json_obj'], 'Ending': 'NA', 'Values': ['json', 'loads', 'string'], 'Function': 'loads'}
BRAM-> ['sent'] json_obj 10 2
BRAM-> ['json', 'dumps', 'rec_', 'encode'] json_obj 10 4
BRAM-> ['urllib', 'request', 'Request', 'url_encode'] json_obj 10 5
BRAM-> ['urllib', 'request', 'urlopen', '_request'] json_obj 10 8
BRAM-> ['response', 'read', 'decode'] json_obj 10 9
BRAM-> ['json', 'loads', 'string'] json_obj 10 10
Furthest assignment of  json_obj  is  -1 10000
cmpOldNew-> {'Type': 'Assignment', 'Targets': ['string'], 'Ending': 'NA', 'Values': ['response', 'read', 'decode'], 'Function': 'decode'}
NEW TGT-> json_obj 9 10
Sending the entire code of < returnEmbed > for review
STAGE1-> self chunking :=  [{'file': 'code_db/python/createJsonFeats.py', 'old_start': 133, 'old_length': 8, 'new_start': 133, 'new_length': 8, 'old_code': ["    string = response.read().decode('utf-8')\n"], 'new_code': ['    string = response.read()\n'], 'method_class_nm_old': {'class_nm': '', 'method_nm': 'returnEmbed'}, 'method_class_nm_new': {'class_nm': '', 'method_nm': 'returnEmbed'}, 'method_context': "\n    rec_ = { 'sentence': sent }\n\n    data = json.dumps( rec_ ).encode('utf-8')\n    _request = urllib.request.Request( url_encode, data=data, method='POST', \\\n                                        headers={'Content-Type': 'application/json'} )\n\n    response = urllib.request.urlopen( _request )\n    string = response.read().decode('utf-8')\n    json_obj = json.loads(string)\n    \n    return json_obj['encoded_'], True\n"}]
executeInstruction => you will be given the following: Existing Code , Changed lines in new code. You will have to analyze the impact of the changes made to the base method and add notes on how this impacts the method . Also rate the criticality of this change with 1 being nominal and 5 being deserves immediate attention. Kindly format the data in the form of a json with 3 keys - Issues, Criticality and Recommendations. The values of these keys can be lists with as many values as you can think of and return a final dictionary with the 3 keys

Existing Code :


    rec_ = { 'sentence': sent }

    data = json.dumps( rec_ ).encode('utf-8')
    _request = urllib.request.Request( url_encode, data=data, method='POST', \
                                        headers={'Content-Type': 'application/json'} )

    response = urllib.request.urlopen( _request )
    string = response.read().decode('utf-8')
    json_obj = json.loads(string)
    
    return json_obj['encoded_'], True

Changed line - NEW :     string = response.read()

 OLD :     string = response.read().decode('utf-8')


CRITICALITY-> ": 4,
Traversal Beginning-> returnEmbed /datadrive/IKG/code_db/python/createJsonFeats.py 
        MATCH ( startNode:Method { method_name: "returnEmbed", file_name: "/datadrive/IKG/code_db/python/createJsonFeats.py" } )
        CALL apoc.path.subgraphNodes(startNode, {
            relationshipFilter: "global_uses>",
            minLevel: 1
        }) YIELD node
        RETURN node
        
Traversal Beginning-> returnEmbed /datadrive/IKG/code_db/python/createJsonFeats.py 
        MATCH ( startNode:Method { method_name: "returnEmbed", file_name: "/datadrive/IKG/code_db/python/createJsonFeats.py" } )
        CALL apoc.path.subgraphNodes(startNode, {
            relationshipFilter: "local_uses>",
            minLevel: 1
        }) YIELD node
        RETURN node
        
DEEPER-> /datadrive/IKG/code_db/python/searchDB.py {'method_name': 'pos', 'method_begin': 'def pos( res_ ):\n', 'method_end': "                ' BM25 : ', score_title_[keyid] )\n", 'range': [14, 99], 'global_uses': [], 'local_uses': []} returnEmbed
POE-> ["            hdr_info_D[ ( resD[ 'payload' ][ 'summary' ] ) ] = createJsonFeats.returnEmbed( resD['payload']['hdr_info'] )\n"]
CODE_SNIP-> hdr_info_D[ ( resD[ 'payload' ][ 'summary' ] ) ] = createJsonFeats.returnEmbed( resD['payload']['hdr_info'] )

CODE_SNIP-> hdr_info_D[ ( resD[ 'payload' ][ 'summary' ] ) ] = createJsonFeats.returnEmbed( resD['payload']['hdr_info'] )

CODE_SNIP-> hdr_info_D[ ( resD[ 'payload' ][ 'summary' ] ) ] = createJsonFeats.returnEmbed( resD['payload']['hdr_info'] )

cmpOldNew-> {'Type': 'Assignment', 'Targets': [], 'Ending': 'NA', 'Values': ['createJsonFeats', 'returnEmbed', 'resD'], 'Function': 'returnEmbed'}
CODE CONTEXT EXTRACTION ERROR-> Traceback (most recent call last):
  File "/datadrive/IKG/utils/LLM_INTERFACE/chunking_utils.py", line 286, in createChunkInDownStreamFile
    code_review_range_ = getSphereOfInfluence( ast_details_, changed_code_, old_code_ )
  File "/datadrive/IKG/utils/LLM_INTERFACE/chunking_utils.py", line 145, in getSphereOfInfluence
    return cmpOldNew( old_code_vars_, new_code_vars_, target_dict_ )
  File "/datadrive/IKG/utils/LLM_INTERFACE/chunking_utils.py", line 69, in cmpOldNew
    tgt_of_interest_ = assignment_deets['Targets'][0]
IndexError: list index out of range

TIMMY-> (10000, -1)
Sending the entire code of < ["            hdr_info_D[ ( resD[ 'payload' ][ 'summary' ] ) ] = createJsonFeats.returnEmbed( resD['payload']['hdr_info'] )\n"] > for review 14 99
RETURNING->   if 'searchRes_' in res_:
    act_ = res_[ 'searchRes_' ]
    print( act_ )
    tokenized_hdr_info_ , tokenized_sample_summary_, tokenized_dates_, title = [], [], [], []
    hdr_info_D = dict()

    for res_nm, resD in act_.items():
        if 'payload' in resD and 'summary' in resD[ 'payload' ]:
            corpus_[ ( resD[ 'payload' ][ 'summary' ] ) ] = resD[ 'score' ]

            tokenized_hdr_info_.append( resD[ 'payload' ][ 'hdr_info' ] )
            tokenized_sample_summary_.append( 'sample' )
            tokenized_dates_.append( resD[ 'payload' ][ 'date_range' ] )
            title.append( resD[ 'payload' ]['file_name'] )

            hdr_info_D[ ( resD[ 'payload' ][ 'summary' ] ) ] = createJsonFeats.returnEmbed( resD['payload']['hdr_info'] )

    top_by_vector_score_ = dict( sorted( corpus_.items(), key=lambda x:x[1], reverse=True ) )
    for idx, key in enumerate( list( top_by_vector_score_.keys() )[:10] ):
        print('-----------------------------------------')
        cos_dist_ = distance.cosine( emb_, hdr_info_D[ key ] )
        print('Rank ',idx+1,' CONTEXT->', key, ' SCORE->', top_by_vector_score_[key], ' HDR DISTANCE->', cos_dist_ )
        print('-----------------------------------------')

    tokenized_corpus = [doc.split(" ") for doc in list( corpus_.keys() )]
    bm25_summary_, bm25_hdr_info_, bm25_sample_summary_, bm25_dates_, title = \
            BM25Okapi(tokenized_corpus), BM25Okapi( tokenized_hdr_info_ ), \
            BM25Okapi( tokenized_sample_summary_ ), BM25Okapi( tokenized_dates_ ), BM25Okapi( title )

    tokenized_query = txt.split(" ")
    bm25_score_summary_  = bm25_summary_.get_scores(tokenized_query)
    bm25_score_hdr_  = bm25_hdr_info_.get_scores(tokenized_query)
    bm25_score_sample_  = bm25_sample_summary_.get_scores(tokenized_query)
    bm25_score_dt_  = bm25_dates_.get_scores(tokenized_query)
    score_title_  = title.get_scores(tokenized_query)

    enum_doc_scores_ = list( enumerate( bm25_score_summary_ ) )
    sorted_doc_score_ = sorted( enum_doc_scores_, key=lambda x:x[1] , reverse=True )

    enum_doc_scores_ = list( enumerate( bm25_score_hdr_ ) )
    sorted_doc_score_1 = sorted( enum_doc_scores_, key=lambda x:x[1] , reverse=True )

    enum_doc_scores_ = list( enumerate( bm25_score_sample_ ) )
    sorted_doc_score_2 = sorted( enum_doc_scores_, key=lambda x:x[1] , reverse=True )

    enum_doc_scores_ = list( enumerate( bm25_score_dt_ ) )
    sorted_doc_score_3 = sorted( enum_doc_scores_, key=lambda x:x[1] , reverse=True )

    enum_doc_scores_ = list( enumerate( score_title_ ) )
    sorted_doc_score_4 = sorted( enum_doc_scores_, key=lambda x:x[1] , reverse=True )

    for keyid, keys in enumerate( list( corpus_.keys() ) ):
        print('--------', keyid, np.asarray( sorted_doc_score_ )[:3, :1])    
        if [keyid] not in np.asarray( sorted_doc_score_ )[:3, :1]: continue

        print( 'BM25 Summary :: Text: ', keys, ' Vector score: ', corpus_[ keys ],\
                ' BM25 : ', bm25_score_summary_[keyid] )

    for keyid, keys in enumerate( list( corpus_.keys() ) ):
        print('--------', keyid, np.asarray( sorted_doc_score_1 )[:3, :1])    
        if [keyid] not in np.asarray( sorted_doc_score_1 )[:3, :1]: continue

        print( 'BM25 HDR :: Text: ', keys, ' Vector score: ', corpus_[ keys ],\
                ' BM25 : ', bm25_score_hdr_[keyid] )

    for keyid, keys in enumerate( list( corpus_.keys() ) ):
        print('--------', keyid, np.asarray( sorted_doc_score_2 )[:3, :1])    
        if [keyid] not in np.asarray( sorted_doc_score_2 )[:3, :1]: continue

        print( 'BM25 Sample :: Text: ', keys, ' Vector score: ', corpus_[ keys ],\
                ' BM25 : ', bm25_score_sample_[keyid] )

    for keyid, keys in enumerate( list( corpus_.keys() ) ):
        print('--------', keyid, np.asarray( sorted_doc_score_3 )[:3, :1])    
        if [keyid] not in np.asarray( sorted_doc_score_3 )[:3, :1]: continue

        print( 'BM25 Date :: Text: ', keys, ' Vector score: ', corpus_[ keys ],\
                ' BM25 : ', bm25_score_dt_[keyid] )

    for keyid, keys in enumerate( list( corpus_.keys() ) ):
        print('--------', keyid, np.asarray( sorted_doc_score_4 )[:3, :1])    
        if [keyid] not in np.asarray( sorted_doc_score_4 )[:3, :1]: continue

        print( 'BM25 Date :: Text: ', keys, ' Vector score: ', corpus_[ keys ],\
                ' BM25 : ', score_title_[keyid] )

CALLING LLM addChangeImpactOnDownstreamFile-> 
    rec_ = { 'sentence': sent }

    data = json.dumps( rec_ ).encode('utf-8')
    _request = urllib.request.Request( url_encode, data=data, method='POST', \
                                        headers={'Content-Type': 'application/json'} )

    response = urllib.request.urlopen( _request )
    string = response.read().decode('utf-8')
    json_obj = json.loads(string)
    
    return json_obj['encoded_'], True

Changed line - NEW :     string = response.read()

 OLD :     string = response.read().decode('utf-8')

 downstream file importing returnEmbed
  if 'searchRes_' in res_:
    act_ = res_[ 'searchRes_' ]
    print( act_ )
    tokenized_hdr_info_ , tokenized_sample_summary_, tokenized_dates_, title = [], [], [], []
    hdr_info_D = dict()

    for res_nm, resD in act_.items():
        if 'payload' in resD and 'summary' in resD[ 'payload' ]:
            corpus_[ ( resD[ 'payload' ][ 'summary' ] ) ] = resD[ 'score' ]

            tokenized_hdr_info_.append( resD[ 'payload' ][ 'hdr_info' ] )
            tokenized_sample_summary_.append( 'sample' )
            tokenized_dates_.append( resD[ 'payload' ][ 'date_range' ] )
            title.append( resD[ 'payload' ]['file_name'] )

            hdr_info_D[ ( resD[ 'payload' ][ 'summary' ] ) ] = createJsonFeats.returnEmbed( resD['payload']['hdr_info'] )

    top_by_vector_score_ = dict( sorted( corpus_.items(), key=lambda x:x[1], reverse=True ) )
    for idx, key in enumerate( list( top_by_vector_score_.keys() )[:10] ):
        print('-----------------------------------------')
        cos_dist_ = distance.cosine( emb_, hdr_info_D[ key ] )
        print('Rank ',idx+1,' CONTEXT->', key, ' SCORE->', top_by_vector_score_[key], ' HDR DISTANCE->', cos_dist_ )
        print('-----------------------------------------')

    tokenized_corpus = [doc.split(" ") for doc in list( corpus_.keys() )]
    bm25_summary_, bm25_hdr_info_, bm25_sample_summary_, bm25_dates_, title = \
            BM25Okapi(tokenized_corpus), BM25Okapi( tokenized_hdr_info_ ), \
            BM25Okapi( tokenized_sample_summary_ ), BM25Okapi( tokenized_dates_ ), BM25Okapi( title )

    tokenized_query = txt.split(" ")
    bm25_score_summary_  = bm25_summary_.get_scores(tokenized_query)
    bm25_score_hdr_  = bm25_hdr_info_.get_scores(tokenized_query)
    bm25_score_sample_  = bm25_sample_summary_.get_scores(tokenized_query)
    bm25_score_dt_  = bm25_dates_.get_scores(tokenized_query)
    score_title_  = title.get_scores(tokenized_query)

    enum_doc_scores_ = list( enumerate( bm25_score_summary_ ) )
    sorted_doc_score_ = sorted( enum_doc_scores_, key=lambda x:x[1] , reverse=True )

    enum_doc_scores_ = list( enumerate( bm25_score_hdr_ ) )
    sorted_doc_score_1 = sorted( enum_doc_scores_, key=lambda x:x[1] , reverse=True )

    enum_doc_scores_ = list( enumerate( bm25_score_sample_ ) )
    sorted_doc_score_2 = sorted( enum_doc_scores_, key=lambda x:x[1] , reverse=True )

    enum_doc_scores_ = list( enumerate( bm25_score_dt_ ) )
    sorted_doc_score_3 = sorted( enum_doc_scores_, key=lambda x:x[1] , reverse=True )

    enum_doc_scores_ = list( enumerate( score_title_ ) )
    sorted_doc_score_4 = sorted( enum_doc_scores_, key=lambda x:x[1] , reverse=True )

    for keyid, keys in enumerate( list( corpus_.keys() ) ):
        print('--------', keyid, np.asarray( sorted_doc_score_ )[:3, :1])    
        if [keyid] not in np.asarray( sorted_doc_score_ )[:3, :1]: continue

        print( 'BM25 Summary :: Text: ', keys, ' Vector score: ', corpus_[ keys ],\
                ' BM25 : ', bm25_score_summary_[keyid] )

    for keyid, keys in enumerate( list( corpus_.keys() ) ):
        print('--------', keyid, np.asarray( sorted_doc_score_1 )[:3, :1])    
        if [keyid] not in np.asarray( sorted_doc_score_1 )[:3, :1]: continue

        print( 'BM25 HDR :: Text: ', keys, ' Vector score: ', corpus_[ keys ],\
                ' BM25 : ', bm25_score_hdr_[keyid] )

    for keyid, keys in enumerate( list( corpus_.keys() ) ):
        print('--------', keyid, np.asarray( sorted_doc_score_2 )[:3, :1])    
        if [keyid] not in np.asarray( sorted_doc_score_2 )[:3, :1]: continue

        print( 'BM25 Sample :: Text: ', keys, ' Vector score: ', corpus_[ keys ],\
                ' BM25 : ', bm25_score_sample_[keyid] )

    for keyid, keys in enumerate( list( corpus_.keys() ) ):
        print('--------', keyid, np.asarray( sorted_doc_score_3 )[:3, :1])    
        if [keyid] not in np.asarray( sorted_doc_score_3 )[:3, :1]: continue

        print( 'BM25 Date :: Text: ', keys, ' Vector score: ', corpus_[ keys ],\
                ' BM25 : ', bm25_score_dt_[keyid] )

    for keyid, keys in enumerate( list( corpus_.keys() ) ):
        print('--------', keyid, np.asarray( sorted_doc_score_4 )[:3, :1])    
        if [keyid] not in np.asarray( sorted_doc_score_4 )[:3, :1]: continue

        print( 'BM25 Date :: Text: ', keys, ' Vector score: ', corpus_[ keys ],\
                ' BM25 : ', score_title_[keyid] )

executeInstruction => you will be given the following: Existing Code , Changed lines in new code and the code snippet for a file that imports OR uses the changed method. You will have to analyze the impact of the changes made to the base method and add notes on how this impacts the downstream method that imports OR uses this code. Also rate the criticality of this change with 1 being nominal and 5 being deserves immediate attention. Kindly format the data in the form of a json with 3 keys - Issues, Criticality and Recommendations. The values of these keys can be lists with as many values as you can think of and return a final dictionary with the 3 keys

Existing Code :


    rec_ = { 'sentence': sent }

    data = json.dumps( rec_ ).encode('utf-8')
    _request = urllib.request.Request( url_encode, data=data, method='POST', \
                                        headers={'Content-Type': 'application/json'} )

    response = urllib.request.urlopen( _request )
    string = response.read().decode('utf-8')
    json_obj = json.loads(string)
    
    return json_obj['encoded_'], True

Changed line - NEW :     string = response.read()

 OLD :     string = response.read().decode('utf-8')

 downstream file importing returnEmbed
  if 'searchRes_' in res_:
    act_ = res_[ 'searchRes_' ]
    print( act_ )
    tokenized_hdr_info_ , tokenized_sample_summary_, tokenized_dates_, title = [], [], [], []
    hdr_info_D = dict()

    for res_nm, resD in act_.items():
        if 'payload' in resD and 'summary' in resD[ 'payload' ]:
            corpus_[ ( resD[ 'payload' ][ 'summary' ] ) ] = resD[ 'score' ]

            tokenized_hdr_info_.append( resD[ 'payload' ][ 'hdr_info' ] )
            tokenized_sample_summary_.append( 'sample' )
            tokenized_dates_.append( resD[ 'payload' ][ 'date_range' ] )
            title.append( resD[ 'payload' ]['file_name'] )

            hdr_info_D[ ( resD[ 'payload' ][ 'summary' ] ) ] = createJsonFeats.returnEmbed( resD['payload']['hdr_info'] )

    top_by_vector_score_ = dict( sorted( corpus_.items(), key=lambda x:x[1], reverse=True ) )
    for idx, key in enumerate( list( top_by_vector_score_.keys() )[:10] ):
        print('-----------------------------------------')
        cos_dist_ = distance.cosine( emb_, hdr_info_D[ key ] )
        print('Rank ',idx+1,' CONTEXT->', key, ' SCORE->', top_by_vector_score_[key], ' HDR DISTANCE->', cos_dist_ )
        print('-----------------------------------------')

    tokenized_corpus = [doc.split(" ") for doc in list( corpus_.keys() )]
    bm25_summary_, bm25_hdr_info_, bm25_sample_summary_, bm25_dates_, title = \
            BM25Okapi(tokenized_corpus), BM25Okapi( tokenized_hdr_info_ ), \
            BM25Okapi( tokenized_sample_summary_ ), BM25Okapi( tokenized_dates_ ), BM25Okapi( title )

    tokenized_query = txt.split(" ")
    bm25_score_summary_  = bm25_summary_.get_scores(tokenized_query)
    bm25_score_hdr_  = bm25_hdr_info_.get_scores(tokenized_query)
    bm25_score_sample_  = bm25_sample_summary_.get_scores(tokenized_query)
    bm25_score_dt_  = bm25_dates_.get_scores(tokenized_query)
    score_title_  = title.get_scores(tokenized_query)

    enum_doc_scores_ = list( enumerate( bm25_score_summary_ ) )
    sorted_doc_score_ = sorted( enum_doc_scores_, key=lambda x:x[1] , reverse=True )

    enum_doc_scores_ = list( enumerate( bm25_score_hdr_ ) )
    sorted_doc_score_1 = sorted( enum_doc_scores_, key=lambda x:x[1] , reverse=True )

    enum_doc_scores_ = list( enumerate( bm25_score_sample_ ) )
    sorted_doc_score_2 = sorted( enum_doc_scores_, key=lambda x:x[1] , reverse=True )

    enum_doc_scores_ = list( enumerate( bm25_score_dt_ ) )
    sorted_doc_score_3 = sorted( enum_doc_scores_, key=lambda x:x[1] , reverse=True )

    enum_doc_scores_ = list( enumerate( score_title_ ) )
    sorted_doc_score_4 = sorted( enum_doc_scores_, key=lambda x:x[1] , reverse=True )

    for keyid, keys in enumerate( list( corpus_.keys() ) ):
        print('--------', keyid, np.asarray( sorted_doc_score_ )[:3, :1])    
        if [keyid] not in np.asarray( sorted_doc_score_ )[:3, :1]: continue

        print( 'BM25 Summary :: Text: ', keys, ' Vector score: ', corpus_[ keys ],\
                ' BM25 : ', bm25_score_summary_[keyid] )

    for keyid, keys in enumerate( list( corpus_.keys() ) ):
        print('--------', keyid, np.asarray( sorted_doc_score_1 )[:3, :1])    
        if [keyid] not in np.asarray( sorted_doc_score_1 )[:3, :1]: continue

        print( 'BM25 HDR :: Text: ', keys, ' Vector score: ', corpus_[ keys ],\
                ' BM25 : ', bm25_score_hdr_[keyid] )

    for keyid, keys in enumerate( list( corpus_.keys() ) ):
        print('--------', keyid, np.asarray( sorted_doc_score_2 )[:3, :1])    
        if [keyid] not in np.asarray( sorted_doc_score_2 )[:3, :1]: continue

        print( 'BM25 Sample :: Text: ', keys, ' Vector score: ', corpus_[ keys ],\
                ' BM25 : ', bm25_score_sample_[keyid] )

    for keyid, keys in enumerate( list( corpus_.keys() ) ):
        print('--------', keyid, np.asarray( sorted_doc_score_3 )[:3, :1])    
        if [keyid] not in np.asarray( sorted_doc_score_3 )[:3, :1]: continue

        print( 'BM25 Date :: Text: ', keys, ' Vector score: ', corpus_[ keys ],\
                ' BM25 : ', bm25_score_dt_[keyid] )

    for keyid, keys in enumerate( list( corpus_.keys() ) ):
        print('--------', keyid, np.asarray( sorted_doc_score_4 )[:3, :1])    
        if [keyid] not in np.asarray( sorted_doc_score_4 )[:3, :1]: continue

        print( 'BM25 Date :: Text: ', keys, ' Vector score: ', corpus_[ keys ],\
                ' BM25 : ', score_title_[keyid] )

CRITICALITY-> ": 4,
DEEPER-> /datadrive/IKG/code_db/python/basic_generateXLMetaData.py {'method_name': 'is_date', 'method_begin': 'def is_date( input_str):\n', 'method_end': '            return None\n', 'range': [15, 32], 'global_uses': [], 'local_uses': [{'file_path': '/datadrive/IKG/code_db/python/basic_generateXLMetaData.py', 'method_nm': 'process', 'method_defn': 'def process( colNum, sheet, tbl ):\n', 'usage': '                dtVal_ = is_date( str( sheet.cell(row=rw, column=colNum).value ) )\n', 'method_end': '        return ( False, None, None )\n'}]} returnEmbed
DEEPER-> /datadrive/IKG/code_db/python/basic_generateXLMetaData.py {'method_name': 'process', 'method_begin': 'def process( colNum, sheet, tbl ):\n', 'method_end': '        return ( False, None, None )\n', 'range': [34, 52], 'global_uses': [], 'local_uses': [{'file_path': '/datadrive/IKG/code_db/python/basic_generateXLMetaData.py', 'method_nm': 'findDateRange', 'method_defn': '    def findDateRange( self, tbl ):\n', 'usage': '            results = process(col, self.sheet, tbl)\n', 'method_end': '        return (None, None)\n'}]} returnEmbed
DEEPER-> /datadrive/IKG/code_db/python/basic_generateXLMetaData.py {'method_name': '__init__', 'method_begin': "    def __init__(self, file_path, llm='LLAMA'):\n", 'method_end': '            self.query_fn_ = llama3\n', 'range': [55, 81], 'global_uses': [], 'local_uses': []} returnEmbed
DEEPER-> /datadrive/IKG/code_db/python/basic_generateXLMetaData.py {'method_name': 'find_bounds', 'method_begin': '    def find_bounds( self, sheet, max_row, max_col, start_row ,end_row ,start_col ,end_col ):\n', 'method_end': '        return start_row, end_row, start_col, end_col\n', 'range': [83, 120], 'global_uses': [], 'local_uses': [{'file_path': '/datadrive/IKG/code_db/python/basic_generateXLMetaData.py', 'method_nm': 'find_tables', 'method_defn': '    def find_tables(self, sheet):\n', 'usage': '        start_row, end_row, start_col, end_col = self.find_bounds( sheet, max_row, max_col, start_row ,\\\n', 'method_end': '        return final_resp_[ : min( self.max_tables_per_sheet_, len( final_resp_ ) ) ]\n'}, {'file_path': '/datadrive/IKG/code_db/python/basic_generateXLMetaData.py', 'method_nm': 'find_tables', 'method_defn': '    def find_tables(self, sheet):\n', 'usage': '            start_row, end_row, start_col, end_col = self.find_bounds( sheet, max_row, max_col, end_row ,\\\n', 'method_end': '        return final_resp_[ : min( self.max_tables_per_sheet_, len( final_resp_ ) ) ]\n'}, {'file_path': '/datadrive/IKG/code_db/python/basic_generateXLMetaData.py', 'method_nm': 'find_tables', 'method_defn': '    def find_tables(self, sheet):\n', 'usage': '            start_row, end_row, start_col, end_col = self.find_bounds( sheet, max_row, max_col, None ,\\\n', 'method_end': '        return final_resp_[ : min( self.max_tables_per_sheet_, len( final_resp_ ) ) ]\n'}]} returnEmbed
DEEPER-> /datadrive/IKG/code_db/python/basic_generateXLMetaData.py {'method_name': 'is_hdr_row_format', 'method_begin': '    def is_hdr_row_format( self, tbl_bound, sheet ):\n', 'method_end': '        return True\n', 'range': [122, 131], 'global_uses': [], 'local_uses': []} returnEmbed
DEEPER-> /datadrive/IKG/code_db/python/basic_generateXLMetaData.py {'method_name': 'find_tables', 'method_begin': '    def find_tables(self, sheet):\n', 'method_end': '        return final_resp_[ : min( self.max_tables_per_sheet_, len( final_resp_ ) ) ]\n', 'range': [133, 217], 'global_uses': [], 'local_uses': [{'file_path': '/datadrive/IKG/code_db/python/basic_generateXLMetaData.py', 'method_nm': 'read_excel_file', 'method_defn': '    def read_excel_file(self):\n', 'usage': '            all_tables_ = self.find_tables( self.sheet )\n', 'method_end': '                self.masterInfo_[ sheet_name ] = summary_D\n'}]} returnEmbed
DEEPER-> /datadrive/IKG/code_db/python/basic_generateXLMetaData.py {'method_name': 'findDateRange', 'method_begin': '    def findDateRange( self, tbl ):\n', 'method_end': '        return (None, None)\n', 'range': [219, 230], 'global_uses': [], 'local_uses': [{'file_path': '/datadrive/IKG/code_db/python/basic_generateXLMetaData.py', 'method_nm': 'read_excel_file', 'method_defn': '    def read_excel_file(self):\n', 'usage': "                    summary_D['date_range'] = self.findDateRange( tbl_ )\n", 'method_end': '                self.masterInfo_[ sheet_name ] = summary_D\n'}]} returnEmbed
DEEPER-> /datadrive/IKG/code_db/python/basic_generateXLMetaData.py {'method_name': 'findHeaderInfo', 'method_begin': '    def findHeaderInfo(self, tbl):\n', 'method_end': '        return col_frame_\n', 'range': [232, 252], 'global_uses': [], 'local_uses': [{'file_path': '/datadrive/IKG/code_db/python/basic_generateXLMetaData.py', 'method_nm': 'read_excel_file', 'method_defn': '    def read_excel_file(self):\n', 'usage': '                    hdr_frame_ = self.findHeaderInfo( tbl_ )\n', 'method_end': '                self.masterInfo_[ sheet_name ] = summary_D\n'}]} returnEmbed
DEEPER-> /datadrive/IKG/code_db/python/basic_generateXLMetaData.py {'method_name': 'findHighVarianceColumns', 'method_begin': '    def findHighVarianceColumns(self, start_hdr_row_, sheet, tbl ):\n', 'method_end': '        return list( high_var_indices_ ), hdr_col_names_\n', 'range': [254, 310], 'global_uses': [], 'local_uses': [{'file_path': '/datadrive/IKG/code_db/python/basic_generateXLMetaData.py', 'method_nm': 'returnSummary', 'method_defn': '    def returnSummary(self, tbl ):\n', 'usage': '        high_variance_cols_, hdr_col_names_ = self.findHighVarianceColumns( start_hdr_row_, self.sheet, tbl )\n', 'method_end': '        return frame_, high_variance_cols_, list( set(hdr_col_names_) )\n'}]} returnEmbed
DEEPER-> /datadrive/IKG/code_db/python/basic_generateXLMetaData.py {'method_name': 'returnSummary', 'method_begin': '    def returnSummary(self, tbl ):\n', 'method_end': '        return frame_, high_variance_cols_, list( set(hdr_col_names_) )\n', 'range': [313, 338], 'global_uses': [], 'local_uses': [{'file_path': '/datadrive/IKG/code_db/python/basic_generateXLMetaData.py', 'method_nm': 'read_excel_file', 'method_defn': '    def read_excel_file(self):\n', 'usage': '                    frame_, high_variance_cols_, col_names_ = self.returnSummary( tbl_ )\n', 'method_end': '                self.masterInfo_[ sheet_name ] = summary_D\n'}]} returnEmbed
DEEPER-> /datadrive/IKG/code_db/python/basic_generateXLMetaData.py {'method_name': 'findHdrRow', 'method_begin': '    def findHdrRow( self, tbl ):\n', 'method_end': '        return None # so default value of row #1 applies to table start\n', 'range': [340, 358], 'global_uses': [], 'local_uses': [{'file_path': '/datadrive/IKG/code_db/python/basic_generateXLMetaData.py', 'method_nm': 'findHeaderInfo', 'method_defn': '    def findHeaderInfo(self, tbl):\n', 'usage': '        hdr_row_start_ = self.findHdrRow( tbl )\n', 'method_end': '        return col_frame_\n'}]} returnEmbed
DEEPER-> /datadrive/IKG/code_db/python/basic_generateXLMetaData.py {'method_name': 'createDBRec', 'method_begin': "    def createDBRec( self, summary_D, mode='NORM' ):\n", 'method_end': '        return insertRec\n', 'range': [360, 384], 'global_uses': [], 'local_uses': [{'file_path': '/datadrive/IKG/code_db/python/basic_generateXLMetaData.py', 'method_nm': 'mergeAndInsert', 'method_defn': '    def mergeAndInsert( self, summary_D ):\n', 'usage': "        rec_ = self.createDBRec( summary_D, 'NORM' )\n", 'method_end': '        db_utils.insertNewSignature( rec_ )\n'}]} returnEmbed
POE-> ['        emb_ = createJsonFeats.returnEmbed( unified_key_ )\n']
CODE_SNIP-> emb_ = createJsonFeats.returnEmbed( unified_key_ )

CODE_SNIP-> emb_ = createJsonFeats.returnEmbed( unified_key_ )

CODE_SNIP-> emb_ = createJsonFeats.returnEmbed( unified_key_ )

BRAM-> ['createJsonFeats', 'returnEmbed', 'unified_key_'] emb_ 1 1
Furthest assignment of  emb_  is  -1 10000
cmpOldNew-> {'Type': 'Assignment', 'Targets': ['emb_'], 'Ending': 'NA', 'Values': ['createJsonFeats', 'returnEmbed', 'unified_key_'], 'Function': 'returnEmbed'}
TIMMY-> (10000, -1)
Sending the entire code of < ['        emb_ = createJsonFeats.returnEmbed( unified_key_ )\n'] > for review 360 384
RETURNING-> 
        insertRec = dict()
        insertRec['docID'] = random.randint( 1000, 100000 )
        ## combine all necessary fields to form vector signature
        ## keys-> 'sample_summary'; 'date_range' ; 'hdr_info'

        hdr_info = summary_D['hdr_info']
        sample_summary_ = summary_D['sample_summary']

        unified_key_ =   'Date Range : '+ str( summary_D['date_range'] ) + '\n' \
                       + 'Column Headers : '+ ' , '.join( summary_D['col_names_'] ).strip() + '\n' \
                       + 'LLM Summary : '+ ( sample_summary_ ) if self.add_ai_summary_to_embedding_ is True else ''

        emb_ = createJsonFeats.returnEmbed( unified_key_ )
        insertRec['docSignature'] = emb_
        insertRec['summary'] = unified_key_
        insertRec['file_path'] = summary_D['file_path']
        insertRec['file_name'] = summary_D['file_path'].split('/')[-1]
        insertRec['sheet_name'] = summary_D['sheet_name']
        insertRec['date_range'] = summary_D['date_range']
        insertRec['hdr_info'] = hdr_info

        print('Inserting RECORD->', insertRec['file_name'], insertRec['sheet_name'], unified_key_ )
        return insertRec

CALLING LLM addChangeImpactOnDownstreamFile-> 
    rec_ = { 'sentence': sent }

    data = json.dumps( rec_ ).encode('utf-8')
    _request = urllib.request.Request( url_encode, data=data, method='POST', \
                                        headers={'Content-Type': 'application/json'} )

    response = urllib.request.urlopen( _request )
    string = response.read().decode('utf-8')
    json_obj = json.loads(string)
    
    return json_obj['encoded_'], True

Changed line - NEW :     string = response.read()

 OLD :     string = response.read().decode('utf-8')

 downstream file importing returnEmbed

        insertRec = dict()
        insertRec['docID'] = random.randint( 1000, 100000 )
        ## combine all necessary fields to form vector signature
        ## keys-> 'sample_summary'; 'date_range' ; 'hdr_info'

        hdr_info = summary_D['hdr_info']
        sample_summary_ = summary_D['sample_summary']

        unified_key_ =   'Date Range : '+ str( summary_D['date_range'] ) + '\n' \
                       + 'Column Headers : '+ ' , '.join( summary_D['col_names_'] ).strip() + '\n' \
                       + 'LLM Summary : '+ ( sample_summary_ ) if self.add_ai_summary_to_embedding_ is True else ''

        emb_ = createJsonFeats.returnEmbed( unified_key_ )
        insertRec['docSignature'] = emb_
        insertRec['summary'] = unified_key_
        insertRec['file_path'] = summary_D['file_path']
        insertRec['file_name'] = summary_D['file_path'].split('/')[-1]
        insertRec['sheet_name'] = summary_D['sheet_name']
        insertRec['date_range'] = summary_D['date_range']
        insertRec['hdr_info'] = hdr_info

        print('Inserting RECORD->', insertRec['file_name'], insertRec['sheet_name'], unified_key_ )
        return insertRec

executeInstruction => you will be given the following: Existing Code , Changed lines in new code and the code snippet for a file that imports OR uses the changed method. You will have to analyze the impact of the changes made to the base method and add notes on how this impacts the downstream method that imports OR uses this code. Also rate the criticality of this change with 1 being nominal and 5 being deserves immediate attention. Kindly format the data in the form of a json with 3 keys - Issues, Criticality and Recommendations. The values of these keys can be lists with as many values as you can think of and return a final dictionary with the 3 keys

Existing Code :


    rec_ = { 'sentence': sent }

    data = json.dumps( rec_ ).encode('utf-8')
    _request = urllib.request.Request( url_encode, data=data, method='POST', \
                                        headers={'Content-Type': 'application/json'} )

    response = urllib.request.urlopen( _request )
    string = response.read().decode('utf-8')
    json_obj = json.loads(string)
    
    return json_obj['encoded_'], True

Changed line - NEW :     string = response.read()

 OLD :     string = response.read().decode('utf-8')

 downstream file importing returnEmbed

        insertRec = dict()
        insertRec['docID'] = random.randint( 1000, 100000 )
        ## combine all necessary fields to form vector signature
        ## keys-> 'sample_summary'; 'date_range' ; 'hdr_info'

        hdr_info = summary_D['hdr_info']
        sample_summary_ = summary_D['sample_summary']

        unified_key_ =   'Date Range : '+ str( summary_D['date_range'] ) + '\n' \
                       + 'Column Headers : '+ ' , '.join( summary_D['col_names_'] ).strip() + '\n' \
                       + 'LLM Summary : '+ ( sample_summary_ ) if self.add_ai_summary_to_embedding_ is True else ''

        emb_ = createJsonFeats.returnEmbed( unified_key_ )
        insertRec['docSignature'] = emb_
        insertRec['summary'] = unified_key_
        insertRec['file_path'] = summary_D['file_path']
        insertRec['file_name'] = summary_D['file_path'].split('/')[-1]
        insertRec['sheet_name'] = summary_D['sheet_name']
        insertRec['date_range'] = summary_D['date_range']
        insertRec['hdr_info'] = hdr_info

        print('Inserting RECORD->', insertRec['file_name'], insertRec['sheet_name'], unified_key_ )
        return insertRec

CRITICALITY-> ": 3,
DEEPER-> /datadrive/IKG/code_db/python/addtoDB.py {'method_name': 'addToDB', 'method_begin': 'def addToDB():\n', 'method_end': '            db_utils.insertNewSignature( dd_ )\n', 'range': [9, 16], 'global_uses': [], 'local_uses': []} returnEmbed
POE-> ['            emb_ = createJsonFeats.returnEmbed( txt )\n']
CODE_SNIP-> emb_ = createJsonFeats.returnEmbed( txt )

CODE_SNIP-> emb_ = createJsonFeats.returnEmbed( txt )

CODE_SNIP-> emb_ = createJsonFeats.returnEmbed( txt )

BRAM-> ['createJsonFeats', 'returnEmbed', 'txt'] emb_ 1 1
Furthest assignment of  emb_  is  -1 10000
cmpOldNew-> {'Type': 'Assignment', 'Targets': ['emb_'], 'Ending': 'NA', 'Values': ['createJsonFeats', 'returnEmbed', 'txt'], 'Function': 'returnEmbed'}
TIMMY-> (10000, -1)
Sending the entire code of < ['            emb_ = createJsonFeats.returnEmbed( txt )\n'] > for review 9 16
RETURNING->     for fnm, sheets in js_.items():
        for sheetname, txt in sheets.items():
            cnt_ += 1
            emb_ = createJsonFeats.returnEmbed( txt )
            dd_ = { 'text': txt, 'docSignature': emb_, 'docID': cnt_ }

            db_utils.insertNewSignature( dd_ )

CALLING LLM addChangeImpactOnDownstreamFile-> 
    rec_ = { 'sentence': sent }

    data = json.dumps( rec_ ).encode('utf-8')
    _request = urllib.request.Request( url_encode, data=data, method='POST', \
                                        headers={'Content-Type': 'application/json'} )

    response = urllib.request.urlopen( _request )
    string = response.read().decode('utf-8')
    json_obj = json.loads(string)
    
    return json_obj['encoded_'], True

Changed line - NEW :     string = response.read()

 OLD :     string = response.read().decode('utf-8')

 downstream file importing returnEmbed
    for fnm, sheets in js_.items():
        for sheetname, txt in sheets.items():
            cnt_ += 1
            emb_ = createJsonFeats.returnEmbed( txt )
            dd_ = { 'text': txt, 'docSignature': emb_, 'docID': cnt_ }

            db_utils.insertNewSignature( dd_ )

executeInstruction => you will be given the following: Existing Code , Changed lines in new code and the code snippet for a file that imports OR uses the changed method. You will have to analyze the impact of the changes made to the base method and add notes on how this impacts the downstream method that imports OR uses this code. Also rate the criticality of this change with 1 being nominal and 5 being deserves immediate attention. Kindly format the data in the form of a json with 3 keys - Issues, Criticality and Recommendations. The values of these keys can be lists with as many values as you can think of and return a final dictionary with the 3 keys

Existing Code :


    rec_ = { 'sentence': sent }

    data = json.dumps( rec_ ).encode('utf-8')
    _request = urllib.request.Request( url_encode, data=data, method='POST', \
                                        headers={'Content-Type': 'application/json'} )

    response = urllib.request.urlopen( _request )
    string = response.read().decode('utf-8')
    json_obj = json.loads(string)
    
    return json_obj['encoded_'], True

Changed line - NEW :     string = response.read()

 OLD :     string = response.read().decode('utf-8')

 downstream file importing returnEmbed
    for fnm, sheets in js_.items():
        for sheetname, txt in sheets.items():
            cnt_ += 1
            emb_ = createJsonFeats.returnEmbed( txt )
            dd_ = { 'text': txt, 'docSignature': emb_, 'docID': cnt_ }

            db_utils.insertNewSignature( dd_ )

CRITICALITY-> ": 4,
HULLO ALLO-> [
    {
        "file": "code_db/python/createJsonFeats.py",
        "old_start": 133,
        "old_length": 8,
        "new_start": 133,
        "new_length": 8,
        "old_code": [
            "    string = response.read().decode('utf-8')\n"
        ],
        "new_code": [
            "    string = response.read()\n"
        ],
        "method_class_nm_old": {
            "class_nm": "",
            "method_nm": "returnEmbed"
        },
        "method_class_nm_new": {
            "class_nm": "",
            "method_nm": "returnEmbed"
        },
        "method_context": "\n    rec_ = { 'sentence': sent }\n\n    data = json.dumps( rec_ ).encode('utf-8')\n    _request = urllib.request.Request( url_encode, data=data, method='POST', \\\n                                        headers={'Content-Type': 'application/json'} )\n\n    response = urllib.request.urlopen( _request )\n    string = response.read().decode('utf-8')\n    json_obj = json.loads(string)\n    \n    return json_obj['encoded_'], True\n",
        "base_change_impact": "Here is the analysis of the changed code:\n\n```\n{\n    \"Issues\": [\n        \"The change removes the decoding of the response from UTF-8, which may lead to issues with non-ASCII characters\",\n        \"The response is now a bytes object instead of a string, which may cause errors in subsequent processing\"\n    ],\n    \"Criticality\": 4,\n    \"Recommendations\": [\n        \"Revert the change and keep the decoding to ensure correct handling of non-ASCII characters\",\n        \"Consider adding error handling to deal with potential encoding issues\",\n        \"Verify that the subsequent code can handle bytes objects instead of strings\"\n    ]\n}\n```\n\nThe criticality is rated as 4 because the change can cause issues with non-ASCII characters and may break the subsequent processing of the response.",
        "base_change_criticality": "4",
        "impact_analysis": [
            {
                "impacted_method": "/datadrive/IKG/code_db/python/searchDB.py/pos",
                "impacted_code_snippet": [
                    "            hdr_info_D[ ( resD[ 'payload' ][ 'summary' ] ) ] = createJsonFeats.returnEmbed( resD['payload']['hdr_info'] )\n"
                ],
                "impacted_code_range": [
                    14,
                    99
                ],
                "impacted_code_context": "  if 'searchRes_' in res_:\n    act_ = res_[ 'searchRes_' ]\n    print( act_ )\n    tokenized_hdr_info_ , tokenized_sample_summary_, tokenized_dates_, title = [], [], [], []\n    hdr_info_D = dict()\n\n    for res_nm, resD in act_.items():\n        if 'payload' in resD and 'summary' in resD[ 'payload' ]:\n            corpus_[ ( resD[ 'payload' ][ 'summary' ] ) ] = resD[ 'score' ]\n\n            tokenized_hdr_info_.append( resD[ 'payload' ][ 'hdr_info' ] )\n            tokenized_sample_summary_.append( 'sample' )\n            tokenized_dates_.append( resD[ 'payload' ][ 'date_range' ] )\n            title.append( resD[ 'payload' ]['file_name'] )\n\n            hdr_info_D[ ( resD[ 'payload' ][ 'summary' ] ) ] = createJsonFeats.returnEmbed( resD['payload']['hdr_info'] )\n\n    top_by_vector_score_ = dict( sorted( corpus_.items(), key=lambda x:x[1], reverse=True ) )\n    for idx, key in enumerate( list( top_by_vector_score_.keys() )[:10] ):\n        print('-----------------------------------------')\n        cos_dist_ = distance.cosine( emb_, hdr_info_D[ key ] )\n        print('Rank ',idx+1,' CONTEXT->', key, ' SCORE->', top_by_vector_score_[key], ' HDR DISTANCE->', cos_dist_ )\n        print('-----------------------------------------')\n\n    tokenized_corpus = [doc.split(\" \") for doc in list( corpus_.keys() )]\n    bm25_summary_, bm25_hdr_info_, bm25_sample_summary_, bm25_dates_, title = \\\n            BM25Okapi(tokenized_corpus), BM25Okapi( tokenized_hdr_info_ ), \\\n            BM25Okapi( tokenized_sample_summary_ ), BM25Okapi( tokenized_dates_ ), BM25Okapi( title )\n\n    tokenized_query = txt.split(\" \")\n    bm25_score_summary_  = bm25_summary_.get_scores(tokenized_query)\n    bm25_score_hdr_  = bm25_hdr_info_.get_scores(tokenized_query)\n    bm25_score_sample_  = bm25_sample_summary_.get_scores(tokenized_query)\n    bm25_score_dt_  = bm25_dates_.get_scores(tokenized_query)\n    score_title_  = title.get_scores(tokenized_query)\n\n    enum_doc_scores_ = list( enumerate( bm25_score_summary_ ) )\n    sorted_doc_score_ = sorted( enum_doc_scores_, key=lambda x:x[1] , reverse=True )\n\n    enum_doc_scores_ = list( enumerate( bm25_score_hdr_ ) )\n    sorted_doc_score_1 = sorted( enum_doc_scores_, key=lambda x:x[1] , reverse=True )\n\n    enum_doc_scores_ = list( enumerate( bm25_score_sample_ ) )\n    sorted_doc_score_2 = sorted( enum_doc_scores_, key=lambda x:x[1] , reverse=True )\n\n    enum_doc_scores_ = list( enumerate( bm25_score_dt_ ) )\n    sorted_doc_score_3 = sorted( enum_doc_scores_, key=lambda x:x[1] , reverse=True )\n\n    enum_doc_scores_ = list( enumerate( score_title_ ) )\n    sorted_doc_score_4 = sorted( enum_doc_scores_, key=lambda x:x[1] , reverse=True )\n\n    for keyid, keys in enumerate( list( corpus_.keys() ) ):\n        print('--------', keyid, np.asarray( sorted_doc_score_ )[:3, :1])    \n        if [keyid] not in np.asarray( sorted_doc_score_ )[:3, :1]: continue\n\n        print( 'BM25 Summary :: Text: ', keys, ' Vector score: ', corpus_[ keys ],\\\n                ' BM25 : ', bm25_score_summary_[keyid] )\n\n    for keyid, keys in enumerate( list( corpus_.keys() ) ):\n        print('--------', keyid, np.asarray( sorted_doc_score_1 )[:3, :1])    \n        if [keyid] not in np.asarray( sorted_doc_score_1 )[:3, :1]: continue\n\n        print( 'BM25 HDR :: Text: ', keys, ' Vector score: ', corpus_[ keys ],\\\n                ' BM25 : ', bm25_score_hdr_[keyid] )\n\n    for keyid, keys in enumerate( list( corpus_.keys() ) ):\n        print('--------', keyid, np.asarray( sorted_doc_score_2 )[:3, :1])    \n        if [keyid] not in np.asarray( sorted_doc_score_2 )[:3, :1]: continue\n\n        print( 'BM25 Sample :: Text: ', keys, ' Vector score: ', corpus_[ keys ],\\\n                ' BM25 : ', bm25_score_sample_[keyid] )\n\n    for keyid, keys in enumerate( list( corpus_.keys() ) ):\n        print('--------', keyid, np.asarray( sorted_doc_score_3 )[:3, :1])    \n        if [keyid] not in np.asarray( sorted_doc_score_3 )[:3, :1]: continue\n\n        print( 'BM25 Date :: Text: ', keys, ' Vector score: ', corpus_[ keys ],\\\n                ' BM25 : ', bm25_score_dt_[keyid] )\n\n    for keyid, keys in enumerate( list( corpus_.keys() ) ):\n        print('--------', keyid, np.asarray( sorted_doc_score_4 )[:3, :1])    \n        if [keyid] not in np.asarray( sorted_doc_score_4 )[:3, :1]: continue\n\n        print( 'BM25 Date :: Text: ', keys, ' Vector score: ', corpus_[ keys ],\\\n                ' BM25 : ', score_title_[keyid] )\n",
                "criticality": "4",
                "impact_analysis": "Here is the analysis of the impact of the change in the `returnEmbed` function:\n\n```\n{\n  \"Issues\": [\n    \"The change from `string = response.read().decode('utf-8')` to `string = response.read()` may cause issues with character encoding. The downstream code may not be prepared to handle binary data.\",\n    \"The downstream code is using the return values of `returnEmbed` to perform various operations, including BM25 calculations. If the encoding is not correct, these calculations may produce incorrect results.\"\n  ],\n  \"Criticality\": 4,\n  \"Recommendations\": [\n    \"Verify that the downstream code can handle binary data correctly.\",\n    \"Test the entire pipeline with the new `returnEmbed` function to ensure that the results are correct.\",\n    \"Consider adding error handling to the downstream code to detect and handle any encoding issues.\"\n  ]\n}\n```\n\nThe criticality of this change is rated as 4, as it may cause issues with character encoding and affect the downstream code that relies on the correct encoding.",
                "impact_type": "global"
            },
            {
                "impacted_method": "/datadrive/IKG/code_db/python/basic_generateXLMetaData.py/createDBRec",
                "impacted_code_snippet": [
                    "        emb_ = createJsonFeats.returnEmbed( unified_key_ )\n"
                ],
                "impacted_code_range": [
                    360,
                    384
                ],
                "impacted_code_context": "\n        insertRec = dict()\n        insertRec['docID'] = random.randint( 1000, 100000 )\n        ## combine all necessary fields to form vector signature\n        ## keys-> 'sample_summary'; 'date_range' ; 'hdr_info'\n\n        hdr_info = summary_D['hdr_info']\n        sample_summary_ = summary_D['sample_summary']\n\n        unified_key_ =   'Date Range : '+ str( summary_D['date_range'] ) + '\\n' \\\n                       + 'Column Headers : '+ ' , '.join( summary_D['col_names_'] ).strip() + '\\n' \\\n                       + 'LLM Summary : '+ ( sample_summary_ ) if self.add_ai_summary_to_embedding_ is True else ''\n\n        emb_ = createJsonFeats.returnEmbed( unified_key_ )\n        insertRec['docSignature'] = emb_\n        insertRec['summary'] = unified_key_\n        insertRec['file_path'] = summary_D['file_path']\n        insertRec['file_name'] = summary_D['file_path'].split('/')[-1]\n        insertRec['sheet_name'] = summary_D['sheet_name']\n        insertRec['date_range'] = summary_D['date_range']\n        insertRec['hdr_info'] = hdr_info\n\n        print('Inserting RECORD->', insertRec['file_name'], insertRec['sheet_name'], unified_key_ )\n        return insertRec\n",
                "criticality": "3",
                "impact_analysis": "Here is the analysis of the changed code:\n\n```\n{\n    \"Issues\": [\n        \"The `decode('utf-8')` method has been removed from the `response.read()` line, which may cause issues with character encoding.\"\n    ],\n    \"Criticality\": 3,\n    \"Recommendations\": [\n        \"Verify that the response from the API is in the expected encoding format.\",\n        \"Consider adding error handling to handle potential encoding issues.\",\n        \"Test the downstream method `returnEmbed` to ensure it can handle the changed response format.\"\n    ]\n}\n```\n\nExplanation:\n\n* The changed line removes the `.decode('utf-8')` method, which was used to decode the response from the API. This may cause issues with character encoding, especially if the API response contains non-ASCII characters.\n* The criticality of this change is rated as 3, as it may not affect the functionality of the code immediately, but may cause issues down the line if the encoding is not handled correctly.\n* Recommendations include verifying the encoding format of the API response, adding error handling to handle potential encoding issues, and testing the downstream method `returnEmbed` to ensure it can handle the changed response format.",
                "impact_type": "global"
            },
            {
                "impacted_method": "/datadrive/IKG/code_db/python/addtoDB.py/addToDB",
                "impacted_code_snippet": [
                    "            emb_ = createJsonFeats.returnEmbed( txt )\n"
                ],
                "impacted_code_range": [
                    9,
                    16
                ],
                "impacted_code_context": "    for fnm, sheets in js_.items():\n        for sheetname, txt in sheets.items():\n            cnt_ += 1\n            emb_ = createJsonFeats.returnEmbed( txt )\n            dd_ = { 'text': txt, 'docSignature': emb_, 'docID': cnt_ }\n\n            db_utils.insertNewSignature( dd_ )\n",
                "criticality": "4",
                "impact_analysis": "Here is the analysis of the change and its impact on the downstream file:\n\n```\n{\n    \"Issues\": [\n        \"The change from `string = response.read().decode('utf-8')` to `string = response.read()` may cause issues with character encoding. The original code explicitly decoded the response from bytes to string using UTF-8 encoding. Without this decoding, the string may contain unknown or incorrect characters.\",\n        \"This change may also cause issues with the JSON parsing in the downstream file, as the JSON library may not be able to parse the string correctly if it contains invalid characters.\"\n    ],\n    \"Criticality\": 4,\n    \"Recommendations\": [\n        \"Verify that the response from the POST request is encoded in a format that does not require explicit decoding, such as ASCII or UTF-8.\",\n        \"Test the downstream file thoroughly to ensure that it can handle the changed response format.\",\n        \"Consider adding error handling to the downstream file to catch any potential parsing errors.\"\n    ]\n}\n```\n\nNote: The criticality rating of 4 indicates that this change may cause significant issues in the downstream file, and should be thoroughly tested and reviewed before deployment.",
                "impact_type": "global"
            }
        ]
    }
]
<Response [200]>
Checking -> docx ['xls', 'xlsx']
Checking -> docx ['docx']
Checking -> docx ['docx']
Test-File-> 1809 
 /datadrive/IKG/test_db/test_plans/BRD-Project Mark.docx
executeInstruction => 
Here is the analysis of the change and its impact on the downstream file:

```
{
    "Issues": [
        "The change from `string = response.read().decode('utf-8')` to `string = response.read()` may cause issues with character encoding. The original code explicitly decoded the response from bytes to string using UTF-8 encoding. Without this decoding, the string may contain unknown or incorrect characters.",
        "This change may also cause issues with the JSON parsing in the downstream file, as the JSON library may not be able to parse the string correctly if it contains invalid characters."
    ],
    "Criticality": 4,
    "Recommendations": [
        "Verify that the response from the POST request is encoded in a format that does not require explicit decoding, such as ASCII or UTF-8.",
        "Test the downstream file thoroughly to ensure that it can handle the changed response format.",
        "Consider adding error handling to the downstream file to catch any potential parsing errors."
    ]
}
```

Note: The criticality rating of 4 indicates that this change may cause significant issues in the downstream file, and should be thoroughly tested and reviewed before deployment.
The above is the summary of a code change and given below are some test cases. You will have to analyze the summary of the code changes and figure out if any of the test cases below require review OR will be impacted by the code changes.

 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Contents
 
1.   	Introduction
2.   	Purpose of the Document
3.   	Scope of Work
4.   	Reports and Accuracy Logics
5.   	Billing Logic
6.   	Technical Infrastructure Requirement Details [PROD & UAT]
 
 
 
 
 
 
 
 
1.	Introduction
The main purpose of this project is to provide an automated invoice processing system “IDP (Intelligent Document Processing)” to ensure smooth, quick and efﬁcient services to REQUORDIT by AmyGB.ai. As compared to other mechanism of invoice processing, IDP will ensure that the requests, which falls under the scope can be addressed within the agreed and definite TAT committed to the customer. This will help the REQUORDIT team to reduce the TAT for document processing and have a higher productivity in their day to day operations.
 
 
 
 
 
2.	Purpose of the Document
The purpose of the document is to list down and document each and every necessary steps like Current Flow, Proposed Flow, Scope of the Project, Technical Infra Details and Report Mechanism.
This Document is prepared on the basis of the discussions carried out by various discussions between REQUORDIT and AmyGB Team.
 
 
 
 
 
 
 
 
 
 
3.    Requirement Scope
An application to be developed by AmyGB to Automate the invoices wherein After the invoice is uploaded via API, Application will carry out the OCR, extract the defined key values and get the review done by the indexers and post review, make the file available to download in the PDF/tiff format.
 
 
An avg. of 8000 documents will get processed/day except month end wherein this may reach up to 20000 (last couple of days of the month). Each doc will contain an average of 5 pages. 80% of the docs for the day are uploaded between 6AM -8AM CDT which to be processed by noon and output will be downloaded by 6 PM CDT. Processing shall happen on AWS - on cloud. Extracted data is deleted (OCR- text + image) post 23 Hrs(From the time file is downloaded) and retain the meta data (which include the kvp, line items, doc class etc.)
 
 
 
 
Below are the discussed scoped items to be delivered in the phase wise manner-
 
1.	Classification/Re-Classification
2.	Extraction & Review
3.	Supervisor QC model
4.	User & Team Management
5.	PDF & TIFF Download options
6.	Rotation of a file
7.	File merging and Re-ordering
8.     File lock and release 
9.	Tabular Content Extraction + Draw Table Bounds only for tabular failures
10.  Tabular OCR Review with merging/splitting of rows on UI
11.  Real time generation of reports of various doc lifecycle
12.  Feedback Module
 
 
 
 
 
 
 
 
 
The following is the High Level process flow-
 
 
 
 
 
 
 
4.	Scope of Work
The Scope of work for the project will be divided into three parts:
I.          Phase 1 Scope
II.        Phase 2 Scope
III.  	Phase 3 Scope
 
 
 
 
 
I.         Phase 1 Scope:
   Features to be considered for Phase 1 are as follows based on the discussions with the REQUORDIT team.
 
1.     Classification & Extraction Review
2.     Supervisor QC model
3.     User & Team Management
4.     PDF & TIFF Download options
5.     Rotation of a file
6.     File merging and Re-ordering
7. 	File lock and release 
8.  	Real time generation of reports
 
 
   	Phase 1 will consist scenarios related to the above categories described as below
l  Classification/Re-Classification & Extraction Review-
A document should go through the OCR, and get classified as per the standard Invoice Or Supporting Document. Post Classification, if we realize there was error in classification, it should have option to re-classify and submit classification. Once classification is done, 16 defined key values with mandatory and optional fields to be extracted and reviewed by the indexers.
 
l  Supervisor QC model-
Also review can be done by the supervisor and admin as part of Supervisor QC review process wherein set % of the particular customer id based on the team it belongs to, files will flow to mapped supervisor for the QC
 
l  User & Team Management-
This will have 3 roles namely as 1) Indexer 2) Supervisor 3) Admin
Sup and Admin role will be allowed the addition, update and mapping of the users to customer IDs and supervisors
 
l  PDF & TIFF Download options-
Once the review is completed and submitted from both indexers and supervisors (in 100% QC model), file should be available to be downloaded as PDF/Tiff for the end customer
 
 
l  Rotation of a file-
If a document is skewed as an input, while performing the classification, it should rotate the image to 90 Degree or more and same should get downloaded as an output post completion of the Classification and Extraction
 
l  File merging and Re-ordering-
If an invoice is available in file1 out of multiple files in the same batch and Supporting Document of the respective invoice is available in file2, we should have option to merge the SD from File2 with Invoice in file1 and vice versa. This should have option of re-ordering of the page count with respect to the changes
 
l  File lock and release-
This feature will function as, once a file is opened by any indexer, it will be locked for 30 minutes and no one else can work on that file. Also, if an indexer opens multiple tabs or try to work on multiple files together, it will release the older files and allow to work on the latest file only to avoid the ambiguity 
 
l  Real time generation of reports of various doc lifecycle-
There should be an option available which can be used to generate    the reports on the real time for the document processing stats from Admin panel
 
 
Note- After the Go Live, the timeline for the Phase 1 hyper-care stage is 75 days where the AmyGB team will analyze the accuracy on the area of opportunities and fix the bugs comes on the way
 
 
II.      Phase 2 Scope:
Features to be considered for Phase 2 are as follows based on the discussions with the REQUORDIT team.
 
1.  	Table extraction spread across multiple pages within same/different files
2.  	Draw table bounds if table missed or incomplete table identified
3.  	Addition of rows at end of the table (new rows or missing row)
4.  	Merge/split/editing of rows of the table
5.  	QC of the extracted tabular data/line items with option to pick values via snippet
   	Phase 2 will consist scenarios related to the above categories described as below
l  Table extraction spread across multiple pages within same/different files-
This feature should allow the data to be extracted from the tabular invoices across multiple pages within same/different files
 
l  Draw table bounds if table is missed or incomplete -
Draw table bounds if table is missed or incomplete table identification for reprocessing via OCR
 
l  Addition of rows-
This feature will allow to add of rows at end of the table (new rows or missing row)
 
l  Merge/split/editing of rows of the table-
This feature should allow to merge/split/Edit the table rows where it is needed
 
l  QC of the extracted tabular data/line items -
This feature should allow to do the QC of the extracted tabular data/line items with option to pick values via snippet
 
 
Note- After the Go Live, the timeline for the Phase 2 to start is after 75 days, the requirements for the Phase 2 will commence.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
III.   Phase 3 Scope:
Features to be considered for Phase 3 are as follows based on the discussions with the REQUORDIT team.
 
1.     Feedback Module
 
 
   	Phase 3 will consist scenarios related to the above categories described as below
l  Feedback Module-
This feature should allow the application to accept the feedback bases the inputs from the users. This will allow Key - Value Feedback - Instead of manual corrections on all key value extraction give feedback on selected failed files. The USP of this feature is that it also retrains the doc extraction module so that similar errors are not repeated in future batches
 
 
5.	Reports and Accuracy Logics
This section will give a clarity on the Mechanism of Reports for Uploaded and Processed Data.
 
After we go live with the solution in place, we shall generate the reports for document life cycle which should have visibility on page level details of the Invoices and supporting documents for billing purpose.
 
l  Monthly Billing Reports: Monthly billing reports will be created for billing purpose. This report will contain all the invoices along with page detail processed by the IDP for a specific month. This report will be used by AmyGB for all the invoicing purpose
 
 
6.	Billing Logic
The billing will be based on the total number of pages of the Invoices processed by the application. Commercials are closed over other email thread.
 
 
 
 
 
7.	Technical Infrastructure Requirement Details. [PROD & UAT]
This section will list down all the technical level infrastructural details for Production Server that will be necessary for this project. The scope of the project will require technical infrastructure at the REQUORDIT UAT server and also at the Production server.
 
 
 
 
 
 
Infrastructure for UAT/Staging Server for UAT Testing:
After the training of the Application, the codes will be deployed on the UAT server of the REQUORDIT. It is important for the UAT server infrastructure to be similar to PROD server, as all the initial level testing will get carried out on the UAT server. UAT server should also have the similar API’s as the PROD server for the AmyGB to carry out the UAT level testing for the same.
 
Note- Any changes, bug fix or new requirements to be tested on Staging first before deploying it to Prod to have the no impact on the Prod environment
 
 
THANK YOU
 


Checking -> xlsx ['xls', 'xlsx']
Checking -> xlsx ['docx']
Checking -> xlsx ['xls', 'xlsx']
Test-File-> 5888 
 /datadrive/IKG/test_db/test_plans/IKG-testing.xlsx
executeInstruction => 
Here is the analysis of the change and its impact on the downstream file:

```
{
    "Issues": [
        "The change from `string = response.read().decode('utf-8')` to `string = response.read()` may cause issues with character encoding. The original code explicitly decoded the response from bytes to string using UTF-8 encoding. Without this decoding, the string may contain unknown or incorrect characters.",
        "This change may also cause issues with the JSON parsing in the downstream file, as the JSON library may not be able to parse the string correctly if it contains invalid characters."
    ],
    "Criticality": 4,
    "Recommendations": [
        "Verify that the response from the POST request is encoded in a format that does not require explicit decoding, such as ASCII or UTF-8.",
        "Test the downstream file thoroughly to ensure that it can handle the changed response format.",
        "Consider adding error handling to the downstream file to catch any potential parsing errors."
    ]
}
```

Note: The criticality rating of 4 indicates that this change may cause significant issues in the downstream file, and should be thoroughly tested and reviewed before deployment.
The above is the summary of a code change and given below are some test cases. You will have to analyze the summary of the code changes and figure out if any of the test cases below require review OR will be impacted by the code changes.

Basic testing multiple screens	1	
Basic testing multiple screens	2	
Basic testing multiple screens	3	
Basic testing multiple screens	4	
Basic testing multiple screens	5	
Basic testing multiple screens	6	
Basic testing multiple screens	7	
Basic testing multiple screens	8	
Basic testing multiple screens	9	
Basic testing multiple screens	10	
Basic testing multiple screens	11		>>After click on the login button the my document page should be opened 
Basic testing multiple screens	12	
Basic testing multiple screens	13		>>Correct Email id>> A mail should be come on registered Email ID after click on the Send reset link and a notification popup should be shown up 
Basic testing multiple screens	14	
Basic testing multiple screens	15		>>After click on click here hyper link , which is sent on registered email id, Password reset screen should be open
Basic testing multiple screens	16	
Basic testing multiple screens	17		>>After click on reset password , The password should be reset
Basic testing multiple screens	18		>>Indexer able to see two tabs - API Uploads,  My Documents
Basic testing multiple screens	19		>>Supervisor able to see 4 tabs - API Uploads,  My Documents, QC Documents, Process flow 
Basic testing multiple screens	20		>>Admin able to see tabs - API Uploads,  My Documents, QC Documents,OCR Fail, API Logs, OCR failure & Process Flow, Split file detail
Basic testing multiple screens	21	
Basic testing multiple screens	22		>>Able to see all batches uploaded for the team to which the indexer belongs
Basic testing multiple screens	23		>>Batch list - Able to see all batches uploaded for the team to which the Supervisor belongs
Basic testing multiple screens	24	
Basic testing multiple screens	25	
Basic testing multiple screens	26		>>Check with after refresh LIFO sequence should not be change 
Basic testing multiple screens	27		>>After uploading new file, LIFO sequence should not be change 
Basic testing multiple screens	28		>>Batch, Customer ID, External batch ID, Files Uploaded, OCR successfull, OCR failed, Uploaded on, OCR status should be shown
Basic testing multiple screens	29		>>Total no of uploaded file count should be shown under file uploaded column
Basic testing multiple screens	30		>>Total no of OCR Successfull file count should be shown under OCR Successfull column
Basic testing multiple screens	31		>>Total no of OCR Failed file count should be shown under OCR Failed column
Basic testing multiple screens	32		>> Click on Up and down arrow, user can change the order like assending or Dessanding 
Basic testing multiple screens	33		>>User can search the file by external batch id, batch id and customer id
Basic testing multiple screens	34	
Basic testing multiple screens	35	
Basic testing multiple screens	36	
Basic testing multiple screens	37		>>If duing the processing if admin is delete the file then OCR processing should be stopped and The color of that batch should turn gray.
Basic testing multiple screens	38		>>if user hover on the delete batch then a notification should be appear "All file in this batch have been deleted"
Basic testing multiple screens	39	
Basic testing multiple screens	40	
Basic testing multiple screens	41		>>User can select the column according to his preferance through of toggle button.
Basic testing multiple screens	42		>>The time it will take for a batch to process is being displayed
Basic testing multiple screens	43	
Basic testing multiple screens	44		>>Check with after refresh FIFO sequence should not be change 
Basic testing multiple screens	45		>>After uploading new file, FIFO sequence should not be change 
Basic testing multiple screens	46		>>Batch, Customer ID, External batch ID, Files Uploaded, OCR successfull, OCR failed, Uploaded on, OCR status should be shown
Basic testing multiple screens	47		>>Total no of uploaded file count should be shown under file uploaded column
Basic testing multiple screens	48		>>Total no of OCR Successfull file count should be shown under OCR Successfull column
Basic testing multiple screens	49		>>Total no of OCR Failed file count should be shown under OCR Failed column
Basic testing multiple screens	50		>> Click on Up and down arrow, user can change the order like assending or Dessanding 
Basic testing multiple screens	51		>>User can search the file by external batch id, batch id and customer id
Basic testing multiple screens	52	
Basic testing multiple screens	53		>>Files that were deleted during processing should not appear in the "Batch Review Pending" section.
Basic testing multiple screens	54		>>User can able to see total review pending batch count 
Basic testing multiple screens	55	
Basic testing multiple screens	56		>>User can select the column according to his preferance through of toggle button.
Basic testing multiple screens	57		>>The time it will take for a batch to process is being displayed
Basic testing multiple screens	58	
Basic testing multiple screens	59		>>Check with after refresh LIFO sequence should not be change 
Basic testing multiple screens	60		>>After uploading new file, LIFO sequence should not be change 
Basic testing multiple screens	61		>>Batch, Customer ID, External batch ID, Files Uploaded, OCR successfull, OCR failed, Uploaded on, OCR status should be shown
Basic testing multiple screens	62		>>Total no of uploaded file count should be shown under file uploaded column
Basic testing multiple screens	63		>>Total no of OCR Successfull file count should be shown under OCR Successfull column
Basic testing multiple screens	64		>>Total no of OCR Failed file count should be shown under OCR Failed column
Basic testing multiple screens	65		>> Click on Up and down arrow, user can change the order like assending or Dessanding 
Basic testing multiple screens	66		>>User can search the file by external batch id, batch id and customer id
Basic testing multiple screens	67	
Basic testing multiple screens	68		>>Files that were deleted during processing should not appear in the "Batch Review Completed" section.
Basic testing multiple screens	69		>>User can able to see total review completed batch count 
Basic testing multiple screens	70	
Basic testing multiple screens	71		>>User can select the column according to his preferance through of toggle button.
Basic testing multiple screens	72		>>The time it will take for a batch to process is being displayed
Basic testing multiple screens	73		>>If I click on any batch from the API upload screen, the name of that batch should automatically appear in the search bar of the My Documents _Review Pending screen, and the documents should be filtered based on that batch.
Basic testing multiple screens	74		>>When searching for the "Delete Batch" name in the My Documents screen, no files should be displayed.
Basic testing multiple screens	75		>>User can select the column according to his preferance through of toggle button.
Basic testing multiple screens	76		>>User can able to see total review PND batch count 
Basic testing multiple screens	77	
Basic testing multiple screens	78	
Basic testing multiple screens	79	
Basic testing multiple screens	80	
Basic testing multiple screens	81	
Basic testing multiple screens	82	
Basic testing multiple screens	83		>>All the files inside that batch should be visible on that screen.
Basic testing multiple screens	84		>>The first page of the first file should be selected by default, and on the right-hand side of the screen, there should be a maximized view of that file.
Basic testing multiple screens	85	
Basic testing multiple screens	86	
Basic testing multiple screens	87		>>After clicking the reset button once, the document should reset, removing any zoom in or out adjustments.
Basic testing multiple screens	88	
Basic testing multiple screens	89	
Basic testing multiple screens	90		>>Batch name/ Total files/ Total Pages count should be shown on the top right hand side 
Basic testing multiple screens	91		
>>Documents should be displayed according to the file within the batch, and the total number of pages in a file should also be shown above the file's box.
Basic testing multiple screens	92	
Basic testing multiple screens	93		>>If there are three documents within a single file, the first document should be highlighted in blue, the second in orange, and the third in black.
Basic testing multiple screens	94		>>If I move any document into another file set, the moved file should be automatically displayed as a split until it is manually merged>> Submit
Basic testing multiple screens	95	
Basic testing multiple screens	96		NOTE: All the split combination use cases is continue in "Split combination use case" tab
Basic testing multiple screens	97		>>On right-clicking the mouse, a dropdown menu should appear with the option to Mark as SD>> Submit
Basic testing multiple screens	98		>>The "Mark as SD" option should appear directly on top of an invoice.
Basic testing multiple screens	99		>>After clicking on "Mark as SD," the invoice should be converted into SD.
Basic testing multiple screens	100		>>On right-clicking the mouse, a dropdown menu should appear with the option to Rotate.
Basic testing multiple screens	101	
Basic testing multiple screens	102	
Basic testing multiple screens	103		>> Rotation has to be done manually, auto rotation from AI will not happen.
Basic testing multiple screens	104		 >> Rotated file would be downloaded as per the changes made by user.
Basic testing multiple screens	105		>>Rotated file will have extraction and snippet support on Extraction Screen.
Basic testing multiple screens	106		>>Rotated file will have extraction and snippet support on Extraction Screen.
Basic testing multiple screens	107		>>On right-clicking the mouse, a dropdown menu should appear with the option to Move page on another file>> submit 
Basic testing multiple screens	108		>>If there are multiple pages within a file, any page of that file can be moved to another file set in the classification screen.
Basic testing multiple screens	109		>> Move page on another file option should be appeared any type of document 
Basic testing multiple screens	110		>>On right-clicking the mouse, a dropdown menu should appear with the option to Move Document  on another file 
Basic testing multiple screens	111		>>  Move Document  on another file option should be appeared any type of document 
Basic testing multiple screens	112		>>If a document within a file contains multiple pages, and the user wants to move the entire document to another file, they can use this option. It's important to note that all pages of that document should go into the file you are selecting.
Basic testing multiple screens	113		>>To move a file's page or the entire document, if the user clicks on any of these options, a pop-up should appear, allowing the user to choose the file into which they want to move the content. In this use case, ensure that all options of your files are visible in the dropdown, but the file from which you are moving the document or page should appear grayed out.







Basic testing multiple screens	114		>>On right-clicking the mouse, a dropdown menu should appear with the option to Orient Image 
Basic testing multiple screens	115		>>If an image is tillted on an angle ranging from -10 to +10 the ai will orient the image 
Basic testing multiple screens	116	
Basic testing multiple screens	117		>>On right-clicking the mouse, a dropdown menu should appear with the option to DeOrient Image 
Basic testing multiple screens	118	
Basic testing multiple screens	119		>>On right-clicking the mouse, a dropdown menu should appear with the option to Reprocess Images and user can see 4 type of strategy (A/ B/ C/ Z) 
Basic testing multiple screens	120		>>This feature will change the resolution to detact the batter quality of the image 
Basic testing multiple screens	121		>>On right-clicking the mouse, a dropdown menu should appear with the option to mark preceding pages as INV
Basic testing multiple screens	122		>>This option will show when the document is multiple pages 
Basic testing multiple screens	123		>>all the previous pages of the file will get converted into INV
Basic testing multiple screens	124		>>On right-clicking the mouse, a dropdown menu should appear with the option to mark Subsequent pages as INV
Basic testing multiple screens	125		>>This option will show when the document is multiple pages 
Basic testing multiple screens	126		>>all the next pages of the file will get converted into INV
Basic testing multiple screens	127		>>On right-clicking the mouse, a dropdown menu should appear with the option to mark preceding pages as SD
Basic testing multiple screens	128		>>This option will show when the document is multiple pages 
Basic testing multiple screens	129		>>all the previous pages of the file will get converted into SD
Basic testing multiple screens	130		>>On right-clicking the mouse, a dropdown menu should appear with the option to mark Subsequent pages as SD
Basic testing multiple screens	131		>>This option will show when the document is multiple pages 
Basic testing multiple screens	132		>>all the next pages of the file will get converted into SD
Basic testing multiple screens	133		>>After making any changes in the classification screen, if I click the reset button, all changes made by the user should be refreshed.
Basic testing multiple screens	134	
Basic testing multiple screens	135		>>After click on submit button , the changes should be visible which is done from Indexer on classification screen 
Basic testing multiple screens	136		>>Submit classification review without making any changes, all the flow should be going fine 
Basic testing multiple screens	137		>>Submit classification review making  changes which is given in the "Split combination use case" sheet, all the flow should be going fine 
Sanity testing usecase_Prod	1	
Sanity testing usecase_Prod	2	
Sanity testing usecase_Prod	3	
Sanity testing usecase_Prod	4		Indexer able to see two tabs - API Uploads, My Documents
Sanity testing usecase_Prod	5		Batch list - Able to see all batches uploaded for the team to which the indexer belongs
Sanity testing usecase_Prod	6		Batch Filter - User can filter the batches from his option
Sanity testing usecase_Prod	7		Rev Pending Files as per FIFO, other Rev completed and All Batches should be shown as per LIFO
Sanity testing usecase_Prod	8		Go to a batch and open a file under "Review Pending"
Sanity testing usecase_Prod	9	
Sanity testing usecase_Prod	10	
Sanity testing usecase_Prod	11	
Sanity testing usecase_Prod	12	
Sanity testing usecase_Prod	13		Rotation has to be done manually, auto rotation from AI will not happen.
Sanity testing usecase_Prod	14		Rotated file will have extraction and snippet support on Extraction Screen.
Sanity testing usecase_Prod	15		In case if failure or Extraction & snippet support, user will have to fill the extraction data himself but the output will be as per point 2. (Controlled testing with Abhijeet)
Sanity testing usecase_Prod	16		Draging, Movement of Pg's, Files, reordering on Classification Review Screen
Sanity testing usecase_Prod	17		Hard reset: Will come from Extraction Screen>Class Review Screen and will reset the changes made to the original AI classifcation (file uploded).
1) all files in extraction
2) few files in review completed
3) few files in all 3 headers
and so on On clicking Hard Reset, all files of the batch123 (under any tab) will all move to Rev pending and come to original state. The Batch123 filter will reflect and the user will be able to see that Batch/file in Rev Pending.
eg if Sup QC>Class Rev screen>Hard reset... the batch will move to Indexer Rev Pending in My Docs Screen and the user will be auto navigated there.
eg: if admin del the file/doc from UI.. Hard reset will get the same back from Rev Class screen and files will move to Rev Pending
Sanity testing usecase_Prod	18	
Sanity testing usecase_Prod	19		In a Batch, multiple files, Do changes in Rev classification (split/merge/rotate/movement) in few files/with all files and submit.
Sanity testing usecase_Prod	20		Step 1: Classification Review by NON INDEXER, Step 2: Extraction Review ONLY BY INDEXER, Step 3: Sup QC. Test use cases with role's combo for a Batch
Sanity testing usecase_Prod	21	
Sanity testing usecase_Prod	22		If in a batch there are 10 files, file 1, 2,3,4 have been worked upon, and 5th File is modified via Rev Clas then only 5th file will go for OCR and the 4 files which hav been worked will remain the same (reset will not happen). Reset will not happen to File 6,7,8,9,10 also
Sanity testing usecase_Prod	23	
Sanity testing usecase_Prod	24		Go to a batch and open a file under "Classification Reviewed"
Sanity testing usecase_Prod	25		Extraction screen : Editing mode, Extraction contouring check, Page 1, 2,3 navigation check
Sanity testing usecase_Prod	26	
Sanity testing usecase_Prod	27		Check if OCR extracted values are correct and document type is correct
Sanity testing usecase_Prod	28	
Sanity testing usecase_Prod	29	
Sanity testing usecase_Prod	30	
Sanity testing usecase_Prod	31		Click on "Review Classification" button and submit classification again and check whole flow as above
Sanity testing usecase_Prod	32		Change values through contouring and manually typing in the fields, Check field for Validations
Sanity testing usecase_Prod	33		Submit KV extraction. Make sure that the final output is matching with the editing done.
Sanity testing usecase_Prod	34	
Sanity testing usecase_Prod	35		Check if a partially reviewed document has gone through classification review for the second time, the reviewed status should be reset
Sanity testing usecase_Prod	36		Go to a batch and open a file under "Review Completed"
Sanity testing usecase_Prod	37	
Sanity testing usecase_Prod	38	
Sanity testing usecase_Prod	39	
Sanity testing usecase_Prod	40	
Sanity testing usecase_Prod	41	
Sanity testing usecase_Prod	42		The next file which is not locked (being worked upon) in the batch should open, the docs on left side (thumbnails) should match with the middle image and KV extraction on the rt.
Sanity testing usecase_Prod	43		Vendor missing in the list if changed by Indexer and submitted. The same changed vendor should get submitted by Supervisor
Sanity testing usecase_Prod	44	
Sanity testing usecase_Prod	45	
Sanity testing usecase_Prod	46		Supervisor able to see 4 tabs - API Uploads, My Documents, QC Documents, Vendor Mgmt
Sanity testing usecase_Prod	47		Batch list - Able to see all batches uploaded for the team to which the Supervisor belongs
Sanity testing usecase_Prod	48		Batch Filter - User can filter the batches from his option
Sanity testing usecase_Prod	49		QC Documents - Able to see all BATCHES assigned to him based on the review percentage mentioned in team
Sanity testing usecase_Prod	50		Rev Pending Files as per FIFO, other Rev completed and All Batches should be shown as per LIFO
Sanity testing usecase_Prod	51	
Sanity testing usecase_Prod	52	
Sanity testing usecase_Prod	53	
Sanity testing usecase_Prod	54	
Sanity testing usecase_Prod	55	
Sanity testing usecase_Prod	56		Rotation has to be done manually, auto rotation from AI will not happen.
Sanity testing usecase_Prod	57		Rotated file will have extraction and snippet support on Extraction Screen.
Sanity testing usecase_Prod	58		In case if failure or Extraction & snippet support, user will have to fill the extraction data himself but the output will be as per point 2. (Controlled testing with Abhijeet)
Sanity testing usecase_Prod	59		Draging, Movement of Pg's, Files, reordering on Classification Review Screen
Sanity testing usecase_Prod	60		Hard reset: Will come from Extraction Screen>Class Review Screen and will reset the changes made to the original AI classifcation (file uploded).
1) all files in extraction
2) few files in review completed
3) few files in all 3 headers
and so on On clicking Hard Reset, all files of the batch123 (under any tab) will all move to Rev pending and come to original state. The Batch123 filter will reflect and the user will be able to see that Batch/file in Rev Pending.
eg if Sup QC>Class Rev screen>Hard reset... the batch will move to Indexer Rev Pending in My Docs Screen and the user will be auto navigated there.
Sanity testing usecase_Prod	61	
Sanity testing usecase_Prod	62		Move : From one file to another with the changes made
Sanity testing usecase_Prod	63	
Sanity testing usecase_Prod	64	
Sanity testing usecase_Prod	65	
Sanity testing usecase_Prod	66	
Sanity testing usecase_Prod	67	
Sanity testing usecase_Prod	68		Check if OCR extracted values are correct and document type is correct
Sanity testing usecase_Prod	69	
Sanity testing usecase_Prod	70	
Sanity testing usecase_Prod	71	
Sanity testing usecase_Prod	72		Click on "Review Classification" button and submit classification again and check whole flow as above
Sanity testing usecase_Prod	73		Change values through contouring and manually typing in the fields, Check field for Validations
Sanity testing usecase_Prod	74	
Sanity testing usecase_Prod	75		Once all the file submitted, file should move to "Review Completed"
Sanity testing usecase_Prod	76		Check if a partially reviewed document has gone through classification review for the second time, the reviewed status should be reset
Sanity testing usecase_Prod	77	
Sanity testing usecase_Prod	78	
Sanity testing usecase_Prod	79	
Sanity testing usecase_Prod	80	
Sanity testing usecase_Prod	81		Review completed files: Input PDF (post splitting/merging) Vs Output should be same once downloaded.
Sanity testing usecase_Prod	82	
Sanity testing usecase_Prod	83	
Sanity testing usecase_Prod	84	
Sanity testing usecase_Prod	85	
Sanity testing usecase_Prod	86	
Sanity testing usecase_Prod	87	
Sanity testing usecase_Prod	88		Same flow as indexer when supervisor chooses to act as indexer
Sanity testing usecase_Prod	89		Make sure the input/changes in Classification & Splitting and extraction data and reflection as Output (while exporting once the file is Review Completed)
Sanity testing usecase_Prod	90		Vendor missing in the list if changed by Indexer and submitted. The same changed vendor should get submitted by Supervisor
Sanity testing usecase_Prod	91	
Sanity testing usecase_Prod	92	
Sanity testing usecase_Prod	93		Admin able to see tabs - API Uploads, My Documents, QC Documents,OCR Fail, API Logs, Process Flow
Sanity testing usecase_Prod	94		Batch list - Able to see all batches uploaded for the team to which the indexer belongs
Sanity testing usecase_Prod	95		Batch Filter - User can filter the batches from his option
Sanity testing usecase_Prod	96		Under Process flow -> should be able to see Vendor Mgmt, Vendor mgmt-Bol, API Settings, User Mgmt, Team Mgmt, Report Generation
Sanity testing usecase_Prod	97	
Sanity testing usecase_Prod	98	
Sanity testing usecase_Prod	99		My Docs, API Upload Sup QC screen should work for Admin also as per the above pointers
Sanity testing usecase_Prod	100	
Sanity testing usecase_Prod	101	
Sanity testing usecase_Prod	102		Batch list - Able to see all batches uploaded for the team to which the indexer belongs
Sanity testing usecase_Prod	103		QC Documents - Able to see all BATCHES assigned to him based on the review percentage mentioned in team
Sanity testing usecase_Prod	104		Rev Pending Files as per FIFO, other Rev completed and All Batches should be shown as per LIFO
Sanity testing usecase_Prod	105	
Sanity testing usecase_Prod	106	
Sanity testing usecase_Prod	107		Go to a batch and open a file under "Review Pending"
Sanity testing usecase_Prod	108	
Sanity testing usecase_Prod	109	
Sanity testing usecase_Prod	110	
Sanity testing usecase_Prod	111		Rotation has to be done manually, auto rotation from AI will not happen.
Sanity testing usecase_Prod	112		Rotated file will have extraction and snippet support on Extraction Screen.
Sanity testing usecase_Prod	113		In case if failure or Extraction & snippet support, user will have to fill the extraction data himself but the output will be as per point 2. (Controlled testing with Abhijeet)
Sanity testing usecase_Prod	114		Draging, Movement of Pg's, Files, reordering on Classification Review Screen
Sanity testing usecase_Prod	115	
Sanity testing usecase_Prod	116	
Sanity testing usecase_Prod	117	
Sanity testing usecase_Prod	118	
Sanity testing usecase_Prod	119	
Sanity testing usecase_Prod	120	
Sanity testing usecase_Prod	121		Check if OCR extracted values are correct and document type is correct
Sanity testing usecase_Prod	122	
Sanity testing usecase_Prod	123	
Sanity testing usecase_Prod	124	
Sanity testing usecase_Prod	125		Click on "Review Classification" button and submit classification again and check whole flow as above
Sanity testing usecase_Prod	126		Change values through contouring and manually typing in the fields, Check field for Validations
Sanity testing usecase_Prod	127	
Sanity testing usecase_Prod	128	
Sanity testing usecase_Prod	129		Check if a partially reviewed document has gone through classification review for the second time, the reviewed status should be reset
Sanity testing usecase_Prod	130	
Sanity testing usecase_Prod	131	
Sanity testing usecase_Prod	132	
Sanity testing usecase_Prod	133	
Sanity testing usecase_Prod	134	
Sanity testing usecase_Prod	135	
Sanity testing usecase_Prod	136	
Sanity testing usecase_Prod	137	
Sanity testing usecase_Prod	138		Make sure the input/changes in Classification & Splitting and extraction data and reflection as Output (while exporting once the file is Review Completed)
Sanity testing usecase_Prod	139	
Sanity testing usecase_Prod	140	
Sanity testing usecase_Prod	141	
Sanity testing usecase_Prod	142	
Sanity testing usecase_Prod	143	
Sanity testing usecase_Prod	144	
Sanity testing usecase_Prod	145		Hard reset: Will come from Extraction Screen>Class Review Screen and will reset the changes made to the original AI classifcation (file uploded).
1) all files in extraction
2) few files in review completed
3) few files in all 3 headers
and so on On clicking Hard Reset, all files of the batch123 (under any tab) will all move to Rev pending and come to original state. The Batch123 filter will reflect and the user will be able to see that Batch/file in Rev Pending.
eg if Sup QC>Class Rev screen>Hard reset... the batch will move to Indexer Rev Pending in My Docs Screen and the user will be auto navigated there.
Sanity testing usecase_Prod	146		Failure Handling : Controlled testing when AI APIs are failed on purpose (OCR, KVP, Snippet, Rotate, PDF Merger)
Contact Abhijeet for further details
Sanity testing usecase_Prod	147		Vendor missing in the list if changed by Indexer and submitted. The same changed vendor should get submitted by Supervisor
Sanity testing usecase_Prod	148	
Sanity testing usecase_Prod	149	
Sanity testing usecase_Prod	150		Draw the table bound:Add a table if not already present/extracted from the document
Sanity testing usecase_Prod	151		Delete the Rows & Columns:Use control and click to delete row/column
Sanity testing usecase_Prod	152		Extract the data:When creating a new table, it is mandatory to define at least 2 horizontal lines ( one full row) to be able to extract data - Table completion API
When a table is already present/extracted, you're going to use AUTO COMPLETION API
Sanity testing usecase_Prod	153		Contouring :Select a table cell to switch context from table to the selected cell - and call the snippet API by moving the contour and resizing it
Sanity testing usecase_Prod	154		Delete the table spanning across multiple pages:We can delete the whole table spanning across the multiple pages
Sanity testing usecase_Prod	155		Extracting the text from description:We can extract the desired text from the description using the contour
Sanity testing usecase_Prod	156		Dropdown against every extracted columns:Column mapping to the local columns extracted from the table itself - filling values of a column using the metadata which present from the local columns
Sanity testing usecase_Prod	157		Detect the table:It is a function which allows to detect the table in the document
Sanity testing usecase_Prod	158		Full Width:To expand the extracted column view in the horizontal view
Sanity testing usecase_Prod	159		Delete the table:This function will allow to delete the table
Sanity testing usecase_Prod	160		Highlighted value in the dropdown :Once we select the local columns, it by default highlights the column for which values are fetched in that column from the document
Sanity testing usecase_Prod	161		Clear the data :It will delete the fetched data in the respective column
Sanity testing usecase_Prod	162		Table always to be continuous:There should not be discontinuity between the pages of the table
Sanity testing usecase_Prod	163		Use Shift only to specific page:Pressing shift while dealing with column movement will allow the user to make this change reflect across all pages which have that particular column
Sanity testing usecase_Prod	164		Add to all the pages using add table:Adding a new table for a multipage document will ask you to select a range in which you want the table to be present
Sanity testing usecase_Prod	165	
Sanity testing usecase_Prod	166		Autofill:Yet to be developed- While we type partial text in the extracted filed, it should autofill the predicted value/text
Sanity testing usecase_Prod	167		Split Columns:If we observe that content of 2 columns are merged in one column, using this feature, we can seperate the columns and contents
Sanity testing usecase_Prod	168		Reset:It will reset the table to the default as it was in the first place
Sanity testing usecase_Prod	169		Autofill:This will allow the user to type few text and then autofill the rest of the identified words from the contoured area
Sanity testing usecase_Prod	170		Addition of H & L in Vendor details:H denotes as Header only and L denotes both Header and Tabular to be available for the indexers to work on
Sanity testing usecase_Prod	171	
Combination	1	
Combination	2		-----Single page/ two page/ 5page/ tilted single page
-----Classification:
1 page: INV TO SD
2 pages: SAME INV apply A and Z strategy
5 pages: 2 doc merge and 2 merged doc Separated and make 2 doc and 5 doc move into 2 pager file and rotate
Tilted file: apply Orient feature
---Submitted
Review screen
1pager direct submit
2pager changes below
Manually edit. Apply contouring, add text, delete value, select value on page 2 
Table changes
Vendor change H TO L
Edit table
Use features
Add row and column, select drop-down, use auto fill, clear space,
Manually edit. Apply contouring, add text, delete value, select value, clear data, use validation submit
Review completed
Check all the changes done on review pending screen, that is visible on the review completed screen
Combination	3		----Single page/ two page/ 5page/ tilted single page
-----Classification:
1 page: INV TO SD
2 pages: SAME INV apply A and Z strategy
5 pages: 2 doc merge and 2 merged doc Separated and make 2 doc and 5 doc move into 2 pager file and rotate
Tilted file: apply Orient feature
---Submitted
Review screen
1pager direct submit
2pager changes below
Manually edit. Apply contouring, add text, delete value, select value on page 2 
Table changes
Vendor change H TO L
Add table
Use features
Draw the table and extract the value, select drop-down, use auto fill, clear space,
Manually edit. Apply contouring, add text, delete value, select value, clear data, use validation submit
Review completed
Check all the changes done on review pending screen, that is visible on the review completed screen
Combination	4		-----Single page/ two page/ 5page/ tilted single page
-----Classification:
1 page: INV TO SD
2 pages: SAME INV apply A and Z strategy
5 pages: 2 doc merge and 2 merged doc Separated and make 2 doc and 5 doc move into 2 pager file and rotate
Tilted file: apply Orient feature
---Submitted
Review screen
1pager direct submit
2pager changes below
Manually edit. Apply contouring, add text, delete value, select value on page 2 
Table changes
Vendor change H TO L

Use features
Deleted the extracted table and add new table
, select drop-down, use auto fill, clear space,
Manually edit. Apply contouring, add text, delete value, select value, clear data, use validation submit
Review completed
Check all the changes done on review pending screen, that is visible on the review completed screen
Combination	5		-----Single page/ two page/ 5page/ tilted single page
-----Classification:
1 page: INV TO SD
2 pages: SAME INV apply A and Z strategy
5 pages: 2 doc merge and 2 merged doc Separated and make 2 doc and 5 doc move into 2 pager file and rotate
Tilted file: apply Orient feature
---Submitted
Review screen
1pager direct submit
2pager changes below
Manually edit. Apply contouring, add text, delete value, select value on page 2 
Table changes
Vendor change H TO L

Use features
Deleted the extracted table and reset table
, select drop-down, use auto fill, clear space,
Manually edit. Apply contouring, add text, delete value, select value, clear data, use validation submit
Review completed
Check all the changes done on review pending screen, that is visible on the review completed screen
Combination	6		-----Single page/ two page/ 5page/ tilted single page
-----Classification:
1 page: INV TO SD
2 pages: SAME INV apply A and Z strategy
5 pages: 2 doc merge and 2 merged doc Separated and make 2 doc and 5 doc move into 2 pager file and rotate
Tilted file: apply Orient feature
---Submitted
Review screen
1pager direct submit
2pager changes below
Manually edit. Apply contouring, add text, delete value, select value on page 2 
Table changes
Vendor change L TO L
Edit table
Use features
Add row and column, select drop-down, use auto fill, clear space,
Manually edit. Apply contouring, add text, delete value, select value, clear data, use validation submit
Review completed
Check all the changes done on review pending screen, that is visible on the review completed screen
Combination	7		-----Single page/ two page/ 5page/ tilted single page
-----Classification:
1 page: INV TO SD
2 pages: SAME INV apply A and Z strategy
5 pages: 2 doc merge and 2 merged doc Separated and make 2 doc and 5 doc move into 2 pager file and rotate
Tilted file: apply Orient feature
---Submitted
Review screen
1pager direct submit
2pager changes below
Manually edit. Apply contouring, add text, delete value, select value on page 2 
Table changes
Vendor change L TO L
Add table
Use features
Draw the table and extract the value, select drop-down, use auto fill, clear space,
Manually edit. Apply contouring, add text, delete value, select value, clear data, use validation submit
Review completed
Check all the changes done on review pending screen, that is visible on the review completed screen
Combination	8		-----Single page/ two page/ 5page/ tilted single page
-----Classification:
1 page: INV TO SD
2 pages: SAME INV apply A and Z strategy
5 pages: 2 doc merge and 2 merged doc Separated and make 2 doc and 5 doc move into 2 pager file and rotate
Tilted file: apply Orient feature
---Submitted
Review screen
1pager direct submit
2pager changes below
Manually edit. Apply contouring, add text, delete value, select value on page 2 
Table changes
Vendor change L TO L

Use features
Deleted the extracted table and add new table
, select drop-down, use auto fill, clear space,
Manually edit. Apply contouring, add text, delete value, select value, clear data, use validation submit
Review completed
Check all the changes done on review pending screen, that is visible on the review completed screen
Checking -> docx ['xls', 'xlsx']
Checking -> docx ['docx']
Checking -> docx ['docx']
Test-File-> 1661 
 /datadrive/IKG/test_db/test_plans/BRD-Project Mark.docx
executeInstruction => 
    for fnm, sheets in js_.items():
        for sheetname, txt in sheets.items():
            cnt_ += 1
            emb_ = createJsonFeats.returnEmbed( txt )
            dd_ = { 'text': txt, 'docSignature': emb_, 'docID': cnt_ }

            db_utils.insertNewSignature( dd_ )

The above is the summary of a code change and given below are some test cases. You will have to analyze the summary of the code changes and figure out if any of the test cases below require review OR will be impacted by the code changes.

 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Contents
 
1.   	Introduction
2.   	Purpose of the Document
3.   	Scope of Work
4.   	Reports and Accuracy Logics
5.   	Billing Logic
6.   	Technical Infrastructure Requirement Details [PROD & UAT]
 
 
 
 
 
 
 
 
1.	Introduction
The main purpose of this project is to provide an automated invoice processing system “IDP (Intelligent Document Processing)” to ensure smooth, quick and efﬁcient services to REQUORDIT by AmyGB.ai. As compared to other mechanism of invoice processing, IDP will ensure that the requests, which falls under the scope can be addressed within the agreed and definite TAT committed to the customer. This will help the REQUORDIT team to reduce the TAT for document processing and have a higher productivity in their day to day operations.
 
 
 
 
 
2.	Purpose of the Document
The purpose of the document is to list down and document each and every necessary steps like Current Flow, Proposed Flow, Scope of the Project, Technical Infra Details and Report Mechanism.
This Document is prepared on the basis of the discussions carried out by various discussions between REQUORDIT and AmyGB Team.
 
 
 
 
 
 
 
 
 
 
3.    Requirement Scope
An application to be developed by AmyGB to Automate the invoices wherein After the invoice is uploaded via API, Application will carry out the OCR, extract the defined key values and get the review done by the indexers and post review, make the file available to download in the PDF/tiff format.
 
 
An avg. of 8000 documents will get processed/day except month end wherein this may reach up to 20000 (last couple of days of the month). Each doc will contain an average of 5 pages. 80% of the docs for the day are uploaded between 6AM -8AM CDT which to be processed by noon and output will be downloaded by 6 PM CDT. Processing shall happen on AWS - on cloud. Extracted data is deleted (OCR- text + image) post 23 Hrs(From the time file is downloaded) and retain the meta data (which include the kvp, line items, doc class etc.)
 
 
 
 
Below are the discussed scoped items to be delivered in the phase wise manner-
 
1.	Classification/Re-Classification
2.	Extraction & Review
3.	Supervisor QC model
4.	User & Team Management
5.	PDF & TIFF Download options
6.	Rotation of a file
7.	File merging and Re-ordering
8.     File lock and release 
9.	Tabular Content Extraction + Draw Table Bounds only for tabular failures
10.  Tabular OCR Review with merging/splitting of rows on UI
11.  Real time generation of reports of various doc lifecycle
12.  Feedback Module
 
 
 
 
 
 
 
 
 
The following is the High Level process flow-
 
 
 
 
 
 
 
4.	Scope of Work
The Scope of work for the project will be divided into three parts:
I.          Phase 1 Scope
II.        Phase 2 Scope
III.  	Phase 3 Scope
 
 
 
 
 
I.         Phase 1 Scope:
   Features to be considered for Phase 1 are as follows based on the discussions with the REQUORDIT team.
 
1.     Classification & Extraction Review
2.     Supervisor QC model
3.     User & Team Management
4.     PDF & TIFF Download options
5.     Rotation of a file
6.     File merging and Re-ordering
7. 	File lock and release 
8.  	Real time generation of reports
 
 
   	Phase 1 will consist scenarios related to the above categories described as below
l  Classification/Re-Classification & Extraction Review-
A document should go through the OCR, and get classified as per the standard Invoice Or Supporting Document. Post Classification, if we realize there was error in classification, it should have option to re-classify and submit classification. Once classification is done, 16 defined key values with mandatory and optional fields to be extracted and reviewed by the indexers.
 
l  Supervisor QC model-
Also review can be done by the supervisor and admin as part of Supervisor QC review process wherein set % of the particular customer id based on the team it belongs to, files will flow to mapped supervisor for the QC
 
l  User & Team Management-
This will have 3 roles namely as 1) Indexer 2) Supervisor 3) Admin
Sup and Admin role will be allowed the addition, update and mapping of the users to customer IDs and supervisors
 
l  PDF & TIFF Download options-
Once the review is completed and submitted from both indexers and supervisors (in 100% QC model), file should be available to be downloaded as PDF/Tiff for the end customer
 
 
l  Rotation of a file-
If a document is skewed as an input, while performing the classification, it should rotate the image to 90 Degree or more and same should get downloaded as an output post completion of the Classification and Extraction
 
l  File merging and Re-ordering-
If an invoice is available in file1 out of multiple files in the same batch and Supporting Document of the respective invoice is available in file2, we should have option to merge the SD from File2 with Invoice in file1 and vice versa. This should have option of re-ordering of the page count with respect to the changes
 
l  File lock and release-
This feature will function as, once a file is opened by any indexer, it will be locked for 30 minutes and no one else can work on that file. Also, if an indexer opens multiple tabs or try to work on multiple files together, it will release the older files and allow to work on the latest file only to avoid the ambiguity 
 
l  Real time generation of reports of various doc lifecycle-
There should be an option available which can be used to generate    the reports on the real time for the document processing stats from Admin panel
 
 
Note- After the Go Live, the timeline for the Phase 1 hyper-care stage is 75 days where the AmyGB team will analyze the accuracy on the area of opportunities and fix the bugs comes on the way
 
 
II.      Phase 2 Scope:
Features to be considered for Phase 2 are as follows based on the discussions with the REQUORDIT team.
 
1.  	Table extraction spread across multiple pages within same/different files
2.  	Draw table bounds if table missed or incomplete table identified
3.  	Addition of rows at end of the table (new rows or missing row)
4.  	Merge/split/editing of rows of the table
5.  	QC of the extracted tabular data/line items with option to pick values via snippet
   	Phase 2 will consist scenarios related to the above categories described as below
l  Table extraction spread across multiple pages within same/different files-
This feature should allow the data to be extracted from the tabular invoices across multiple pages within same/different files
 
l  Draw table bounds if table is missed or incomplete -
Draw table bounds if table is missed or incomplete table identification for reprocessing via OCR
 
l  Addition of rows-
This feature will allow to add of rows at end of the table (new rows or missing row)
 
l  Merge/split/editing of rows of the table-
This feature should allow to merge/split/Edit the table rows where it is needed
 
l  QC of the extracted tabular data/line items -
This feature should allow to do the QC of the extracted tabular data/line items with option to pick values via snippet
 
 
Note- After the Go Live, the timeline for the Phase 2 to start is after 75 days, the requirements for the Phase 2 will commence.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
III.   Phase 3 Scope:
Features to be considered for Phase 3 are as follows based on the discussions with the REQUORDIT team.
 
1.     Feedback Module
 
 
   	Phase 3 will consist scenarios related to the above categories described as below
l  Feedback Module-
This feature should allow the application to accept the feedback bases the inputs from the users. This will allow Key - Value Feedback - Instead of manual corrections on all key value extraction give feedback on selected failed files. The USP of this feature is that it also retrains the doc extraction module so that similar errors are not repeated in future batches
 
 
5.	Reports and Accuracy Logics
This section will give a clarity on the Mechanism of Reports for Uploaded and Processed Data.
 
After we go live with the solution in place, we shall generate the reports for document life cycle which should have visibility on page level details of the Invoices and supporting documents for billing purpose.
 
l  Monthly Billing Reports: Monthly billing reports will be created for billing purpose. This report will contain all the invoices along with page detail processed by the IDP for a specific month. This report will be used by AmyGB for all the invoicing purpose
 
 
6.	Billing Logic
The billing will be based on the total number of pages of the Invoices processed by the application. Commercials are closed over other email thread.
 
 
 
 
 
7.	Technical Infrastructure Requirement Details. [PROD & UAT]
This section will list down all the technical level infrastructural details for Production Server that will be necessary for this project. The scope of the project will require technical infrastructure at the REQUORDIT UAT server and also at the Production server.
 
 
 
 
 
 
Infrastructure for UAT/Staging Server for UAT Testing:
After the training of the Application, the codes will be deployed on the UAT server of the REQUORDIT. It is important for the UAT server infrastructure to be similar to PROD server, as all the initial level testing will get carried out on the UAT server. UAT server should also have the similar API’s as the PROD server for the AmyGB to carry out the UAT level testing for the same.
 
Note- Any changes, bug fix or new requirements to be tested on Staging first before deploying it to Prod to have the no impact on the Prod environment
 
 
THANK YOU
 


Checking -> xlsx ['xls', 'xlsx']
Checking -> xlsx ['docx']
Checking -> xlsx ['xls', 'xlsx']
Test-File-> 5874 
 /datadrive/IKG/test_db/test_plans/IKG-testing.xlsx
executeInstruction => 
    for fnm, sheets in js_.items():
        for sheetname, txt in sheets.items():
            cnt_ += 1
            emb_ = createJsonFeats.returnEmbed( txt )
            dd_ = { 'text': txt, 'docSignature': emb_, 'docID': cnt_ }

            db_utils.insertNewSignature( dd_ )

The above is the summary of a code change and given below are some test cases. You will have to analyze the summary of the code changes and figure out if any of the test cases below require review OR will be impacted by the code changes.

Basic testing multiple screens	1	
Basic testing multiple screens	2	
Basic testing multiple screens	3	
Basic testing multiple screens	4	
Basic testing multiple screens	5	
Basic testing multiple screens	6	
Basic testing multiple screens	7	
Basic testing multiple screens	8	
Basic testing multiple screens	9	
Basic testing multiple screens	10	
Basic testing multiple screens	11		>>After click on the login button the my document page should be opened 
Basic testing multiple screens	12	
Basic testing multiple screens	13		>>Correct Email id>> A mail should be come on registered Email ID after click on the Send reset link and a notification popup should be shown up 
Basic testing multiple screens	14	
Basic testing multiple screens	15		>>After click on click here hyper link , which is sent on registered email id, Password reset screen should be open
Basic testing multiple screens	16	
Basic testing multiple screens	17		>>After click on reset password , The password should be reset
Basic testing multiple screens	18		>>Indexer able to see two tabs - API Uploads,  My Documents
Basic testing multiple screens	19		>>Supervisor able to see 4 tabs - API Uploads,  My Documents, QC Documents, Process flow 
Basic testing multiple screens	20		>>Admin able to see tabs - API Uploads,  My Documents, QC Documents,OCR Fail, API Logs, OCR failure & Process Flow, Split file detail
Basic testing multiple screens	21	
Basic testing multiple screens	22		>>Able to see all batches uploaded for the team to which the indexer belongs
Basic testing multiple screens	23		>>Batch list - Able to see all batches uploaded for the team to which the Supervisor belongs
Basic testing multiple screens	24	
Basic testing multiple screens	25	
Basic testing multiple screens	26		>>Check with after refresh LIFO sequence should not be change 
Basic testing multiple screens	27		>>After uploading new file, LIFO sequence should not be change 
Basic testing multiple screens	28		>>Batch, Customer ID, External batch ID, Files Uploaded, OCR successfull, OCR failed, Uploaded on, OCR status should be shown
Basic testing multiple screens	29		>>Total no of uploaded file count should be shown under file uploaded column
Basic testing multiple screens	30		>>Total no of OCR Successfull file count should be shown under OCR Successfull column
Basic testing multiple screens	31		>>Total no of OCR Failed file count should be shown under OCR Failed column
Basic testing multiple screens	32		>> Click on Up and down arrow, user can change the order like assending or Dessanding 
Basic testing multiple screens	33		>>User can search the file by external batch id, batch id and customer id
Basic testing multiple screens	34	
Basic testing multiple screens	35	
Basic testing multiple screens	36	
Basic testing multiple screens	37		>>If duing the processing if admin is delete the file then OCR processing should be stopped and The color of that batch should turn gray.
Basic testing multiple screens	38		>>if user hover on the delete batch then a notification should be appear "All file in this batch have been deleted"
Basic testing multiple screens	39	
Basic testing multiple screens	40	
Basic testing multiple screens	41		>>User can select the column according to his preferance through of toggle button.
Basic testing multiple screens	42		>>The time it will take for a batch to process is being displayed
Basic testing multiple screens	43	
Basic testing multiple screens	44		>>Check with after refresh FIFO sequence should not be change 
Basic testing multiple screens	45		>>After uploading new file, FIFO sequence should not be change 
Basic testing multiple screens	46		>>Batch, Customer ID, External batch ID, Files Uploaded, OCR successfull, OCR failed, Uploaded on, OCR status should be shown
Basic testing multiple screens	47		>>Total no of uploaded file count should be shown under file uploaded column
Basic testing multiple screens	48		>>Total no of OCR Successfull file count should be shown under OCR Successfull column
Basic testing multiple screens	49		>>Total no of OCR Failed file count should be shown under OCR Failed column
Basic testing multiple screens	50		>> Click on Up and down arrow, user can change the order like assending or Dessanding 
Basic testing multiple screens	51		>>User can search the file by external batch id, batch id and customer id
Basic testing multiple screens	52	
Basic testing multiple screens	53		>>Files that were deleted during processing should not appear in the "Batch Review Pending" section.
Basic testing multiple screens	54		>>User can able to see total review pending batch count 
Basic testing multiple screens	55	
Basic testing multiple screens	56		>>User can select the column according to his preferance through of toggle button.
Basic testing multiple screens	57		>>The time it will take for a batch to process is being displayed
Basic testing multiple screens	58	
Basic testing multiple screens	59		>>Check with after refresh LIFO sequence should not be change 
Basic testing multiple screens	60		>>After uploading new file, LIFO sequence should not be change 
Basic testing multiple screens	61		>>Batch, Customer ID, External batch ID, Files Uploaded, OCR successfull, OCR failed, Uploaded on, OCR status should be shown
Basic testing multiple screens	62		>>Total no of uploaded file count should be shown under file uploaded column
Basic testing multiple screens	63		>>Total no of OCR Successfull file count should be shown under OCR Successfull column
Basic testing multiple screens	64		>>Total no of OCR Failed file count should be shown under OCR Failed column
Basic testing multiple screens	65		>> Click on Up and down arrow, user can change the order like assending or Dessanding 
Basic testing multiple screens	66		>>User can search the file by external batch id, batch id and customer id
Basic testing multiple screens	67	
Basic testing multiple screens	68		>>Files that were deleted during processing should not appear in the "Batch Review Completed" section.
Basic testing multiple screens	69		>>User can able to see total review completed batch count 
Basic testing multiple screens	70	
Basic testing multiple screens	71		>>User can select the column according to his preferance through of toggle button.
Basic testing multiple screens	72		>>The time it will take for a batch to process is being displayed
Basic testing multiple screens	73		>>If I click on any batch from the API upload screen, the name of that batch should automatically appear in the search bar of the My Documents _Review Pending screen, and the documents should be filtered based on that batch.
Basic testing multiple screens	74		>>When searching for the "Delete Batch" name in the My Documents screen, no files should be displayed.
Basic testing multiple screens	75		>>User can select the column according to his preferance through of toggle button.
Basic testing multiple screens	76		>>User can able to see total review PND batch count 
Basic testing multiple screens	77	
Basic testing multiple screens	78	
Basic testing multiple screens	79	
Basic testing multiple screens	80	
Basic testing multiple screens	81	
Basic testing multiple screens	82	
Basic testing multiple screens	83		>>All the files inside that batch should be visible on that screen.
Basic testing multiple screens	84		>>The first page of the first file should be selected by default, and on the right-hand side of the screen, there should be a maximized view of that file.
Basic testing multiple screens	85	
Basic testing multiple screens	86	
Basic testing multiple screens	87		>>After clicking the reset button once, the document should reset, removing any zoom in or out adjustments.
Basic testing multiple screens	88	
Basic testing multiple screens	89	
Basic testing multiple screens	90		>>Batch name/ Total files/ Total Pages count should be shown on the top right hand side 
Basic testing multiple screens	91		
>>Documents should be displayed according to the file within the batch, and the total number of pages in a file should also be shown above the file's box.
Basic testing multiple screens	92	
Basic testing multiple screens	93		>>If there are three documents within a single file, the first document should be highlighted in blue, the second in orange, and the third in black.
Basic testing multiple screens	94		>>If I move any document into another file set, the moved file should be automatically displayed as a split until it is manually merged>> Submit
Basic testing multiple screens	95	
Basic testing multiple screens	96		NOTE: All the split combination use cases is continue in "Split combination use case" tab
Basic testing multiple screens	97		>>On right-clicking the mouse, a dropdown menu should appear with the option to Mark as SD>> Submit
Basic testing multiple screens	98		>>The "Mark as SD" option should appear directly on top of an invoice.
Basic testing multiple screens	99		>>After clicking on "Mark as SD," the invoice should be converted into SD.
Basic testing multiple screens	100		>>On right-clicking the mouse, a dropdown menu should appear with the option to Rotate.
Basic testing multiple screens	101	
Basic testing multiple screens	102	
Basic testing multiple screens	103		>> Rotation has to be done manually, auto rotation from AI will not happen.
Basic testing multiple screens	104		 >> Rotated file would be downloaded as per the changes made by user.
Basic testing multiple screens	105		>>Rotated file will have extraction and snippet support on Extraction Screen.
Basic testing multiple screens	106		>>Rotated file will have extraction and snippet support on Extraction Screen.
Basic testing multiple screens	107		>>On right-clicking the mouse, a dropdown menu should appear with the option to Move page on another file>> submit 
Basic testing multiple screens	108		>>If there are multiple pages within a file, any page of that file can be moved to another file set in the classification screen.
Basic testing multiple screens	109		>> Move page on another file option should be appeared any type of document 
Basic testing multiple screens	110		>>On right-clicking the mouse, a dropdown menu should appear with the option to Move Document  on another file 
Basic testing multiple screens	111		>>  Move Document  on another file option should be appeared any type of document 
Basic testing multiple screens	112		>>If a document within a file contains multiple pages, and the user wants to move the entire document to another file, they can use this option. It's important to note that all pages of that document should go into the file you are selecting.
Basic testing multiple screens	113		>>To move a file's page or the entire document, if the user clicks on any of these options, a pop-up should appear, allowing the user to choose the file into which they want to move the content. In this use case, ensure that all options of your files are visible in the dropdown, but the file from which you are moving the document or page should appear grayed out.







Basic testing multiple screens	114		>>On right-clicking the mouse, a dropdown menu should appear with the option to Orient Image 
Basic testing multiple screens	115		>>If an image is tillted on an angle ranging from -10 to +10 the ai will orient the image 
Basic testing multiple screens	116	
Basic testing multiple screens	117		>>On right-clicking the mouse, a dropdown menu should appear with the option to DeOrient Image 
Basic testing multiple screens	118	
Basic testing multiple screens	119		>>On right-clicking the mouse, a dropdown menu should appear with the option to Reprocess Images and user can see 4 type of strategy (A/ B/ C/ Z) 
Basic testing multiple screens	120		>>This feature will change the resolution to detact the batter quality of the image 
Basic testing multiple screens	121		>>On right-clicking the mouse, a dropdown menu should appear with the option to mark preceding pages as INV
Basic testing multiple screens	122		>>This option will show when the document is multiple pages 
Basic testing multiple screens	123		>>all the previous pages of the file will get converted into INV
Basic testing multiple screens	124		>>On right-clicking the mouse, a dropdown menu should appear with the option to mark Subsequent pages as INV
Basic testing multiple screens	125		>>This option will show when the document is multiple pages 
Basic testing multiple screens	126		>>all the next pages of the file will get converted into INV
Basic testing multiple screens	127		>>On right-clicking the mouse, a dropdown menu should appear with the option to mark preceding pages as SD
Basic testing multiple screens	128		>>This option will show when the document is multiple pages 
Basic testing multiple screens	129		>>all the previous pages of the file will get converted into SD
Basic testing multiple screens	130		>>On right-clicking the mouse, a dropdown menu should appear with the option to mark Subsequent pages as SD
Basic testing multiple screens	131		>>This option will show when the document is multiple pages 
Basic testing multiple screens	132		>>all the next pages of the file will get converted into SD
Basic testing multiple screens	133		>>After making any changes in the classification screen, if I click the reset button, all changes made by the user should be refreshed.
Basic testing multiple screens	134	
Basic testing multiple screens	135		>>After click on submit button , the changes should be visible which is done from Indexer on classification screen 
Basic testing multiple screens	136		>>Submit classification review without making any changes, all the flow should be going fine 
Basic testing multiple screens	137		>>Submit classification review making  changes which is given in the "Split combination use case" sheet, all the flow should be going fine 
Sanity testing usecase_Prod	1	
Sanity testing usecase_Prod	2	
Sanity testing usecase_Prod	3	
Sanity testing usecase_Prod	4		Indexer able to see two tabs - API Uploads, My Documents
Sanity testing usecase_Prod	5		Batch list - Able to see all batches uploaded for the team to which the indexer belongs
Sanity testing usecase_Prod	6		Batch Filter - User can filter the batches from his option
Sanity testing usecase_Prod	7		Rev Pending Files as per FIFO, other Rev completed and All Batches should be shown as per LIFO
Sanity testing usecase_Prod	8		Go to a batch and open a file under "Review Pending"
Sanity testing usecase_Prod	9	
Sanity testing usecase_Prod	10	
Sanity testing usecase_Prod	11	
Sanity testing usecase_Prod	12	
Sanity testing usecase_Prod	13		Rotation has to be done manually, auto rotation from AI will not happen.
Sanity testing usecase_Prod	14		Rotated file will have extraction and snippet support on Extraction Screen.
Sanity testing usecase_Prod	15		In case if failure or Extraction & snippet support, user will have to fill the extraction data himself but the output will be as per point 2. (Controlled testing with Abhijeet)
Sanity testing usecase_Prod	16		Draging, Movement of Pg's, Files, reordering on Classification Review Screen
Sanity testing usecase_Prod	17		Hard reset: Will come from Extraction Screen>Class Review Screen and will reset the changes made to the original AI classifcation (file uploded).
1) all files in extraction
2) few files in review completed
3) few files in all 3 headers
and so on On clicking Hard Reset, all files of the batch123 (under any tab) will all move to Rev pending and come to original state. The Batch123 filter will reflect and the user will be able to see that Batch/file in Rev Pending.
eg if Sup QC>Class Rev screen>Hard reset... the batch will move to Indexer Rev Pending in My Docs Screen and the user will be auto navigated there.
eg: if admin del the file/doc from UI.. Hard reset will get the same back from Rev Class screen and files will move to Rev Pending
Sanity testing usecase_Prod	18	
Sanity testing usecase_Prod	19		In a Batch, multiple files, Do changes in Rev classification (split/merge/rotate/movement) in few files/with all files and submit.
Sanity testing usecase_Prod	20		Step 1: Classification Review by NON INDEXER, Step 2: Extraction Review ONLY BY INDEXER, Step 3: Sup QC. Test use cases with role's combo for a Batch
Sanity testing usecase_Prod	21	
Sanity testing usecase_Prod	22		If in a batch there are 10 files, file 1, 2,3,4 have been worked upon, and 5th File is modified via Rev Clas then only 5th file will go for OCR and the 4 files which hav been worked will remain the same (reset will not happen). Reset will not happen to File 6,7,8,9,10 also
Sanity testing usecase_Prod	23	
Sanity testing usecase_Prod	24		Go to a batch and open a file under "Classification Reviewed"
Sanity testing usecase_Prod	25		Extraction screen : Editing mode, Extraction contouring check, Page 1, 2,3 navigation check
Sanity testing usecase_Prod	26	
Sanity testing usecase_Prod	27		Check if OCR extracted values are correct and document type is correct
Sanity testing usecase_Prod	28	
Sanity testing usecase_Prod	29	
Sanity testing usecase_Prod	30	
Sanity testing usecase_Prod	31		Click on "Review Classification" button and submit classification again and check whole flow as above
Sanity testing usecase_Prod	32		Change values through contouring and manually typing in the fields, Check field for Validations
Sanity testing usecase_Prod	33		Submit KV extraction. Make sure that the final output is matching with the editing done.
Sanity testing usecase_Prod	34	
Sanity testing usecase_Prod	35		Check if a partially reviewed document has gone through classification review for the second time, the reviewed status should be reset
Sanity testing usecase_Prod	36		Go to a batch and open a file under "Review Completed"
Sanity testing usecase_Prod	37	
Sanity testing usecase_Prod	38	
Sanity testing usecase_Prod	39	
Sanity testing usecase_Prod	40	
Sanity testing usecase_Prod	41	
Sanity testing usecase_Prod	42		The next file which is not locked (being worked upon) in the batch should open, the docs on left side (thumbnails) should match with the middle image and KV extraction on the rt.
Sanity testing usecase_Prod	43		Vendor missing in the list if changed by Indexer and submitted. The same changed vendor should get submitted by Supervisor
Sanity testing usecase_Prod	44	
Sanity testing usecase_Prod	45	
Sanity testing usecase_Prod	46		Supervisor able to see 4 tabs - API Uploads, My Documents, QC Documents, Vendor Mgmt
Sanity testing usecase_Prod	47		Batch list - Able to see all batches uploaded for the team to which the Supervisor belongs
Sanity testing usecase_Prod	48		Batch Filter - User can filter the batches from his option
Sanity testing usecase_Prod	49		QC Documents - Able to see all BATCHES assigned to him based on the review percentage mentioned in team
Sanity testing usecase_Prod	50		Rev Pending Files as per FIFO, other Rev completed and All Batches should be shown as per LIFO
Sanity testing usecase_Prod	51	
Sanity testing usecase_Prod	52	
Sanity testing usecase_Prod	53	
Sanity testing usecase_Prod	54	
Sanity testing usecase_Prod	55	
Sanity testing usecase_Prod	56		Rotation has to be done manually, auto rotation from AI will not happen.
Sanity testing usecase_Prod	57		Rotated file will have extraction and snippet support on Extraction Screen.
Sanity testing usecase_Prod	58		In case if failure or Extraction & snippet support, user will have to fill the extraction data himself but the output will be as per point 2. (Controlled testing with Abhijeet)
Sanity testing usecase_Prod	59		Draging, Movement of Pg's, Files, reordering on Classification Review Screen
Sanity testing usecase_Prod	60		Hard reset: Will come from Extraction Screen>Class Review Screen and will reset the changes made to the original AI classifcation (file uploded).
1) all files in extraction
2) few files in review completed
3) few files in all 3 headers
and so on On clicking Hard Reset, all files of the batch123 (under any tab) will all move to Rev pending and come to original state. The Batch123 filter will reflect and the user will be able to see that Batch/file in Rev Pending.
eg if Sup QC>Class Rev screen>Hard reset... the batch will move to Indexer Rev Pending in My Docs Screen and the user will be auto navigated there.
Sanity testing usecase_Prod	61	
Sanity testing usecase_Prod	62		Move : From one file to another with the changes made
Sanity testing usecase_Prod	63	
Sanity testing usecase_Prod	64	
Sanity testing usecase_Prod	65	
Sanity testing usecase_Prod	66	
Sanity testing usecase_Prod	67	
Sanity testing usecase_Prod	68		Check if OCR extracted values are correct and document type is correct
Sanity testing usecase_Prod	69	
Sanity testing usecase_Prod	70	
Sanity testing usecase_Prod	71	
Sanity testing usecase_Prod	72		Click on "Review Classification" button and submit classification again and check whole flow as above
Sanity testing usecase_Prod	73		Change values through contouring and manually typing in the fields, Check field for Validations
Sanity testing usecase_Prod	74	
Sanity testing usecase_Prod	75		Once all the file submitted, file should move to "Review Completed"
Sanity testing usecase_Prod	76		Check if a partially reviewed document has gone through classification review for the second time, the reviewed status should be reset
Sanity testing usecase_Prod	77	
Sanity testing usecase_Prod	78	
Sanity testing usecase_Prod	79	
Sanity testing usecase_Prod	80	
Sanity testing usecase_Prod	81		Review completed files: Input PDF (post splitting/merging) Vs Output should be same once downloaded.
Sanity testing usecase_Prod	82	
Sanity testing usecase_Prod	83	
Sanity testing usecase_Prod	84	
Sanity testing usecase_Prod	85	
Sanity testing usecase_Prod	86	
Sanity testing usecase_Prod	87	
Sanity testing usecase_Prod	88		Same flow as indexer when supervisor chooses to act as indexer
Sanity testing usecase_Prod	89		Make sure the input/changes in Classification & Splitting and extraction data and reflection as Output (while exporting once the file is Review Completed)
Sanity testing usecase_Prod	90		Vendor missing in the list if changed by Indexer and submitted. The same changed vendor should get submitted by Supervisor
Sanity testing usecase_Prod	91	
Sanity testing usecase_Prod	92	
Sanity testing usecase_Prod	93		Admin able to see tabs - API Uploads, My Documents, QC Documents,OCR Fail, API Logs, Process Flow
Sanity testing usecase_Prod	94		Batch list - Able to see all batches uploaded for the team to which the indexer belongs
Sanity testing usecase_Prod	95		Batch Filter - User can filter the batches from his option
Sanity testing usecase_Prod	96		Under Process flow -> should be able to see Vendor Mgmt, Vendor mgmt-Bol, API Settings, User Mgmt, Team Mgmt, Report Generation
Sanity testing usecase_Prod	97	
Sanity testing usecase_Prod	98	
Sanity testing usecase_Prod	99		My Docs, API Upload Sup QC screen should work for Admin also as per the above pointers
Sanity testing usecase_Prod	100	
Sanity testing usecase_Prod	101	
Sanity testing usecase_Prod	102		Batch list - Able to see all batches uploaded for the team to which the indexer belongs
Sanity testing usecase_Prod	103		QC Documents - Able to see all BATCHES assigned to him based on the review percentage mentioned in team
Sanity testing usecase_Prod	104		Rev Pending Files as per FIFO, other Rev completed and All Batches should be shown as per LIFO
Sanity testing usecase_Prod	105	
Sanity testing usecase_Prod	106	
Sanity testing usecase_Prod	107		Go to a batch and open a file under "Review Pending"
Sanity testing usecase_Prod	108	
Sanity testing usecase_Prod	109	
Sanity testing usecase_Prod	110	
Sanity testing usecase_Prod	111		Rotation has to be done manually, auto rotation from AI will not happen.
Sanity testing usecase_Prod	112		Rotated file will have extraction and snippet support on Extraction Screen.
Sanity testing usecase_Prod	113		In case if failure or Extraction & snippet support, user will have to fill the extraction data himself but the output will be as per point 2. (Controlled testing with Abhijeet)
Sanity testing usecase_Prod	114		Draging, Movement of Pg's, Files, reordering on Classification Review Screen
Sanity testing usecase_Prod	115	
Sanity testing usecase_Prod	116	
Sanity testing usecase_Prod	117	
Sanity testing usecase_Prod	118	
Sanity testing usecase_Prod	119	
Sanity testing usecase_Prod	120	
Sanity testing usecase_Prod	121		Check if OCR extracted values are correct and document type is correct
Sanity testing usecase_Prod	122	
Sanity testing usecase_Prod	123	
Sanity testing usecase_Prod	124	
Sanity testing usecase_Prod	125		Click on "Review Classification" button and submit classification again and check whole flow as above
Sanity testing usecase_Prod	126		Change values through contouring and manually typing in the fields, Check field for Validations
Sanity testing usecase_Prod	127	
Sanity testing usecase_Prod	128	
Sanity testing usecase_Prod	129		Check if a partially reviewed document has gone through classification review for the second time, the reviewed status should be reset
Sanity testing usecase_Prod	130	
Sanity testing usecase_Prod	131	
Sanity testing usecase_Prod	132	
Sanity testing usecase_Prod	133	
Sanity testing usecase_Prod	134	
Sanity testing usecase_Prod	135	
Sanity testing usecase_Prod	136	
Sanity testing usecase_Prod	137	
Sanity testing usecase_Prod	138		Make sure the input/changes in Classification & Splitting and extraction data and reflection as Output (while exporting once the file is Review Completed)
Sanity testing usecase_Prod	139	
Sanity testing usecase_Prod	140	
Sanity testing usecase_Prod	141	
Sanity testing usecase_Prod	142	
Sanity testing usecase_Prod	143	
Sanity testing usecase_Prod	144	
Sanity testing usecase_Prod	145		Hard reset: Will come from Extraction Screen>Class Review Screen and will reset the changes made to the original AI classifcation (file uploded).
1) all files in extraction
2) few files in review completed
3) few files in all 3 headers
and so on On clicking Hard Reset, all files of the batch123 (under any tab) will all move to Rev pending and come to original state. The Batch123 filter will reflect and the user will be able to see that Batch/file in Rev Pending.
eg if Sup QC>Class Rev screen>Hard reset... the batch will move to Indexer Rev Pending in My Docs Screen and the user will be auto navigated there.
Sanity testing usecase_Prod	146		Failure Handling : Controlled testing when AI APIs are failed on purpose (OCR, KVP, Snippet, Rotate, PDF Merger)
Contact Abhijeet for further details
Sanity testing usecase_Prod	147		Vendor missing in the list if changed by Indexer and submitted. The same changed vendor should get submitted by Supervisor
Sanity testing usecase_Prod	148	
Sanity testing usecase_Prod	149	
Sanity testing usecase_Prod	150		Draw the table bound:Add a table if not already present/extracted from the document
Sanity testing usecase_Prod	151		Delete the Rows & Columns:Use control and click to delete row/column
Sanity testing usecase_Prod	152		Extract the data:When creating a new table, it is mandatory to define at least 2 horizontal lines ( one full row) to be able to extract data - Table completion API
When a table is already present/extracted, you're going to use AUTO COMPLETION API
Sanity testing usecase_Prod	153		Contouring :Select a table cell to switch context from table to the selected cell - and call the snippet API by moving the contour and resizing it
Sanity testing usecase_Prod	154		Delete the table spanning across multiple pages:We can delete the whole table spanning across the multiple pages
Sanity testing usecase_Prod	155		Extracting the text from description:We can extract the desired text from the description using the contour
Sanity testing usecase_Prod	156		Dropdown against every extracted columns:Column mapping to the local columns extracted from the table itself - filling values of a column using the metadata which present from the local columns
Sanity testing usecase_Prod	157		Detect the table:It is a function which allows to detect the table in the document
Sanity testing usecase_Prod	158		Full Width:To expand the extracted column view in the horizontal view
Sanity testing usecase_Prod	159		Delete the table:This function will allow to delete the table
Sanity testing usecase_Prod	160		Highlighted value in the dropdown :Once we select the local columns, it by default highlights the column for which values are fetched in that column from the document
Sanity testing usecase_Prod	161		Clear the data :It will delete the fetched data in the respective column
Sanity testing usecase_Prod	162		Table always to be continuous:There should not be discontinuity between the pages of the table
Sanity testing usecase_Prod	163		Use Shift only to specific page:Pressing shift while dealing with column movement will allow the user to make this change reflect across all pages which have that particular column
Sanity testing usecase_Prod	164		Add to all the pages using add table:Adding a new table for a multipage document will ask you to select a range in which you want the table to be present
Sanity testing usecase_Prod	165	
Sanity testing usecase_Prod	166		Autofill:Yet to be developed- While we type partial text in the extracted filed, it should autofill the predicted value/text
Sanity testing usecase_Prod	167		Split Columns:If we observe that content of 2 columns are merged in one column, using this feature, we can seperate the columns and contents
Sanity testing usecase_Prod	168		Reset:It will reset the table to the default as it was in the first place
Sanity testing usecase_Prod	169		Autofill:This will allow the user to type few text and then autofill the rest of the identified words from the contoured area
Sanity testing usecase_Prod	170		Addition of H & L in Vendor details:H denotes as Header only and L denotes both Header and Tabular to be available for the indexers to work on
Sanity testing usecase_Prod	171	
Combination	1	
Combination	2		-----Single page/ two page/ 5page/ tilted single page
-----Classification:
1 page: INV TO SD
2 pages: SAME INV apply A and Z strategy
5 pages: 2 doc merge and 2 merged doc Separated and make 2 doc and 5 doc move into 2 pager file and rotate
Tilted file: apply Orient feature
---Submitted
Review screen
1pager direct submit
2pager changes below
Manually edit. Apply contouring, add text, delete value, select value on page 2 
Table changes
Vendor change H TO L
Edit table
Use features
Add row and column, select drop-down, use auto fill, clear space,
Manually edit. Apply contouring, add text, delete value, select value, clear data, use validation submit
Review completed
Check all the changes done on review pending screen, that is visible on the review completed screen
Combination	3		----Single page/ two page/ 5page/ tilted single page
-----Classification:
1 page: INV TO SD
2 pages: SAME INV apply A and Z strategy
5 pages: 2 doc merge and 2 merged doc Separated and make 2 doc and 5 doc move into 2 pager file and rotate
Tilted file: apply Orient feature
---Submitted
Review screen
1pager direct submit
2pager changes below
Manually edit. Apply contouring, add text, delete value, select value on page 2 
Table changes
Vendor change H TO L
Add table
Use features
Draw the table and extract the value, select drop-down, use auto fill, clear space,
Manually edit. Apply contouring, add text, delete value, select value, clear data, use validation submit
Review completed
Check all the changes done on review pending screen, that is visible on the review completed screen
Combination	4		-----Single page/ two page/ 5page/ tilted single page
-----Classification:
1 page: INV TO SD
2 pages: SAME INV apply A and Z strategy
5 pages: 2 doc merge and 2 merged doc Separated and make 2 doc and 5 doc move into 2 pager file and rotate
Tilted file: apply Orient feature
---Submitted
Review screen
1pager direct submit
2pager changes below
Manually edit. Apply contouring, add text, delete value, select value on page 2 
Table changes
Vendor change H TO L

Use features
Deleted the extracted table and add new table
, select drop-down, use auto fill, clear space,
Manually edit. Apply contouring, add text, delete value, select value, clear data, use validation submit
Review completed
Check all the changes done on review pending screen, that is visible on the review completed screen
Combination	5		-----Single page/ two page/ 5page/ tilted single page
-----Classification:
1 page: INV TO SD
2 pages: SAME INV apply A and Z strategy
5 pages: 2 doc merge and 2 merged doc Separated and make 2 doc and 5 doc move into 2 pager file and rotate
Tilted file: apply Orient feature
---Submitted
Review screen
1pager direct submit
2pager changes below
Manually edit. Apply contouring, add text, delete value, select value on page 2 
Table changes
Vendor change H TO L

Use features
Deleted the extracted table and reset table
, select drop-down, use auto fill, clear space,
Manually edit. Apply contouring, add text, delete value, select value, clear data, use validation submit
Review completed
Check all the changes done on review pending screen, that is visible on the review completed screen
Combination	6		-----Single page/ two page/ 5page/ tilted single page
-----Classification:
1 page: INV TO SD
2 pages: SAME INV apply A and Z strategy
5 pages: 2 doc merge and 2 merged doc Separated and make 2 doc and 5 doc move into 2 pager file and rotate
Tilted file: apply Orient feature
---Submitted
Review screen
1pager direct submit
2pager changes below
Manually edit. Apply contouring, add text, delete value, select value on page 2 
Table changes
Vendor change L TO L
Edit table
Use features
Add row and column, select drop-down, use auto fill, clear space,
Manually edit. Apply contouring, add text, delete value, select value, clear data, use validation submit
Review completed
Check all the changes done on review pending screen, that is visible on the review completed screen
Combination	7		-----Single page/ two page/ 5page/ tilted single page
-----Classification:
1 page: INV TO SD
2 pages: SAME INV apply A and Z strategy
5 pages: 2 doc merge and 2 merged doc Separated and make 2 doc and 5 doc move into 2 pager file and rotate
Tilted file: apply Orient feature
---Submitted
Review screen
1pager direct submit
2pager changes below
Manually edit. Apply contouring, add text, delete value, select value on page 2 
Table changes
Vendor change L TO L
Add table
Use features
Draw the table and extract the value, select drop-down, use auto fill, clear space,
Manually edit. Apply contouring, add text, delete value, select value, clear data, use validation submit
Review completed
Check all the changes done on review pending screen, that is visible on the review completed screen
Combination	8		-----Single page/ two page/ 5page/ tilted single page
-----Classification:
1 page: INV TO SD
2 pages: SAME INV apply A and Z strategy
5 pages: 2 doc merge and 2 merged doc Separated and make 2 doc and 5 doc move into 2 pager file and rotate
Tilted file: apply Orient feature
---Submitted
Review screen
1pager direct submit
2pager changes below
Manually edit. Apply contouring, add text, delete value, select value on page 2 
Table changes
Vendor change L TO L

Use features
Deleted the extracted table and add new table
, select drop-down, use auto fill, clear space,
Manually edit. Apply contouring, add text, delete value, select value, clear data, use validation submit
Review completed
Check all the changes done on review pending screen, that is visible on the review completed screen
Combination	9		-----Single page/ two page/ 5page/ tilted single page
-----Classification:
1 page: INV TO SD
2 pages: SAME INV apply A and Z strategy
5 pages: 2 doc merge and 2 merged doc Separated and make 2 doc and 5 doc move into 2 pager file and rotate
Tilted file: apply Orient feature
---Submitted
Review screen
1pager direct submit
2pager changes below
Manually edit. Apply contouring, add text, delete value, select value on page 2 
Table changes
Vendor change L TO L

Use features
Deleted the extracted table and reset table
, select drop-down, use auto fill, clear space,
Manually edit. Apply contouring, add text, delete value, select value, clear data, use validation submit
Review completed
Check all the changes done on review pending screen, that is visible on the review completed screen
Checking -> docx ['xls', 'xlsx']
Checking -> docx ['docx']
Checking -> docx ['docx']
Test-File-> 1631 
 /datadrive/IKG/test_db/test_plans/BRD-Project Mark.docx
executeInstruction => 
4
The above is the summary of a code change and given below are some test cases. You will have to analyze the summary of the code changes and figure out if any of the test cases below require review OR will be impacted by the code changes.

 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Contents
 
1.   	Introduction
2.   	Purpose of the Document
3.   	Scope of Work
4.   	Reports and Accuracy Logics
5.   	Billing Logic
6.   	Technical Infrastructure Requirement Details [PROD & UAT]
 
 
 
 
 
 
 
 
1.	Introduction
The main purpose of this project is to provide an automated invoice processing system “IDP (Intelligent Document Processing)” to ensure smooth, quick and efﬁcient services to REQUORDIT by AmyGB.ai. As compared to other mechanism of invoice processing, IDP will ensure that the requests, which falls under the scope can be addressed within the agreed and definite TAT committed to the customer. This will help the REQUORDIT team to reduce the TAT for document processing and have a higher productivity in their day to day operations.
 
 
 
 
 
2.	Purpose of the Document
The purpose of the document is to list down and document each and every necessary steps like Current Flow, Proposed Flow, Scope of the Project, Technical Infra Details and Report Mechanism.
This Document is prepared on the basis of the discussions carried out by various discussions between REQUORDIT and AmyGB Team.
 
 
 
 
 
 
 
 
 
 
3.    Requirement Scope
An application to be developed by AmyGB to Automate the invoices wherein After the invoice is uploaded via API, Application will carry out the OCR, extract the defined key values and get the review done by the indexers and post review, make the file available to download in the PDF/tiff format.
 
 
An avg. of 8000 documents will get processed/day except month end wherein this may reach up to 20000 (last couple of days of the month). Each doc will contain an average of 5 pages. 80% of the docs for the day are uploaded between 6AM -8AM CDT which to be processed by noon and output will be downloaded by 6 PM CDT. Processing shall happen on AWS - on cloud. Extracted data is deleted (OCR- text + image) post 23 Hrs(From the time file is downloaded) and retain the meta data (which include the kvp, line items, doc class etc.)
 
 
 
 
Below are the discussed scoped items to be delivered in the phase wise manner-
 
1.	Classification/Re-Classification
2.	Extraction & Review
3.	Supervisor QC model
4.	User & Team Management
5.	PDF & TIFF Download options
6.	Rotation of a file
7.	File merging and Re-ordering
8.     File lock and release 
9.	Tabular Content Extraction + Draw Table Bounds only for tabular failures
10.  Tabular OCR Review with merging/splitting of rows on UI
11.  Real time generation of reports of various doc lifecycle
12.  Feedback Module
 
 
 
 
 
 
 
 
 
The following is the High Level process flow-
 
 
 
 
 
 
 
4.	Scope of Work
The Scope of work for the project will be divided into three parts:
I.          Phase 1 Scope
II.        Phase 2 Scope
III.  	Phase 3 Scope
 
 
 
 
 
I.         Phase 1 Scope:
   Features to be considered for Phase 1 are as follows based on the discussions with the REQUORDIT team.
 
1.     Classification & Extraction Review
2.     Supervisor QC model
3.     User & Team Management
4.     PDF & TIFF Download options
5.     Rotation of a file
6.     File merging and Re-ordering
7. 	File lock and release 
8.  	Real time generation of reports
 
 
   	Phase 1 will consist scenarios related to the above categories described as below
l  Classification/Re-Classification & Extraction Review-
A document should go through the OCR, and get classified as per the standard Invoice Or Supporting Document. Post Classification, if we realize there was error in classification, it should have option to re-classify and submit classification. Once classification is done, 16 defined key values with mandatory and optional fields to be extracted and reviewed by the indexers.
 
l  Supervisor QC model-
Also review can be done by the supervisor and admin as part of Supervisor QC review process wherein set % of the particular customer id based on the team it belongs to, files will flow to mapped supervisor for the QC
 
l  User & Team Management-
This will have 3 roles namely as 1) Indexer 2) Supervisor 3) Admin
Sup and Admin role will be allowed the addition, update and mapping of the users to customer IDs and supervisors
 
l  PDF & TIFF Download options-
Once the review is completed and submitted from both indexers and supervisors (in 100% QC model), file should be available to be downloaded as PDF/Tiff for the end customer
 
 
l  Rotation of a file-
If a document is skewed as an input, while performing the classification, it should rotate the image to 90 Degree or more and same should get downloaded as an output post completion of the Classification and Extraction
 
l  File merging and Re-ordering-
If an invoice is available in file1 out of multiple files in the same batch and Supporting Document of the respective invoice is available in file2, we should have option to merge the SD from File2 with Invoice in file1 and vice versa. This should have option of re-ordering of the page count with respect to the changes
 
l  File lock and release-
This feature will function as, once a file is opened by any indexer, it will be locked for 30 minutes and no one else can work on that file. Also, if an indexer opens multiple tabs or try to work on multiple files together, it will release the older files and allow to work on the latest file only to avoid the ambiguity 
 
l  Real time generation of reports of various doc lifecycle-
There should be an option available which can be used to generate    the reports on the real time for the document processing stats from Admin panel
 
 
Note- After the Go Live, the timeline for the Phase 1 hyper-care stage is 75 days where the AmyGB team will analyze the accuracy on the area of opportunities and fix the bugs comes on the way
 
 
II.      Phase 2 Scope:
Features to be considered for Phase 2 are as follows based on the discussions with the REQUORDIT team.
 
1.  	Table extraction spread across multiple pages within same/different files
2.  	Draw table bounds if table missed or incomplete table identified
3.  	Addition of rows at end of the table (new rows or missing row)
4.  	Merge/split/editing of rows of the table
5.  	QC of the extracted tabular data/line items with option to pick values via snippet
   	Phase 2 will consist scenarios related to the above categories described as below
l  Table extraction spread across multiple pages within same/different files-
This feature should allow the data to be extracted from the tabular invoices across multiple pages within same/different files
 
l  Draw table bounds if table is missed or incomplete -
Draw table bounds if table is missed or incomplete table identification for reprocessing via OCR
 
l  Addition of rows-
This feature will allow to add of rows at end of the table (new rows or missing row)
 
l  Merge/split/editing of rows of the table-
This feature should allow to merge/split/Edit the table rows where it is needed
 
l  QC of the extracted tabular data/line items -
This feature should allow to do the QC of the extracted tabular data/line items with option to pick values via snippet
 
 
Note- After the Go Live, the timeline for the Phase 2 to start is after 75 days, the requirements for the Phase 2 will commence.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
III.   Phase 3 Scope:
Features to be considered for Phase 3 are as follows based on the discussions with the REQUORDIT team.
 
1.     Feedback Module
 
 
   	Phase 3 will consist scenarios related to the above categories described as below
l  Feedback Module-
This feature should allow the application to accept the feedback bases the inputs from the users. This will allow Key - Value Feedback - Instead of manual corrections on all key value extraction give feedback on selected failed files. The USP of this feature is that it also retrains the doc extraction module so that similar errors are not repeated in future batches
 
 
5.	Reports and Accuracy Logics
This section will give a clarity on the Mechanism of Reports for Uploaded and Processed Data.
 
After we go live with the solution in place, we shall generate the reports for document life cycle which should have visibility on page level details of the Invoices and supporting documents for billing purpose.
 
l  Monthly Billing Reports: Monthly billing reports will be created for billing purpose. This report will contain all the invoices along with page detail processed by the IDP for a specific month. This report will be used by AmyGB for all the invoicing purpose
 
 
6.	Billing Logic
The billing will be based on the total number of pages of the Invoices processed by the application. Commercials are closed over other email thread.
 
 
 
 
 
7.	Technical Infrastructure Requirement Details. [PROD & UAT]
This section will list down all the technical level infrastructural details for Production Server that will be necessary for this project. The scope of the project will require technical infrastructure at the REQUORDIT UAT server and also at the Production server.
 
 
 
 
 
 
Infrastructure for UAT/Staging Server for UAT Testing:
After the training of the Application, the codes will be deployed on the UAT server of the REQUORDIT. It is important for the UAT server infrastructure to be similar to PROD server, as all the initial level testing will get carried out on the UAT server. UAT server should also have the similar API’s as the PROD server for the AmyGB to carry out the UAT level testing for the same.
 
Note- Any changes, bug fix or new requirements to be tested on Staging first before deploying it to Prod to have the no impact on the Prod environment
 
 
THANK YOU
 


Checking -> xlsx ['xls', 'xlsx']
Checking -> xlsx ['docx']
Checking -> xlsx ['xls', 'xlsx']
Test-File-> 6003 
 /datadrive/IKG/test_db/test_plans/IKG-testing.xlsx
executeInstruction => 
4
The above is the summary of a code change and given below are some test cases. You will have to analyze the summary of the code changes and figure out if any of the test cases below require review OR will be impacted by the code changes.

Basic testing multiple screens	1	
Basic testing multiple screens	2	
Basic testing multiple screens	3	
Basic testing multiple screens	4	
Basic testing multiple screens	5	
Basic testing multiple screens	6	
Basic testing multiple screens	7	
Basic testing multiple screens	8	
Basic testing multiple screens	9	
Basic testing multiple screens	10	
Basic testing multiple screens	11		>>After click on the login button the my document page should be opened 
Basic testing multiple screens	12	
Basic testing multiple screens	13		>>Correct Email id>> A mail should be come on registered Email ID after click on the Send reset link and a notification popup should be shown up 
Basic testing multiple screens	14	
Basic testing multiple screens	15		>>After click on click here hyper link , which is sent on registered email id, Password reset screen should be open
Basic testing multiple screens	16	
Basic testing multiple screens	17		>>After click on reset password , The password should be reset
Basic testing multiple screens	18		>>Indexer able to see two tabs - API Uploads,  My Documents
Basic testing multiple screens	19		>>Supervisor able to see 4 tabs - API Uploads,  My Documents, QC Documents, Process flow 
Basic testing multiple screens	20		>>Admin able to see tabs - API Uploads,  My Documents, QC Documents,OCR Fail, API Logs, OCR failure & Process Flow, Split file detail
Basic testing multiple screens	21	
Basic testing multiple screens	22		>>Able to see all batches uploaded for the team to which the indexer belongs
Basic testing multiple screens	23		>>Batch list - Able to see all batches uploaded for the team to which the Supervisor belongs
Basic testing multiple screens	24	
Basic testing multiple screens	25	
Basic testing multiple screens	26		>>Check with after refresh LIFO sequence should not be change 
Basic testing multiple screens	27		>>After uploading new file, LIFO sequence should not be change 
Basic testing multiple screens	28		>>Batch, Customer ID, External batch ID, Files Uploaded, OCR successfull, OCR failed, Uploaded on, OCR status should be shown
Basic testing multiple screens	29		>>Total no of uploaded file count should be shown under file uploaded column
Basic testing multiple screens	30		>>Total no of OCR Successfull file count should be shown under OCR Successfull column
Basic testing multiple screens	31		>>Total no of OCR Failed file count should be shown under OCR Failed column
Basic testing multiple screens	32		>> Click on Up and down arrow, user can change the order like assending or Dessanding 
Basic testing multiple screens	33		>>User can search the file by external batch id, batch id and customer id
Basic testing multiple screens	34	
Basic testing multiple screens	35	
Basic testing multiple screens	36	
Basic testing multiple screens	37		>>If duing the processing if admin is delete the file then OCR processing should be stopped and The color of that batch should turn gray.
Basic testing multiple screens	38		>>if user hover on the delete batch then a notification should be appear "All file in this batch have been deleted"
Basic testing multiple screens	39	
Basic testing multiple screens	40	
Basic testing multiple screens	41		>>User can select the column according to his preferance through of toggle button.
Basic testing multiple screens	42		>>The time it will take for a batch to process is being displayed
Basic testing multiple screens	43	
Basic testing multiple screens	44		>>Check with after refresh FIFO sequence should not be change 
Basic testing multiple screens	45		>>After uploading new file, FIFO sequence should not be change 
Basic testing multiple screens	46		>>Batch, Customer ID, External batch ID, Files Uploaded, OCR successfull, OCR failed, Uploaded on, OCR status should be shown
Basic testing multiple screens	47		>>Total no of uploaded file count should be shown under file uploaded column
Basic testing multiple screens	48		>>Total no of OCR Successfull file count should be shown under OCR Successfull column
Basic testing multiple screens	49		>>Total no of OCR Failed file count should be shown under OCR Failed column
Basic testing multiple screens	50		>> Click on Up and down arrow, user can change the order like assending or Dessanding 
Basic testing multiple screens	51		>>User can search the file by external batch id, batch id and customer id
Basic testing multiple screens	52	
Basic testing multiple screens	53		>>Files that were deleted during processing should not appear in the "Batch Review Pending" section.
Basic testing multiple screens	54		>>User can able to see total review pending batch count 
Basic testing multiple screens	55	
Basic testing multiple screens	56		>>User can select the column according to his preferance through of toggle button.
Basic testing multiple screens	57		>>The time it will take for a batch to process is being displayed
Basic testing multiple screens	58	
Basic testing multiple screens	59		>>Check with after refresh LIFO sequence should not be change 
Basic testing multiple screens	60		>>After uploading new file, LIFO sequence should not be change 
Basic testing multiple screens	61		>>Batch, Customer ID, External batch ID, Files Uploaded, OCR successfull, OCR failed, Uploaded on, OCR status should be shown
Basic testing multiple screens	62		>>Total no of uploaded file count should be shown under file uploaded column
Basic testing multiple screens	63		>>Total no of OCR Successfull file count should be shown under OCR Successfull column
Basic testing multiple screens	64		>>Total no of OCR Failed file count should be shown under OCR Failed column
Basic testing multiple screens	65		>> Click on Up and down arrow, user can change the order like assending or Dessanding 
Basic testing multiple screens	66		>>User can search the file by external batch id, batch id and customer id
Basic testing multiple screens	67	
Basic testing multiple screens	68		>>Files that were deleted during processing should not appear in the "Batch Review Completed" section.
Basic testing multiple screens	69		>>User can able to see total review completed batch count 
Basic testing multiple screens	70	
Basic testing multiple screens	71		>>User can select the column according to his preferance through of toggle button.
Basic testing multiple screens	72		>>The time it will take for a batch to process is being displayed
Basic testing multiple screens	73		>>If I click on any batch from the API upload screen, the name of that batch should automatically appear in the search bar of the My Documents _Review Pending screen, and the documents should be filtered based on that batch.
Basic testing multiple screens	74		>>When searching for the "Delete Batch" name in the My Documents screen, no files should be displayed.
Basic testing multiple screens	75		>>User can select the column according to his preferance through of toggle button.
Basic testing multiple screens	76		>>User can able to see total review PND batch count 
Basic testing multiple screens	77	
Basic testing multiple screens	78	
Basic testing multiple screens	79	
Basic testing multiple screens	80	
Basic testing multiple screens	81	
Basic testing multiple screens	82	
Basic testing multiple screens	83		>>All the files inside that batch should be visible on that screen.
Basic testing multiple screens	84		>>The first page of the first file should be selected by default, and on the right-hand side of the screen, there should be a maximized view of that file.
Basic testing multiple screens	85	
Basic testing multiple screens	86	
Basic testing multiple screens	87		>>After clicking the reset button once, the document should reset, removing any zoom in or out adjustments.
Basic testing multiple screens	88	
Basic testing multiple screens	89	
Basic testing multiple screens	90		>>Batch name/ Total files/ Total Pages count should be shown on the top right hand side 
Basic testing multiple screens	91		
>>Documents should be displayed according to the file within the batch, and the total number of pages in a file should also be shown above the file's box.
Basic testing multiple screens	92	
Basic testing multiple screens	93		>>If there are three documents within a single file, the first document should be highlighted in blue, the second in orange, and the third in black.
Basic testing multiple screens	94		>>If I move any document into another file set, the moved file should be automatically displayed as a split until it is manually merged>> Submit
Basic testing multiple screens	95	
Basic testing multiple screens	96		NOTE: All the split combination use cases is continue in "Split combination use case" tab
Basic testing multiple screens	97		>>On right-clicking the mouse, a dropdown menu should appear with the option to Mark as SD>> Submit
Basic testing multiple screens	98		>>The "Mark as SD" option should appear directly on top of an invoice.
Basic testing multiple screens	99		>>After clicking on "Mark as SD," the invoice should be converted into SD.
Basic testing multiple screens	100		>>On right-clicking the mouse, a dropdown menu should appear with the option to Rotate.
Basic testing multiple screens	101	
Basic testing multiple screens	102	
Basic testing multiple screens	103		>> Rotation has to be done manually, auto rotation from AI will not happen.
Basic testing multiple screens	104		 >> Rotated file would be downloaded as per the changes made by user.
Basic testing multiple screens	105		>>Rotated file will have extraction and snippet support on Extraction Screen.
Basic testing multiple screens	106		>>Rotated file will have extraction and snippet support on Extraction Screen.
Basic testing multiple screens	107		>>On right-clicking the mouse, a dropdown menu should appear with the option to Move page on another file>> submit 
Basic testing multiple screens	108		>>If there are multiple pages within a file, any page of that file can be moved to another file set in the classification screen.
Basic testing multiple screens	109		>> Move page on another file option should be appeared any type of document 
Basic testing multiple screens	110		>>On right-clicking the mouse, a dropdown menu should appear with the option to Move Document  on another file 
Basic testing multiple screens	111		>>  Move Document  on another file option should be appeared any type of document 
Basic testing multiple screens	112		>>If a document within a file contains multiple pages, and the user wants to move the entire document to another file, they can use this option. It's important to note that all pages of that document should go into the file you are selecting.
Basic testing multiple screens	113		>>To move a file's page or the entire document, if the user clicks on any of these options, a pop-up should appear, allowing the user to choose the file into which they want to move the content. In this use case, ensure that all options of your files are visible in the dropdown, but the file from which you are moving the document or page should appear grayed out.







Basic testing multiple screens	114		>>On right-clicking the mouse, a dropdown menu should appear with the option to Orient Image 
Basic testing multiple screens	115		>>If an image is tillted on an angle ranging from -10 to +10 the ai will orient the image 
Basic testing multiple screens	116	
Basic testing multiple screens	117		>>On right-clicking the mouse, a dropdown menu should appear with the option to DeOrient Image 
Basic testing multiple screens	118	
Basic testing multiple screens	119		>>On right-clicking the mouse, a dropdown menu should appear with the option to Reprocess Images and user can see 4 type of strategy (A/ B/ C/ Z) 
Basic testing multiple screens	120		>>This feature will change the resolution to detact the batter quality of the image 
Basic testing multiple screens	121		>>On right-clicking the mouse, a dropdown menu should appear with the option to mark preceding pages as INV
Basic testing multiple screens	122		>>This option will show when the document is multiple pages 
Basic testing multiple screens	123		>>all the previous pages of the file will get converted into INV
Basic testing multiple screens	124		>>On right-clicking the mouse, a dropdown menu should appear with the option to mark Subsequent pages as INV
Basic testing multiple screens	125		>>This option will show when the document is multiple pages 
Basic testing multiple screens	126		>>all the next pages of the file will get converted into INV
Basic testing multiple screens	127		>>On right-clicking the mouse, a dropdown menu should appear with the option to mark preceding pages as SD
Basic testing multiple screens	128		>>This option will show when the document is multiple pages 
Basic testing multiple screens	129		>>all the previous pages of the file will get converted into SD
Basic testing multiple screens	130		>>On right-clicking the mouse, a dropdown menu should appear with the option to mark Subsequent pages as SD
Basic testing multiple screens	131		>>This option will show when the document is multiple pages 
Basic testing multiple screens	132		>>all the next pages of the file will get converted into SD
Basic testing multiple screens	133		>>After making any changes in the classification screen, if I click the reset button, all changes made by the user should be refreshed.
Basic testing multiple screens	134	
Basic testing multiple screens	135		>>After click on submit button , the changes should be visible which is done from Indexer on classification screen 
Basic testing multiple screens	136		>>Submit classification review without making any changes, all the flow should be going fine 
Basic testing multiple screens	137		>>Submit classification review making  changes which is given in the "Split combination use case" sheet, all the flow should be going fine 
Sanity testing usecase_Prod	1	
Sanity testing usecase_Prod	2	
Sanity testing usecase_Prod	3	
Sanity testing usecase_Prod	4		Indexer able to see two tabs - API Uploads, My Documents
Sanity testing usecase_Prod	5		Batch list - Able to see all batches uploaded for the team to which the indexer belongs
Sanity testing usecase_Prod	6		Batch Filter - User can filter the batches from his option
Sanity testing usecase_Prod	7		Rev Pending Files as per FIFO, other Rev completed and All Batches should be shown as per LIFO
Sanity testing usecase_Prod	8		Go to a batch and open a file under "Review Pending"
Sanity testing usecase_Prod	9	
Sanity testing usecase_Prod	10	
Sanity testing usecase_Prod	11	
Sanity testing usecase_Prod	12	
Sanity testing usecase_Prod	13		Rotation has to be done manually, auto rotation from AI will not happen.
Sanity testing usecase_Prod	14		Rotated file will have extraction and snippet support on Extraction Screen.
Sanity testing usecase_Prod	15		In case if failure or Extraction & snippet support, user will have to fill the extraction data himself but the output will be as per point 2. (Controlled testing with Abhijeet)
Sanity testing usecase_Prod	16		Draging, Movement of Pg's, Files, reordering on Classification Review Screen
Sanity testing usecase_Prod	17		Hard reset: Will come from Extraction Screen>Class Review Screen and will reset the changes made to the original AI classifcation (file uploded).
1) all files in extraction
2) few files in review completed
3) few files in all 3 headers
and so on On clicking Hard Reset, all files of the batch123 (under any tab) will all move to Rev pending and come to original state. The Batch123 filter will reflect and the user will be able to see that Batch/file in Rev Pending.
eg if Sup QC>Class Rev screen>Hard reset... the batch will move to Indexer Rev Pending in My Docs Screen and the user will be auto navigated there.
eg: if admin del the file/doc from UI.. Hard reset will get the same back from Rev Class screen and files will move to Rev Pending
Sanity testing usecase_Prod	18	
Sanity testing usecase_Prod	19		In a Batch, multiple files, Do changes in Rev classification (split/merge/rotate/movement) in few files/with all files and submit.
Sanity testing usecase_Prod	20		Step 1: Classification Review by NON INDEXER, Step 2: Extraction Review ONLY BY INDEXER, Step 3: Sup QC. Test use cases with role's combo for a Batch
Sanity testing usecase_Prod	21	
Sanity testing usecase_Prod	22		If in a batch there are 10 files, file 1, 2,3,4 have been worked upon, and 5th File is modified via Rev Clas then only 5th file will go for OCR and the 4 files which hav been worked will remain the same (reset will not happen). Reset will not happen to File 6,7,8,9,10 also
Sanity testing usecase_Prod	23	
Sanity testing usecase_Prod	24		Go to a batch and open a file under "Classification Reviewed"
Sanity testing usecase_Prod	25		Extraction screen : Editing mode, Extraction contouring check, Page 1, 2,3 navigation check
Sanity testing usecase_Prod	26	
Sanity testing usecase_Prod	27		Check if OCR extracted values are correct and document type is correct
Sanity testing usecase_Prod	28	
Sanity testing usecase_Prod	29	
Sanity testing usecase_Prod	30	
Sanity testing usecase_Prod	31		Click on "Review Classification" button and submit classification again and check whole flow as above
Sanity testing usecase_Prod	32		Change values through contouring and manually typing in the fields, Check field for Validations
Sanity testing usecase_Prod	33		Submit KV extraction. Make sure that the final output is matching with the editing done.
Sanity testing usecase_Prod	34	
Sanity testing usecase_Prod	35		Check if a partially reviewed document has gone through classification review for the second time, the reviewed status should be reset
Sanity testing usecase_Prod	36		Go to a batch and open a file under "Review Completed"
Sanity testing usecase_Prod	37	
Sanity testing usecase_Prod	38	
Sanity testing usecase_Prod	39	
Sanity testing usecase_Prod	40	
Sanity testing usecase_Prod	41	
Sanity testing usecase_Prod	42		The next file which is not locked (being worked upon) in the batch should open, the docs on left side (thumbnails) should match with the middle image and KV extraction on the rt.
Sanity testing usecase_Prod	43		Vendor missing in the list if changed by Indexer and submitted. The same changed vendor should get submitted by Supervisor
Sanity testing usecase_Prod	44	
Sanity testing usecase_Prod	45	
Sanity testing usecase_Prod	46		Supervisor able to see 4 tabs - API Uploads, My Documents, QC Documents, Vendor Mgmt
Sanity testing usecase_Prod	47		Batch list - Able to see all batches uploaded for the team to which the Supervisor belongs
Sanity testing usecase_Prod	48		Batch Filter - User can filter the batches from his option
Sanity testing usecase_Prod	49		QC Documents - Able to see all BATCHES assigned to him based on the review percentage mentioned in team
Sanity testing usecase_Prod	50		Rev Pending Files as per FIFO, other Rev completed and All Batches should be shown as per LIFO
Sanity testing usecase_Prod	51	
Sanity testing usecase_Prod	52	
Sanity testing usecase_Prod	53	
Sanity testing usecase_Prod	54	
Sanity testing usecase_Prod	55	
Sanity testing usecase_Prod	56		Rotation has to be done manually, auto rotation from AI will not happen.
Sanity testing usecase_Prod	57		Rotated file will have extraction and snippet support on Extraction Screen.
Sanity testing usecase_Prod	58		In case if failure or Extraction & snippet support, user will have to fill the extraction data himself but the output will be as per point 2. (Controlled testing with Abhijeet)
Sanity testing usecase_Prod	59		Draging, Movement of Pg's, Files, reordering on Classification Review Screen
Sanity testing usecase_Prod	60		Hard reset: Will come from Extraction Screen>Class Review Screen and will reset the changes made to the original AI classifcation (file uploded).
1) all files in extraction
2) few files in review completed
3) few files in all 3 headers
and so on On clicking Hard Reset, all files of the batch123 (under any tab) will all move to Rev pending and come to original state. The Batch123 filter will reflect and the user will be able to see that Batch/file in Rev Pending.
eg if Sup QC>Class Rev screen>Hard reset... the batch will move to Indexer Rev Pending in My Docs Screen and the user will be auto navigated there.
Sanity testing usecase_Prod	61	
Sanity testing usecase_Prod	62		Move : From one file to another with the changes made
Sanity testing usecase_Prod	63	
Sanity testing usecase_Prod	64	
Sanity testing usecase_Prod	65	
Sanity testing usecase_Prod	66	
Sanity testing usecase_Prod	67	
Sanity testing usecase_Prod	68		Check if OCR extracted values are correct and document type is correct
Sanity testing usecase_Prod	69	
Sanity testing usecase_Prod	70	
Sanity testing usecase_Prod	71	
Sanity testing usecase_Prod	72		Click on "Review Classification" button and submit classification again and check whole flow as above
Sanity testing usecase_Prod	73		Change values through contouring and manually typing in the fields, Check field for Validations
Sanity testing usecase_Prod	74	
Sanity testing usecase_Prod	75		Once all the file submitted, file should move to "Review Completed"
Sanity testing usecase_Prod	76		Check if a partially reviewed document has gone through classification review for the second time, the reviewed status should be reset
Sanity testing usecase_Prod	77	
Sanity testing usecase_Prod	78	
Sanity testing usecase_Prod	79	
Sanity testing usecase_Prod	80	
Sanity testing usecase_Prod	81		Review completed files: Input PDF (post splitting/merging) Vs Output should be same once downloaded.
Sanity testing usecase_Prod	82	
Sanity testing usecase_Prod	83	
Sanity testing usecase_Prod	84	
Sanity testing usecase_Prod	85	
Sanity testing usecase_Prod	86	
Sanity testing usecase_Prod	87	
Sanity testing usecase_Prod	88		Same flow as indexer when supervisor chooses to act as indexer
Sanity testing usecase_Prod	89		Make sure the input/changes in Classification & Splitting and extraction data and reflection as Output (while exporting once the file is Review Completed)
Sanity testing usecase_Prod	90		Vendor missing in the list if changed by Indexer and submitted. The same changed vendor should get submitted by Supervisor
Sanity testing usecase_Prod	91	
Sanity testing usecase_Prod	92	
Sanity testing usecase_Prod	93		Admin able to see tabs - API Uploads, My Documents, QC Documents,OCR Fail, API Logs, Process Flow
Sanity testing usecase_Prod	94		Batch list - Able to see all batches uploaded for the team to which the indexer belongs
Sanity testing usecase_Prod	95		Batch Filter - User can filter the batches from his option
Sanity testing usecase_Prod	96		Under Process flow -> should be able to see Vendor Mgmt, Vendor mgmt-Bol, API Settings, User Mgmt, Team Mgmt, Report Generation
Sanity testing usecase_Prod	97	
Sanity testing usecase_Prod	98	
Sanity testing usecase_Prod	99		My Docs, API Upload Sup QC screen should work for Admin also as per the above pointers
Sanity testing usecase_Prod	100	
Sanity testing usecase_Prod	101	
Sanity testing usecase_Prod	102		Batch list - Able to see all batches uploaded for the team to which the indexer belongs
Sanity testing usecase_Prod	103		QC Documents - Able to see all BATCHES assigned to him based on the review percentage mentioned in team
Sanity testing usecase_Prod	104		Rev Pending Files as per FIFO, other Rev completed and All Batches should be shown as per LIFO
Sanity testing usecase_Prod	105	
Sanity testing usecase_Prod	106	
Sanity testing usecase_Prod	107		Go to a batch and open a file under "Review Pending"
Sanity testing usecase_Prod	108	
Sanity testing usecase_Prod	109	
Sanity testing usecase_Prod	110	
Sanity testing usecase_Prod	111		Rotation has to be done manually, auto rotation from AI will not happen.
Sanity testing usecase_Prod	112		Rotated file will have extraction and snippet support on Extraction Screen.
Sanity testing usecase_Prod	113		In case if failure or Extraction & snippet support, user will have to fill the extraction data himself but the output will be as per point 2. (Controlled testing with Abhijeet)
Sanity testing usecase_Prod	114		Draging, Movement of Pg's, Files, reordering on Classification Review Screen
Sanity testing usecase_Prod	115	
Sanity testing usecase_Prod	116	
Sanity testing usecase_Prod	117	
Sanity testing usecase_Prod	118	
Sanity testing usecase_Prod	119	
Sanity testing usecase_Prod	120	
Sanity testing usecase_Prod	121		Check if OCR extracted values are correct and document type is correct
Sanity testing usecase_Prod	122	
Sanity testing usecase_Prod	123	
Sanity testing usecase_Prod	124	
Sanity testing usecase_Prod	125		Click on "Review Classification" button and submit classification again and check whole flow as above
Sanity testing usecase_Prod	126		Change values through contouring and manually typing in the fields, Check field for Validations
Sanity testing usecase_Prod	127	
Sanity testing usecase_Prod	128	
Sanity testing usecase_Prod	129		Check if a partially reviewed document has gone through classification review for the second time, the reviewed status should be reset
Sanity testing usecase_Prod	130	
Sanity testing usecase_Prod	131	
Sanity testing usecase_Prod	132	
Sanity testing usecase_Prod	133	
Sanity testing usecase_Prod	134	
Sanity testing usecase_Prod	135	
Sanity testing usecase_Prod	136	
Sanity testing usecase_Prod	137	
Sanity testing usecase_Prod	138		Make sure the input/changes in Classification & Splitting and extraction data and reflection as Output (while exporting once the file is Review Completed)
Sanity testing usecase_Prod	139	
Sanity testing usecase_Prod	140	
Sanity testing usecase_Prod	141	
Sanity testing usecase_Prod	142	
Sanity testing usecase_Prod	143	
Sanity testing usecase_Prod	144	
Sanity testing usecase_Prod	145		Hard reset: Will come from Extraction Screen>Class Review Screen and will reset the changes made to the original AI classifcation (file uploded).
1) all files in extraction
2) few files in review completed
3) few files in all 3 headers
and so on On clicking Hard Reset, all files of the batch123 (under any tab) will all move to Rev pending and come to original state. The Batch123 filter will reflect and the user will be able to see that Batch/file in Rev Pending.
eg if Sup QC>Class Rev screen>Hard reset... the batch will move to Indexer Rev Pending in My Docs Screen and the user will be auto navigated there.
Sanity testing usecase_Prod	146		Failure Handling : Controlled testing when AI APIs are failed on purpose (OCR, KVP, Snippet, Rotate, PDF Merger)
Contact Abhijeet for further details
Sanity testing usecase_Prod	147		Vendor missing in the list if changed by Indexer and submitted. The same changed vendor should get submitted by Supervisor
Sanity testing usecase_Prod	148	
Sanity testing usecase_Prod	149	
Sanity testing usecase_Prod	150		Draw the table bound:Add a table if not already present/extracted from the document
Sanity testing usecase_Prod	151		Delete the Rows & Columns:Use control and click to delete row/column
Sanity testing usecase_Prod	152		Extract the data:When creating a new table, it is mandatory to define at least 2 horizontal lines ( one full row) to be able to extract data - Table completion API
When a table is already present/extracted, you're going to use AUTO COMPLETION API
Sanity testing usecase_Prod	153		Contouring :Select a table cell to switch context from table to the selected cell - and call the snippet API by moving the contour and resizing it
Sanity testing usecase_Prod	154		Delete the table spanning across multiple pages:We can delete the whole table spanning across the multiple pages
Sanity testing usecase_Prod	155		Extracting the text from description:We can extract the desired text from the description using the contour
Sanity testing usecase_Prod	156		Dropdown against every extracted columns:Column mapping to the local columns extracted from the table itself - filling values of a column using the metadata which present from the local columns
Sanity testing usecase_Prod	157		Detect the table:It is a function which allows to detect the table in the document
Sanity testing usecase_Prod	158		Full Width:To expand the extracted column view in the horizontal view
Sanity testing usecase_Prod	159		Delete the table:This function will allow to delete the table
Sanity testing usecase_Prod	160		Highlighted value in the dropdown :Once we select the local columns, it by default highlights the column for which values are fetched in that column from the document
Sanity testing usecase_Prod	161		Clear the data :It will delete the fetched data in the respective column
Sanity testing usecase_Prod	162		Table always to be continuous:There should not be discontinuity between the pages of the table
Sanity testing usecase_Prod	163		Use Shift only to specific page:Pressing shift while dealing with column movement will allow the user to make this change reflect across all pages which have that particular column
Sanity testing usecase_Prod	164		Add to all the pages using add table:Adding a new table for a multipage document will ask you to select a range in which you want the table to be present
Sanity testing usecase_Prod	165	
Sanity testing usecase_Prod	166		Autofill:Yet to be developed- While we type partial text in the extracted filed, it should autofill the predicted value/text
Sanity testing usecase_Prod	167		Split Columns:If we observe that content of 2 columns are merged in one column, using this feature, we can seperate the columns and contents
Sanity testing usecase_Prod	168		Reset:It will reset the table to the default as it was in the first place
Sanity testing usecase_Prod	169		Autofill:This will allow the user to type few text and then autofill the rest of the identified words from the contoured area
Sanity testing usecase_Prod	170		Addition of H & L in Vendor details:H denotes as Header only and L denotes both Header and Tabular to be available for the indexers to work on
Sanity testing usecase_Prod	171	
Combination	1	
Combination	2		-----Single page/ two page/ 5page/ tilted single page
-----Classification:
1 page: INV TO SD
2 pages: SAME INV apply A and Z strategy
5 pages: 2 doc merge and 2 merged doc Separated and make 2 doc and 5 doc move into 2 pager file and rotate
Tilted file: apply Orient feature
---Submitted
Review screen
1pager direct submit
2pager changes below
Manually edit. Apply contouring, add text, delete value, select value on page 2 
Table changes
Vendor change H TO L
Edit table
Use features
Add row and column, select drop-down, use auto fill, clear space,
Manually edit. Apply contouring, add text, delete value, select value, clear data, use validation submit
Review completed
Check all the changes done on review pending screen, that is visible on the review completed screen
Combination	3		----Single page/ two page/ 5page/ tilted single page
-----Classification:
1 page: INV TO SD
2 pages: SAME INV apply A and Z strategy
5 pages: 2 doc merge and 2 merged doc Separated and make 2 doc and 5 doc move into 2 pager file and rotate
Tilted file: apply Orient feature
---Submitted
Review screen
1pager direct submit
2pager changes below
Manually edit. Apply contouring, add text, delete value, select value on page 2 
Table changes
Vendor change H TO L
Add table
Use features
Draw the table and extract the value, select drop-down, use auto fill, clear space,
Manually edit. Apply contouring, add text, delete value, select value, clear data, use validation submit
Review completed
Check all the changes done on review pending screen, that is visible on the review completed screen
Combination	4		-----Single page/ two page/ 5page/ tilted single page
-----Classification:
1 page: INV TO SD
2 pages: SAME INV apply A and Z strategy
5 pages: 2 doc merge and 2 merged doc Separated and make 2 doc and 5 doc move into 2 pager file and rotate
Tilted file: apply Orient feature
---Submitted
Review screen
1pager direct submit
2pager changes below
Manually edit. Apply contouring, add text, delete value, select value on page 2 
Table changes
Vendor change H TO L

Use features
Deleted the extracted table and add new table
, select drop-down, use auto fill, clear space,
Manually edit. Apply contouring, add text, delete value, select value, clear data, use validation submit
Review completed
Check all the changes done on review pending screen, that is visible on the review completed screen
Combination	5		-----Single page/ two page/ 5page/ tilted single page
-----Classification:
1 page: INV TO SD
2 pages: SAME INV apply A and Z strategy
5 pages: 2 doc merge and 2 merged doc Separated and make 2 doc and 5 doc move into 2 pager file and rotate
Tilted file: apply Orient feature
---Submitted
Review screen
1pager direct submit
2pager changes below
Manually edit. Apply contouring, add text, delete value, select value on page 2 
Table changes
Vendor change H TO L

Use features
Deleted the extracted table and reset table
, select drop-down, use auto fill, clear space,
Manually edit. Apply contouring, add text, delete value, select value, clear data, use validation submit
Review completed
Check all the changes done on review pending screen, that is visible on the review completed screen
Combination	6		-----Single page/ two page/ 5page/ tilted single page
-----Classification:
1 page: INV TO SD
2 pages: SAME INV apply A and Z strategy
5 pages: 2 doc merge and 2 merged doc Separated and make 2 doc and 5 doc move into 2 pager file and rotate
Tilted file: apply Orient feature
---Submitted
Review screen
1pager direct submit
2pager changes below
Manually edit. Apply contouring, add text, delete value, select value on page 2 
Table changes
Vendor change L TO L
Edit table
Use features
Add row and column, select drop-down, use auto fill, clear space,
Manually edit. Apply contouring, add text, delete value, select value, clear data, use validation submit
Review completed
Check all the changes done on review pending screen, that is visible on the review completed screen
Combination	7		-----Single page/ two page/ 5page/ tilted single page
-----Classification:
1 page: INV TO SD
2 pages: SAME INV apply A and Z strategy
5 pages: 2 doc merge and 2 merged doc Separated and make 2 doc and 5 doc move into 2 pager file and rotate
Tilted file: apply Orient feature
---Submitted
Review screen
1pager direct submit
2pager changes below
Manually edit. Apply contouring, add text, delete value, select value on page 2 
Table changes
Vendor change L TO L
Add table
Use features
Draw the table and extract the value, select drop-down, use auto fill, clear space,
Manually edit. Apply contouring, add text, delete value, select value, clear data, use validation submit
Review completed
Check all the changes done on review pending screen, that is visible on the review completed screen
Combination	8		-----Single page/ two page/ 5page/ tilted single page
-----Classification:
1 page: INV TO SD
2 pages: SAME INV apply A and Z strategy
5 pages: 2 doc merge and 2 merged doc Separated and make 2 doc and 5 doc move into 2 pager file and rotate
Tilted file: apply Orient feature
---Submitted
Review screen
1pager direct submit
2pager changes below
Manually edit. Apply contouring, add text, delete value, select value on page 2 
Table changes
Vendor change L TO L

Use features
Deleted the extracted table and add new table
, select drop-down, use auto fill, clear space,
Manually edit. Apply contouring, add text, delete value, select value, clear data, use validation submit
Review completed
Check all the changes done on review pending screen, that is visible on the review completed screen
Combination	9		-----Single page/ two page/ 5page/ tilted single page
-----Classification:
1 page: INV TO SD
2 pages: SAME INV apply A and Z strategy
5 pages: 2 doc merge and 2 merged doc Separated and make 2 doc and 5 doc move into 2 pager file and rotate
Tilted file: apply Orient feature
---Submitted
Review screen
1pager direct submit
2pager changes below
Manually edit. Apply contouring, add text, delete value, select value on page 2 
Table changes
Vendor change L TO L

Use features
Deleted the extracted table and reset table
, select drop-down, use auto fill, clear space,
Manually edit. Apply contouring, add text, delete value, select value, clear data, use validation submit
Review completed
Check all the changes done on review pending screen, that is visible on the review completed screen
Combination	10		-----Single page/ two page/ 5page/ tilted single page
-----Classification:
1 page: INV TO SD
2 pages: SAME INV apply A and Z strategy
5 pages: 2 doc merge and 2 merged doc Separated and make 2 doc and 5 doc move into 2 pager file and rotate
Tilted file: apply Orient feature
---Submitted
Review screen
1pager direct submit
2pager changes below
Manually edit. Apply contouring, add text, delete value, select value on page 2 
Table changes
Vendor change H TO L
Edit table
Use features
Add row and column, select drop-down, use auto fill, clear space,
Manually edit. Apply contouring, add text, delete value, select value, clear data, use validation submit
Review completed
Check all the changes done on review pending screen, that is visible on the review completed screen

Login supervisor
Check classification>> Hard Reset>> Indexer again review the File>> and submitted>>File comes to super visor

Supervisor Review 
Supervisor will repeat the step same is above
Checking -> docx ['xls', 'xlsx']
Checking -> docx ['docx']
Checking -> docx ['docx']
Test-File-> 1809 
 /datadrive/IKG/test_db/test_plans/BRD-Project Mark.docx
executeInstruction => 
Here is the analysis of the change and its impact on the downstream file:

```
{
    "Issues": [
        "The change from `string = response.read().decode('utf-8')` to `string = response.read()` may cause issues with character encoding. The original code explicitly decoded the response from bytes to string using UTF-8 encoding. Without this decoding, the string may contain unknown or incorrect characters.",
        "This change may also cause issues with the JSON parsing in the downstream file, as the JSON library may not be able to parse the string correctly if it contains invalid characters."
    ],
    "Criticality": 4,
    "Recommendations": [
        "Verify that the response from the POST request is encoded in a format that does not require explicit decoding, such as ASCII or UTF-8.",
        "Test the downstream file thoroughly to ensure that it can handle the changed response format.",
        "Consider adding error handling to the downstream file to catch any potential parsing errors."
    ]
}
```

Note: The criticality rating of 4 indicates that this change may cause significant issues in the downstream file, and should be thoroughly tested and reviewed before deployment.
The above is the summary of a code change and given below are some test cases. You will have to analyze the summary of the code changes and figure out if any of the test cases below require review OR will be impacted by the code changes.

 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Contents
 
1.   	Introduction
2.   	Purpose of the Document
3.   	Scope of Work
4.   	Reports and Accuracy Logics
5.   	Billing Logic
6.   	Technical Infrastructure Requirement Details [PROD & UAT]
 
 
 
 
 
 
 
 
1.	Introduction
The main purpose of this project is to provide an automated invoice processing system “IDP (Intelligent Document Processing)” to ensure smooth, quick and efﬁcient services to REQUORDIT by AmyGB.ai. As compared to other mechanism of invoice processing, IDP will ensure that the requests, which falls under the scope can be addressed within the agreed and definite TAT committed to the customer. This will help the REQUORDIT team to reduce the TAT for document processing and have a higher productivity in their day to day operations.
 
 
 
 
 
2.	Purpose of the Document
The purpose of the document is to list down and document each and every necessary steps like Current Flow, Proposed Flow, Scope of the Project, Technical Infra Details and Report Mechanism.
This Document is prepared on the basis of the discussions carried out by various discussions between REQUORDIT and AmyGB Team.
 
 
 
 
 
 
 
 
 
 
3.    Requirement Scope
An application to be developed by AmyGB to Automate the invoices wherein After the invoice is uploaded via API, Application will carry out the OCR, extract the defined key values and get the review done by the indexers and post review, make the file available to download in the PDF/tiff format.
 
 
An avg. of 8000 documents will get processed/day except month end wherein this may reach up to 20000 (last couple of days of the month). Each doc will contain an average of 5 pages. 80% of the docs for the day are uploaded between 6AM -8AM CDT which to be processed by noon and output will be downloaded by 6 PM CDT. Processing shall happen on AWS - on cloud. Extracted data is deleted (OCR- text + image) post 23 Hrs(From the time file is downloaded) and retain the meta data (which include the kvp, line items, doc class etc.)
 
 
 
 
Below are the discussed scoped items to be delivered in the phase wise manner-
 
1.	Classification/Re-Classification
2.	Extraction & Review
3.	Supervisor QC model
4.	User & Team Management
5.	PDF & TIFF Download options
6.	Rotation of a file
7.	File merging and Re-ordering
8.     File lock and release 
9.	Tabular Content Extraction + Draw Table Bounds only for tabular failures
10.  Tabular OCR Review with merging/splitting of rows on UI
11.  Real time generation of reports of various doc lifecycle
12.  Feedback Module
 
 
 
 
 
 
 
 
 
The following is the High Level process flow-
 
 
 
 
 
 
 
4.	Scope of Work
The Scope of work for the project will be divided into three parts:
I.          Phase 1 Scope
II.        Phase 2 Scope
III.  	Phase 3 Scope
 
 
 
 
 
I.         Phase 1 Scope:
   Features to be considered for Phase 1 are as follows based on the discussions with the REQUORDIT team.
 
1.     Classification & Extraction Review
2.     Supervisor QC model
3.     User & Team Management
4.     PDF & TIFF Download options
5.     Rotation of a file
6.     File merging and Re-ordering
7. 	File lock and release 
8.  	Real time generation of reports
 
 
   	Phase 1 will consist scenarios related to the above categories described as below
l  Classification/Re-Classification & Extraction Review-
A document should go through the OCR, and get classified as per the standard Invoice Or Supporting Document. Post Classification, if we realize there was error in classification, it should have option to re-classify and submit classification. Once classification is done, 16 defined key values with mandatory and optional fields to be extracted and reviewed by the indexers.
 
l  Supervisor QC model-
Also review can be done by the supervisor and admin as part of Supervisor QC review process wherein set % of the particular customer id based on the team it belongs to, files will flow to mapped supervisor for the QC
 
l  User & Team Management-
This will have 3 roles namely as 1) Indexer 2) Supervisor 3) Admin
Sup and Admin role will be allowed the addition, update and mapping of the users to customer IDs and supervisors
 
l  PDF & TIFF Download options-
Once the review is completed and submitted from both indexers and supervisors (in 100% QC model), file should be available to be downloaded as PDF/Tiff for the end customer
 
 
l  Rotation of a file-
If a document is skewed as an input, while performing the classification, it should rotate the image to 90 Degree or more and same should get downloaded as an output post completion of the Classification and Extraction
 
l  File merging and Re-ordering-
If an invoice is available in file1 out of multiple files in the same batch and Supporting Document of the respective invoice is available in file2, we should have option to merge the SD from File2 with Invoice in file1 and vice versa. This should have option of re-ordering of the page count with respect to the changes
 
l  File lock and release-
This feature will function as, once a file is opened by any indexer, it will be locked for 30 minutes and no one else can work on that file. Also, if an indexer opens multiple tabs or try to work on multiple files together, it will release the older files and allow to work on the latest file only to avoid the ambiguity 
 
l  Real time generation of reports of various doc lifecycle-
There should be an option available which can be used to generate    the reports on the real time for the document processing stats from Admin panel
 
 
Note- After the Go Live, the timeline for the Phase 1 hyper-care stage is 75 days where the AmyGB team will analyze the accuracy on the area of opportunities and fix the bugs comes on the way
 
 
II.      Phase 2 Scope:
Features to be considered for Phase 2 are as follows based on the discussions with the REQUORDIT team.
 
1.  	Table extraction spread across multiple pages within same/different files
2.  	Draw table bounds if table missed or incomplete table identified
3.  	Addition of rows at end of the table (new rows or missing row)
4.  	Merge/split/editing of rows of the table
5.  	QC of the extracted tabular data/line items with option to pick values via snippet
   	Phase 2 will consist scenarios related to the above categories described as below
l  Table extraction spread across multiple pages within same/different files-
This feature should allow the data to be extracted from the tabular invoices across multiple pages within same/different files
 
l  Draw table bounds if table is missed or incomplete -
Draw table bounds if table is missed or incomplete table identification for reprocessing via OCR
 
l  Addition of rows-
This feature will allow to add of rows at end of the table (new rows or missing row)
 
l  Merge/split/editing of rows of the table-
This feature should allow to merge/split/Edit the table rows where it is needed
 
l  QC of the extracted tabular data/line items -
This feature should allow to do the QC of the extracted tabular data/line items with option to pick values via snippet
 
 
Note- After the Go Live, the timeline for the Phase 2 to start is after 75 days, the requirements for the Phase 2 will commence.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
III.   Phase 3 Scope:
Features to be considered for Phase 3 are as follows based on the discussions with the REQUORDIT team.
 
1.     Feedback Module
 
 
   	Phase 3 will consist scenarios related to the above categories described as below
l  Feedback Module-
This feature should allow the application to accept the feedback bases the inputs from the users. This will allow Key - Value Feedback - Instead of manual corrections on all key value extraction give feedback on selected failed files. The USP of this feature is that it also retrains the doc extraction module so that similar errors are not repeated in future batches
 
 
5.	Reports and Accuracy Logics
This section will give a clarity on the Mechanism of Reports for Uploaded and Processed Data.
 
After we go live with the solution in place, we shall generate the reports for document life cycle which should have visibility on page level details of the Invoices and supporting documents for billing purpose.
 
l  Monthly Billing Reports: Monthly billing reports will be created for billing purpose. This report will contain all the invoices along with page detail processed by the IDP for a specific month. This report will be used by AmyGB for all the invoicing purpose
 
 
6.	Billing Logic
The billing will be based on the total number of pages of the Invoices processed by the application. Commercials are closed over other email thread.
 
 
 
 
 
7.	Technical Infrastructure Requirement Details. [PROD & UAT]
This section will list down all the technical level infrastructural details for Production Server that will be necessary for this project. The scope of the project will require technical infrastructure at the REQUORDIT UAT server and also at the Production server.
 
 
 
 
 
 
Infrastructure for UAT/Staging Server for UAT Testing:
After the training of the Application, the codes will be deployed on the UAT server of the REQUORDIT. It is important for the UAT server infrastructure to be similar to PROD server, as all the initial level testing will get carried out on the UAT server. UAT server should also have the similar API’s as the PROD server for the AmyGB to carry out the UAT level testing for the same.
 
Note- Any changes, bug fix or new requirements to be tested on Staging first before deploying it to Prod to have the no impact on the Prod environment
 
 
THANK YOU
 


Checking -> xlsx ['xls', 'xlsx']
Checking -> xlsx ['docx']
Checking -> xlsx ['xls', 'xlsx']
Test-File-> 5888 
 /datadrive/IKG/test_db/test_plans/IKG-testing.xlsx
executeInstruction => 
Here is the analysis of the change and its impact on the downstream file:

```
{
    "Issues": [
        "The change from `string = response.read().decode('utf-8')` to `string = response.read()` may cause issues with character encoding. The original code explicitly decoded the response from bytes to string using UTF-8 encoding. Without this decoding, the string may contain unknown or incorrect characters.",
        "This change may also cause issues with the JSON parsing in the downstream file, as the JSON library may not be able to parse the string correctly if it contains invalid characters."
    ],
    "Criticality": 4,
    "Recommendations": [
        "Verify that the response from the POST request is encoded in a format that does not require explicit decoding, such as ASCII or UTF-8.",
        "Test the downstream file thoroughly to ensure that it can handle the changed response format.",
        "Consider adding error handling to the downstream file to catch any potential parsing errors."
    ]
}
```

Note: The criticality rating of 4 indicates that this change may cause significant issues in the downstream file, and should be thoroughly tested and reviewed before deployment.
The above is the summary of a code change and given below are some test cases. You will have to analyze the summary of the code changes and figure out if any of the test cases below require review OR will be impacted by the code changes.

Basic testing multiple screens	1	
Basic testing multiple screens	2	
Basic testing multiple screens	3	
Basic testing multiple screens	4	
Basic testing multiple screens	5	
Basic testing multiple screens	6	
Basic testing multiple screens	7	
Basic testing multiple screens	8	
Basic testing multiple screens	9	
Basic testing multiple screens	10	
Basic testing multiple screens	11		>>After click on the login button the my document page should be opened 
Basic testing multiple screens	12	
Basic testing multiple screens	13		>>Correct Email id>> A mail should be come on registered Email ID after click on the Send reset link and a notification popup should be shown up 
Basic testing multiple screens	14	
Basic testing multiple screens	15		>>After click on click here hyper link , which is sent on registered email id, Password reset screen should be open
Basic testing multiple screens	16	
Basic testing multiple screens	17		>>After click on reset password , The password should be reset
Basic testing multiple screens	18		>>Indexer able to see two tabs - API Uploads,  My Documents
Basic testing multiple screens	19		>>Supervisor able to see 4 tabs - API Uploads,  My Documents, QC Documents, Process flow 
Basic testing multiple screens	20		>>Admin able to see tabs - API Uploads,  My Documents, QC Documents,OCR Fail, API Logs, OCR failure & Process Flow, Split file detail
Basic testing multiple screens	21	
Basic testing multiple screens	22		>>Able to see all batches uploaded for the team to which the indexer belongs
Basic testing multiple screens	23		>>Batch list - Able to see all batches uploaded for the team to which the Supervisor belongs
Basic testing multiple screens	24	
Basic testing multiple screens	25	
Basic testing multiple screens	26		>>Check with after refresh LIFO sequence should not be change 
Basic testing multiple screens	27		>>After uploading new file, LIFO sequence should not be change 
Basic testing multiple screens	28		>>Batch, Customer ID, External batch ID, Files Uploaded, OCR successfull, OCR failed, Uploaded on, OCR status should be shown
Basic testing multiple screens	29		>>Total no of uploaded file count should be shown under file uploaded column
Basic testing multiple screens	30		>>Total no of OCR Successfull file count should be shown under OCR Successfull column
Basic testing multiple screens	31		>>Total no of OCR Failed file count should be shown under OCR Failed column
Basic testing multiple screens	32		>> Click on Up and down arrow, user can change the order like assending or Dessanding 
Basic testing multiple screens	33		>>User can search the file by external batch id, batch id and customer id
Basic testing multiple screens	34	
Basic testing multiple screens	35	
Basic testing multiple screens	36	
Basic testing multiple screens	37		>>If duing the processing if admin is delete the file then OCR processing should be stopped and The color of that batch should turn gray.
Basic testing multiple screens	38		>>if user hover on the delete batch then a notification should be appear "All file in this batch have been deleted"
Basic testing multiple screens	39	
Basic testing multiple screens	40	
Basic testing multiple screens	41		>>User can select the column according to his preferance through of toggle button.
Basic testing multiple screens	42		>>The time it will take for a batch to process is being displayed
Basic testing multiple screens	43	
Basic testing multiple screens	44		>>Check with after refresh FIFO sequence should not be change 
Basic testing multiple screens	45		>>After uploading new file, FIFO sequence should not be change 
Basic testing multiple screens	46		>>Batch, Customer ID, External batch ID, Files Uploaded, OCR successfull, OCR failed, Uploaded on, OCR status should be shown
Basic testing multiple screens	47		>>Total no of uploaded file count should be shown under file uploaded column
Basic testing multiple screens	48		>>Total no of OCR Successfull file count should be shown under OCR Successfull column
Basic testing multiple screens	49		>>Total no of OCR Failed file count should be shown under OCR Failed column
Basic testing multiple screens	50		>> Click on Up and down arrow, user can change the order like assending or Dessanding 
Basic testing multiple screens	51		>>User can search the file by external batch id, batch id and customer id
Basic testing multiple screens	52	
Basic testing multiple screens	53		>>Files that were deleted during processing should not appear in the "Batch Review Pending" section.
Basic testing multiple screens	54		>>User can able to see total review pending batch count 
Basic testing multiple screens	55	
Basic testing multiple screens	56		>>User can select the column according to his preferance through of toggle button.
Basic testing multiple screens	57		>>The time it will take for a batch to process is being displayed
Basic testing multiple screens	58	
Basic testing multiple screens	59		>>Check with after refresh LIFO sequence should not be change 
Basic testing multiple screens	60		>>After uploading new file, LIFO sequence should not be change 
Basic testing multiple screens	61		>>Batch, Customer ID, External batch ID, Files Uploaded, OCR successfull, OCR failed, Uploaded on, OCR status should be shown
Basic testing multiple screens	62		>>Total no of uploaded file count should be shown under file uploaded column
Basic testing multiple screens	63		>>Total no of OCR Successfull file count should be shown under OCR Successfull column
Basic testing multiple screens	64		>>Total no of OCR Failed file count should be shown under OCR Failed column
Basic testing multiple screens	65		>> Click on Up and down arrow, user can change the order like assending or Dessanding 
Basic testing multiple screens	66		>>User can search the file by external batch id, batch id and customer id
Basic testing multiple screens	67	
Basic testing multiple screens	68		>>Files that were deleted during processing should not appear in the "Batch Review Completed" section.
Basic testing multiple screens	69		>>User can able to see total review completed batch count 
Basic testing multiple screens	70	
Basic testing multiple screens	71		>>User can select the column according to his preferance through of toggle button.
Basic testing multiple screens	72		>>The time it will take for a batch to process is being displayed
Basic testing multiple screens	73		>>If I click on any batch from the API upload screen, the name of that batch should automatically appear in the search bar of the My Documents _Review Pending screen, and the documents should be filtered based on that batch.
Basic testing multiple screens	74		>>When searching for the "Delete Batch" name in the My Documents screen, no files should be displayed.
Basic testing multiple screens	75		>>User can select the column according to his preferance through of toggle button.
Basic testing multiple screens	76		>>User can able to see total review PND batch count 
Basic testing multiple screens	77	
Basic testing multiple screens	78	
Basic testing multiple screens	79	
Basic testing multiple screens	80	
Basic testing multiple screens	81	
Basic testing multiple screens	82	
Basic testing multiple screens	83		>>All the files inside that batch should be visible on that screen.
Basic testing multiple screens	84		>>The first page of the first file should be selected by default, and on the right-hand side of the screen, there should be a maximized view of that file.
Basic testing multiple screens	85	
Basic testing multiple screens	86	
Basic testing multiple screens	87		>>After clicking the reset button once, the document should reset, removing any zoom in or out adjustments.
Basic testing multiple screens	88	
Basic testing multiple screens	89	
Basic testing multiple screens	90		>>Batch name/ Total files/ Total Pages count should be shown on the top right hand side 
Basic testing multiple screens	91		
>>Documents should be displayed according to the file within the batch, and the total number of pages in a file should also be shown above the file's box.
Basic testing multiple screens	92	
Basic testing multiple screens	93		>>If there are three documents within a single file, the first document should be highlighted in blue, the second in orange, and the third in black.
Basic testing multiple screens	94		>>If I move any document into another file set, the moved file should be automatically displayed as a split until it is manually merged>> Submit
Basic testing multiple screens	95	
Basic testing multiple screens	96		NOTE: All the split combination use cases is continue in "Split combination use case" tab
Basic testing multiple screens	97		>>On right-clicking the mouse, a dropdown menu should appear with the option to Mark as SD>> Submit
Basic testing multiple screens	98		>>The "Mark as SD" option should appear directly on top of an invoice.
Basic testing multiple screens	99		>>After clicking on "Mark as SD," the invoice should be converted into SD.
Basic testing multiple screens	100		>>On right-clicking the mouse, a dropdown menu should appear with the option to Rotate.
Basic testing multiple screens	101	
Basic testing multiple screens	102	
Basic testing multiple screens	103		>> Rotation has to be done manually, auto rotation from AI will not happen.
Basic testing multiple screens	104		 >> Rotated file would be downloaded as per the changes made by user.
Basic testing multiple screens	105		>>Rotated file will have extraction and snippet support on Extraction Screen.
Basic testing multiple screens	106		>>Rotated file will have extraction and snippet support on Extraction Screen.
Basic testing multiple screens	107		>>On right-clicking the mouse, a dropdown menu should appear with the option to Move page on another file>> submit 
Basic testing multiple screens	108		>>If there are multiple pages within a file, any page of that file can be moved to another file set in the classification screen.
Basic testing multiple screens	109		>> Move page on another file option should be appeared any type of document 
Basic testing multiple screens	110		>>On right-clicking the mouse, a dropdown menu should appear with the option to Move Document  on another file 
Basic testing multiple screens	111		>>  Move Document  on another file option should be appeared any type of document 
Basic testing multiple screens	112		>>If a document within a file contains multiple pages, and the user wants to move the entire document to another file, they can use this option. It's important to note that all pages of that document should go into the file you are selecting.
Basic testing multiple screens	113		>>To move a file's page or the entire document, if the user clicks on any of these options, a pop-up should appear, allowing the user to choose the file into which they want to move the content. In this use case, ensure that all options of your files are visible in the dropdown, but the file from which you are moving the document or page should appear grayed out.







Basic testing multiple screens	114		>>On right-clicking the mouse, a dropdown menu should appear with the option to Orient Image 
Basic testing multiple screens	115		>>If an image is tillted on an angle ranging from -10 to +10 the ai will orient the image 
Basic testing multiple screens	116	
Basic testing multiple screens	117		>>On right-clicking the mouse, a dropdown menu should appear with the option to DeOrient Image 
Basic testing multiple screens	118	
Basic testing multiple screens	119		>>On right-clicking the mouse, a dropdown menu should appear with the option to Reprocess Images and user can see 4 type of strategy (A/ B/ C/ Z) 
Basic testing multiple screens	120		>>This feature will change the resolution to detact the batter quality of the image 
Basic testing multiple screens	121		>>On right-clicking the mouse, a dropdown menu should appear with the option to mark preceding pages as INV
Basic testing multiple screens	122		>>This option will show when the document is multiple pages 
Basic testing multiple screens	123		>>all the previous pages of the file will get converted into INV
Basic testing multiple screens	124		>>On right-clicking the mouse, a dropdown menu should appear with the option to mark Subsequent pages as INV
Basic testing multiple screens	125		>>This option will show when the document is multiple pages 
Basic testing multiple screens	126		>>all the next pages of the file will get converted into INV
Basic testing multiple screens	127		>>On right-clicking the mouse, a dropdown menu should appear with the option to mark preceding pages as SD
Basic testing multiple screens	128		>>This option will show when the document is multiple pages 
Basic testing multiple screens	129		>>all the previous pages of the file will get converted into SD
Basic testing multiple screens	130		>>On right-clicking the mouse, a dropdown menu should appear with the option to mark Subsequent pages as SD
Basic testing multiple screens	131		>>This option will show when the document is multiple pages 
Basic testing multiple screens	132		>>all the next pages of the file will get converted into SD
Basic testing multiple screens	133		>>After making any changes in the classification screen, if I click the reset button, all changes made by the user should be refreshed.
Basic testing multiple screens	134	
Basic testing multiple screens	135		>>After click on submit button , the changes should be visible which is done from Indexer on classification screen 
Basic testing multiple screens	136		>>Submit classification review without making any changes, all the flow should be going fine 
Basic testing multiple screens	137		>>Submit classification review making  changes which is given in the "Split combination use case" sheet, all the flow should be going fine 
Sanity testing usecase_Prod	1	
Sanity testing usecase_Prod	2	
Sanity testing usecase_Prod	3	
Sanity testing usecase_Prod	4		Indexer able to see two tabs - API Uploads, My Documents
Sanity testing usecase_Prod	5		Batch list - Able to see all batches uploaded for the team to which the indexer belongs
Sanity testing usecase_Prod	6		Batch Filter - User can filter the batches from his option
Sanity testing usecase_Prod	7		Rev Pending Files as per FIFO, other Rev completed and All Batches should be shown as per LIFO
Sanity testing usecase_Prod	8		Go to a batch and open a file under "Review Pending"
Sanity testing usecase_Prod	9	
Sanity testing usecase_Prod	10	
Sanity testing usecase_Prod	11	
Sanity testing usecase_Prod	12	
Sanity testing usecase_Prod	13		Rotation has to be done manually, auto rotation from AI will not happen.
Sanity testing usecase_Prod	14		Rotated file will have extraction and snippet support on Extraction Screen.
Sanity testing usecase_Prod	15		In case if failure or Extraction & snippet support, user will have to fill the extraction data himself but the output will be as per point 2. (Controlled testing with Abhijeet)
Sanity testing usecase_Prod	16		Draging, Movement of Pg's, Files, reordering on Classification Review Screen
Sanity testing usecase_Prod	17		Hard reset: Will come from Extraction Screen>Class Review Screen and will reset the changes made to the original AI classifcation (file uploded).
1) all files in extraction
2) few files in review completed
3) few files in all 3 headers
and so on On clicking Hard Reset, all files of the batch123 (under any tab) will all move to Rev pending and come to original state. The Batch123 filter will reflect and the user will be able to see that Batch/file in Rev Pending.
eg if Sup QC>Class Rev screen>Hard reset... the batch will move to Indexer Rev Pending in My Docs Screen and the user will be auto navigated there.
eg: if admin del the file/doc from UI.. Hard reset will get the same back from Rev Class screen and files will move to Rev Pending
Sanity testing usecase_Prod	18	
Sanity testing usecase_Prod	19		In a Batch, multiple files, Do changes in Rev classification (split/merge/rotate/movement) in few files/with all files and submit.
Sanity testing usecase_Prod	20		Step 1: Classification Review by NON INDEXER, Step 2: Extraction Review ONLY BY INDEXER, Step 3: Sup QC. Test use cases with role's combo for a Batch
Sanity testing usecase_Prod	21	
Sanity testing usecase_Prod	22		If in a batch there are 10 files, file 1, 2,3,4 have been worked upon, and 5th File is modified via Rev Clas then only 5th file will go for OCR and the 4 files which hav been worked will remain the same (reset will not happen). Reset will not happen to File 6,7,8,9,10 also
Sanity testing usecase_Prod	23	
Sanity testing usecase_Prod	24		Go to a batch and open a file under "Classification Reviewed"
Sanity testing usecase_Prod	25		Extraction screen : Editing mode, Extraction contouring check, Page 1, 2,3 navigation check
Sanity testing usecase_Prod	26	
Sanity testing usecase_Prod	27		Check if OCR extracted values are correct and document type is correct
Sanity testing usecase_Prod	28	
Sanity testing usecase_Prod	29	
Sanity testing usecase_Prod	30	
Sanity testing usecase_Prod	31		Click on "Review Classification" button and submit classification again and check whole flow as above
Sanity testing usecase_Prod	32		Change values through contouring and manually typing in the fields, Check field for Validations
Sanity testing usecase_Prod	33		Submit KV extraction. Make sure that the final output is matching with the editing done.
Sanity testing usecase_Prod	34	
Sanity testing usecase_Prod	35		Check if a partially reviewed document has gone through classification review for the second time, the reviewed status should be reset
Sanity testing usecase_Prod	36		Go to a batch and open a file under "Review Completed"
Sanity testing usecase_Prod	37	
Sanity testing usecase_Prod	38	
Sanity testing usecase_Prod	39	
Sanity testing usecase_Prod	40	
Sanity testing usecase_Prod	41	
Sanity testing usecase_Prod	42		The next file which is not locked (being worked upon) in the batch should open, the docs on left side (thumbnails) should match with the middle image and KV extraction on the rt.
Sanity testing usecase_Prod	43		Vendor missing in the list if changed by Indexer and submitted. The same changed vendor should get submitted by Supervisor
Sanity testing usecase_Prod	44	
Sanity testing usecase_Prod	45	
Sanity testing usecase_Prod	46		Supervisor able to see 4 tabs - API Uploads, My Documents, QC Documents, Vendor Mgmt
Sanity testing usecase_Prod	47		Batch list - Able to see all batches uploaded for the team to which the Supervisor belongs
Sanity testing usecase_Prod	48		Batch Filter - User can filter the batches from his option
Sanity testing usecase_Prod	49		QC Documents - Able to see all BATCHES assigned to him based on the review percentage mentioned in team
Sanity testing usecase_Prod	50		Rev Pending Files as per FIFO, other Rev completed and All Batches should be shown as per LIFO
Sanity testing usecase_Prod	51	
Sanity testing usecase_Prod	52	
Sanity testing usecase_Prod	53	
Sanity testing usecase_Prod	54	
Sanity testing usecase_Prod	55	
Sanity testing usecase_Prod	56		Rotation has to be done manually, auto rotation from AI will not happen.
Sanity testing usecase_Prod	57		Rotated file will have extraction and snippet support on Extraction Screen.
Sanity testing usecase_Prod	58		In case if failure or Extraction & snippet support, user will have to fill the extraction data himself but the output will be as per point 2. (Controlled testing with Abhijeet)
Sanity testing usecase_Prod	59		Draging, Movement of Pg's, Files, reordering on Classification Review Screen
Sanity testing usecase_Prod	60		Hard reset: Will come from Extraction Screen>Class Review Screen and will reset the changes made to the original AI classifcation (file uploded).
1) all files in extraction
2) few files in review completed
3) few files in all 3 headers
and so on On clicking Hard Reset, all files of the batch123 (under any tab) will all move to Rev pending and come to original state. The Batch123 filter will reflect and the user will be able to see that Batch/file in Rev Pending.
eg if Sup QC>Class Rev screen>Hard reset... the batch will move to Indexer Rev Pending in My Docs Screen and the user will be auto navigated there.
Sanity testing usecase_Prod	61	
Sanity testing usecase_Prod	62		Move : From one file to another with the changes made
Sanity testing usecase_Prod	63	
Sanity testing usecase_Prod	64	
Sanity testing usecase_Prod	65	
Sanity testing usecase_Prod	66	
Sanity testing usecase_Prod	67	
Sanity testing usecase_Prod	68		Check if OCR extracted values are correct and document type is correct
Sanity testing usecase_Prod	69	
Sanity testing usecase_Prod	70	
Sanity testing usecase_Prod	71	
Sanity testing usecase_Prod	72		Click on "Review Classification" button and submit classification again and check whole flow as above
Sanity testing usecase_Prod	73		Change values through contouring and manually typing in the fields, Check field for Validations
Sanity testing usecase_Prod	74	
Sanity testing usecase_Prod	75		Once all the file submitted, file should move to "Review Completed"
Sanity testing usecase_Prod	76		Check if a partially reviewed document has gone through classification review for the second time, the reviewed status should be reset
Sanity testing usecase_Prod	77	
Sanity testing usecase_Prod	78	
Sanity testing usecase_Prod	79	
Sanity testing usecase_Prod	80	
Sanity testing usecase_Prod	81		Review completed files: Input PDF (post splitting/merging) Vs Output should be same once downloaded.
Sanity testing usecase_Prod	82	
Sanity testing usecase_Prod	83	
Sanity testing usecase_Prod	84	
Sanity testing usecase_Prod	85	
Sanity testing usecase_Prod	86	
Sanity testing usecase_Prod	87	
Sanity testing usecase_Prod	88		Same flow as indexer when supervisor chooses to act as indexer
Sanity testing usecase_Prod	89		Make sure the input/changes in Classification & Splitting and extraction data and reflection as Output (while exporting once the file is Review Completed)
Sanity testing usecase_Prod	90		Vendor missing in the list if changed by Indexer and submitted. The same changed vendor should get submitted by Supervisor
Sanity testing usecase_Prod	91	
Sanity testing usecase_Prod	92	
Sanity testing usecase_Prod	93		Admin able to see tabs - API Uploads, My Documents, QC Documents,OCR Fail, API Logs, Process Flow
Sanity testing usecase_Prod	94		Batch list - Able to see all batches uploaded for the team to which the indexer belongs
Sanity testing usecase_Prod	95		Batch Filter - User can filter the batches from his option
Sanity testing usecase_Prod	96		Under Process flow -> should be able to see Vendor Mgmt, Vendor mgmt-Bol, API Settings, User Mgmt, Team Mgmt, Report Generation
Sanity testing usecase_Prod	97	
Sanity testing usecase_Prod	98	
Sanity testing usecase_Prod	99		My Docs, API Upload Sup QC screen should work for Admin also as per the above pointers
Sanity testing usecase_Prod	100	
Sanity testing usecase_Prod	101	
Sanity testing usecase_Prod	102		Batch list - Able to see all batches uploaded for the team to which the indexer belongs
Sanity testing usecase_Prod	103		QC Documents - Able to see all BATCHES assigned to him based on the review percentage mentioned in team
Sanity testing usecase_Prod	104		Rev Pending Files as per FIFO, other Rev completed and All Batches should be shown as per LIFO
Sanity testing usecase_Prod	105	
Sanity testing usecase_Prod	106	
Sanity testing usecase_Prod	107		Go to a batch and open a file under "Review Pending"
Sanity testing usecase_Prod	108	
Sanity testing usecase_Prod	109	
Sanity testing usecase_Prod	110	
Sanity testing usecase_Prod	111		Rotation has to be done manually, auto rotation from AI will not happen.
Sanity testing usecase_Prod	112		Rotated file will have extraction and snippet support on Extraction Screen.
Sanity testing usecase_Prod	113		In case if failure or Extraction & snippet support, user will have to fill the extraction data himself but the output will be as per point 2. (Controlled testing with Abhijeet)
Sanity testing usecase_Prod	114		Draging, Movement of Pg's, Files, reordering on Classification Review Screen
Sanity testing usecase_Prod	115	
Sanity testing usecase_Prod	116	
Sanity testing usecase_Prod	117	
Sanity testing usecase_Prod	118	
Sanity testing usecase_Prod	119	
Sanity testing usecase_Prod	120	
Sanity testing usecase_Prod	121		Check if OCR extracted values are correct and document type is correct
Sanity testing usecase_Prod	122	
Sanity testing usecase_Prod	123	
Sanity testing usecase_Prod	124	
Sanity testing usecase_Prod	125		Click on "Review Classification" button and submit classification again and check whole flow as above
Sanity testing usecase_Prod	126		Change values through contouring and manually typing in the fields, Check field for Validations
Sanity testing usecase_Prod	127	
Sanity testing usecase_Prod	128	
Sanity testing usecase_Prod	129		Check if a partially reviewed document has gone through classification review for the second time, the reviewed status should be reset
Sanity testing usecase_Prod	130	
Sanity testing usecase_Prod	131	
Sanity testing usecase_Prod	132	
Sanity testing usecase_Prod	133	
Sanity testing usecase_Prod	134	
Sanity testing usecase_Prod	135	
Sanity testing usecase_Prod	136	
Sanity testing usecase_Prod	137	
Sanity testing usecase_Prod	138		Make sure the input/changes in Classification & Splitting and extraction data and reflection as Output (while exporting once the file is Review Completed)
Sanity testing usecase_Prod	139	
Sanity testing usecase_Prod	140	
Sanity testing usecase_Prod	141	
Sanity testing usecase_Prod	142	
Sanity testing usecase_Prod	143	
Sanity testing usecase_Prod	144	
Sanity testing usecase_Prod	145		Hard reset: Will come from Extraction Screen>Class Review Screen and will reset the changes made to the original AI classifcation (file uploded).
1) all files in extraction
2) few files in review completed
3) few files in all 3 headers
and so on On clicking Hard Reset, all files of the batch123 (under any tab) will all move to Rev pending and come to original state. The Batch123 filter will reflect and the user will be able to see that Batch/file in Rev Pending.
eg if Sup QC>Class Rev screen>Hard reset... the batch will move to Indexer Rev Pending in My Docs Screen and the user will be auto navigated there.
Sanity testing usecase_Prod	146		Failure Handling : Controlled testing when AI APIs are failed on purpose (OCR, KVP, Snippet, Rotate, PDF Merger)
Contact Abhijeet for further details
Sanity testing usecase_Prod	147		Vendor missing in the list if changed by Indexer and submitted. The same changed vendor should get submitted by Supervisor
Sanity testing usecase_Prod	148	
Sanity testing usecase_Prod	149	
Sanity testing usecase_Prod	150		Draw the table bound:Add a table if not already present/extracted from the document
Sanity testing usecase_Prod	151		Delete the Rows & Columns:Use control and click to delete row/column
Sanity testing usecase_Prod	152		Extract the data:When creating a new table, it is mandatory to define at least 2 horizontal lines ( one full row) to be able to extract data - Table completion API
When a table is already present/extracted, you're going to use AUTO COMPLETION API
Sanity testing usecase_Prod	153		Contouring :Select a table cell to switch context from table to the selected cell - and call the snippet API by moving the contour and resizing it
Sanity testing usecase_Prod	154		Delete the table spanning across multiple pages:We can delete the whole table spanning across the multiple pages
Sanity testing usecase_Prod	155		Extracting the text from description:We can extract the desired text from the description using the contour
Sanity testing usecase_Prod	156		Dropdown against every extracted columns:Column mapping to the local columns extracted from the table itself - filling values of a column using the metadata which present from the local columns
Sanity testing usecase_Prod	157		Detect the table:It is a function which allows to detect the table in the document
Sanity testing usecase_Prod	158		Full Width:To expand the extracted column view in the horizontal view
Sanity testing usecase_Prod	159		Delete the table:This function will allow to delete the table
Sanity testing usecase_Prod	160		Highlighted value in the dropdown :Once we select the local columns, it by default highlights the column for which values are fetched in that column from the document
Sanity testing usecase_Prod	161		Clear the data :It will delete the fetched data in the respective column
Sanity testing usecase_Prod	162		Table always to be continuous:There should not be discontinuity between the pages of the table
Sanity testing usecase_Prod	163		Use Shift only to specific page:Pressing shift while dealing with column movement will allow the user to make this change reflect across all pages which have that particular column
Sanity testing usecase_Prod	164		Add to all the pages using add table:Adding a new table for a multipage document will ask you to select a range in which you want the table to be present
Sanity testing usecase_Prod	165	
Sanity testing usecase_Prod	166		Autofill:Yet to be developed- While we type partial text in the extracted filed, it should autofill the predicted value/text
Sanity testing usecase_Prod	167		Split Columns:If we observe that content of 2 columns are merged in one column, using this feature, we can seperate the columns and contents
Sanity testing usecase_Prod	168		Reset:It will reset the table to the default as it was in the first place
Sanity testing usecase_Prod	169		Autofill:This will allow the user to type few text and then autofill the rest of the identified words from the contoured area
Sanity testing usecase_Prod	170		Addition of H & L in Vendor details:H denotes as Header only and L denotes both Header and Tabular to be available for the indexers to work on
Sanity testing usecase_Prod	171	
Combination	1	
Combination	2		-----Single page/ two page/ 5page/ tilted single page
-----Classification:
1 page: INV TO SD
2 pages: SAME INV apply A and Z strategy
5 pages: 2 doc merge and 2 merged doc Separated and make 2 doc and 5 doc move into 2 pager file and rotate
Tilted file: apply Orient feature
---Submitted
Review screen
1pager direct submit
2pager changes below
Manually edit. Apply contouring, add text, delete value, select value on page 2 
Table changes
Vendor change H TO L
Edit table
Use features
Add row and column, select drop-down, use auto fill, clear space,
Manually edit. Apply contouring, add text, delete value, select value, clear data, use validation submit
Review completed
Check all the changes done on review pending screen, that is visible on the review completed screen
Combination	3		----Single page/ two page/ 5page/ tilted single page
-----Classification:
1 page: INV TO SD
2 pages: SAME INV apply A and Z strategy
5 pages: 2 doc merge and 2 merged doc Separated and make 2 doc and 5 doc move into 2 pager file and rotate
Tilted file: apply Orient feature
---Submitted
Review screen
1pager direct submit
2pager changes below
Manually edit. Apply contouring, add text, delete value, select value on page 2 
Table changes
Vendor change H TO L
Add table
Use features
Draw the table and extract the value, select drop-down, use auto fill, clear space,
Manually edit. Apply contouring, add text, delete value, select value, clear data, use validation submit
Review completed
Check all the changes done on review pending screen, that is visible on the review completed screen
Combination	4		-----Single page/ two page/ 5page/ tilted single page
-----Classification:
1 page: INV TO SD
2 pages: SAME INV apply A and Z strategy
5 pages: 2 doc merge and 2 merged doc Separated and make 2 doc and 5 doc move into 2 pager file and rotate
Tilted file: apply Orient feature
---Submitted
Review screen
1pager direct submit
2pager changes below
Manually edit. Apply contouring, add text, delete value, select value on page 2 
Table changes
Vendor change H TO L

Use features
Deleted the extracted table and add new table
, select drop-down, use auto fill, clear space,
Manually edit. Apply contouring, add text, delete value, select value, clear data, use validation submit
Review completed
Check all the changes done on review pending screen, that is visible on the review completed screen
Combination	5		-----Single page/ two page/ 5page/ tilted single page
-----Classification:
1 page: INV TO SD
2 pages: SAME INV apply A and Z strategy
5 pages: 2 doc merge and 2 merged doc Separated and make 2 doc and 5 doc move into 2 pager file and rotate
Tilted file: apply Orient feature
---Submitted
Review screen
1pager direct submit
2pager changes below
Manually edit. Apply contouring, add text, delete value, select value on page 2 
Table changes
Vendor change H TO L

Use features
Deleted the extracted table and reset table
, select drop-down, use auto fill, clear space,
Manually edit. Apply contouring, add text, delete value, select value, clear data, use validation submit
Review completed
Check all the changes done on review pending screen, that is visible on the review completed screen
Combination	6		-----Single page/ two page/ 5page/ tilted single page
-----Classification:
1 page: INV TO SD
2 pages: SAME INV apply A and Z strategy
5 pages: 2 doc merge and 2 merged doc Separated and make 2 doc and 5 doc move into 2 pager file and rotate
Tilted file: apply Orient feature
---Submitted
Review screen
1pager direct submit
2pager changes below
Manually edit. Apply contouring, add text, delete value, select value on page 2 
Table changes
Vendor change L TO L
Edit table
Use features
Add row and column, select drop-down, use auto fill, clear space,
Manually edit. Apply contouring, add text, delete value, select value, clear data, use validation submit
Review completed
Check all the changes done on review pending screen, that is visible on the review completed screen
Combination	7		-----Single page/ two page/ 5page/ tilted single page
-----Classification:
1 page: INV TO SD
2 pages: SAME INV apply A and Z strategy
5 pages: 2 doc merge and 2 merged doc Separated and make 2 doc and 5 doc move into 2 pager file and rotate
Tilted file: apply Orient feature
---Submitted
Review screen
1pager direct submit
2pager changes below
Manually edit. Apply contouring, add text, delete value, select value on page 2 
Table changes
Vendor change L TO L
Add table
Use features
Draw the table and extract the value, select drop-down, use auto fill, clear space,
Manually edit. Apply contouring, add text, delete value, select value, clear data, use validation submit
Review completed
Check all the changes done on review pending screen, that is visible on the review completed screen
Combination	8		-----Single page/ two page/ 5page/ tilted single page
-----Classification:
1 page: INV TO SD
2 pages: SAME INV apply A and Z strategy
5 pages: 2 doc merge and 2 merged doc Separated and make 2 doc and 5 doc move into 2 pager file and rotate
Tilted file: apply Orient feature
---Submitted
Review screen
1pager direct submit
2pager changes below
Manually edit. Apply contouring, add text, delete value, select value on page 2 
Table changes
Vendor change L TO L

Use features
Deleted the extracted table and add new table
, select drop-down, use auto fill, clear space,
Manually edit. Apply contouring, add text, delete value, select value, clear data, use validation submit
Review completed
Check all the changes done on review pending screen, that is visible on the review completed screen
Checking -> docx ['xls', 'xlsx']
Checking -> docx ['docx']
Checking -> docx ['docx']
Test-File-> 1631 
 /datadrive/IKG/test_db/test_plans/BRD-Project Mark.docx
executeInstruction => 
global
The above is the summary of a code change and given below are some test cases. You will have to analyze the summary of the code changes and figure out if any of the test cases below require review OR will be impacted by the code changes.

 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Contents
 
1.   	Introduction
2.   	Purpose of the Document
3.   	Scope of Work
4.   	Reports and Accuracy Logics
5.   	Billing Logic
6.   	Technical Infrastructure Requirement Details [PROD & UAT]
 
 
 
 
 
 
 
 
1.	Introduction
The main purpose of this project is to provide an automated invoice processing system “IDP (Intelligent Document Processing)” to ensure smooth, quick and efﬁcient services to REQUORDIT by AmyGB.ai. As compared to other mechanism of invoice processing, IDP will ensure that the requests, which falls under the scope can be addressed within the agreed and definite TAT committed to the customer. This will help the REQUORDIT team to reduce the TAT for document processing and have a higher productivity in their day to day operations.
 
 
 
 
 
2.	Purpose of the Document
The purpose of the document is to list down and document each and every necessary steps like Current Flow, Proposed Flow, Scope of the Project, Technical Infra Details and Report Mechanism.
This Document is prepared on the basis of the discussions carried out by various discussions between REQUORDIT and AmyGB Team.
 
 
 
 
 
 
 
 
 
 
3.    Requirement Scope
An application to be developed by AmyGB to Automate the invoices wherein After the invoice is uploaded via API, Application will carry out the OCR, extract the defined key values and get the review done by the indexers and post review, make the file available to download in the PDF/tiff format.
 
 
An avg. of 8000 documents will get processed/day except month end wherein this may reach up to 20000 (last couple of days of the month). Each doc will contain an average of 5 pages. 80% of the docs for the day are uploaded between 6AM -8AM CDT which to be processed by noon and output will be downloaded by 6 PM CDT. Processing shall happen on AWS - on cloud. Extracted data is deleted (OCR- text + image) post 23 Hrs(From the time file is downloaded) and retain the meta data (which include the kvp, line items, doc class etc.)
 
 
 
 
Below are the discussed scoped items to be delivered in the phase wise manner-
 
1.	Classification/Re-Classification
2.	Extraction & Review
3.	Supervisor QC model
4.	User & Team Management
5.	PDF & TIFF Download options
6.	Rotation of a file
7.	File merging and Re-ordering
8.     File lock and release 
9.	Tabular Content Extraction + Draw Table Bounds only for tabular failures
10.  Tabular OCR Review with merging/splitting of rows on UI
11.  Real time generation of reports of various doc lifecycle
12.  Feedback Module
 
 
 
 
 
 
 
 
 
The following is the High Level process flow-
 
 
 
 
 
 
 
4.	Scope of Work
The Scope of work for the project will be divided into three parts:
I.          Phase 1 Scope
II.        Phase 2 Scope
III.  	Phase 3 Scope
 
 
 
 
 
I.         Phase 1 Scope:
   Features to be considered for Phase 1 are as follows based on the discussions with the REQUORDIT team.
 
1.     Classification & Extraction Review
2.     Supervisor QC model
3.     User & Team Management
4.     PDF & TIFF Download options
5.     Rotation of a file
6.     File merging and Re-ordering
7. 	File lock and release 
8.  	Real time generation of reports
 
 
   	Phase 1 will consist scenarios related to the above categories described as below
l  Classification/Re-Classification & Extraction Review-
A document should go through the OCR, and get classified as per the standard Invoice Or Supporting Document. Post Classification, if we realize there was error in classification, it should have option to re-classify and submit classification. Once classification is done, 16 defined key values with mandatory and optional fields to be extracted and reviewed by the indexers.
 
l  Supervisor QC model-
Also review can be done by the supervisor and admin as part of Supervisor QC review process wherein set % of the particular customer id based on the team it belongs to, files will flow to mapped supervisor for the QC
 
l  User & Team Management-
This will have 3 roles namely as 1) Indexer 2) Supervisor 3) Admin
Sup and Admin role will be allowed the addition, update and mapping of the users to customer IDs and supervisors
 
l  PDF & TIFF Download options-
Once the review is completed and submitted from both indexers and supervisors (in 100% QC model), file should be available to be downloaded as PDF/Tiff for the end customer
 
 
l  Rotation of a file-
If a document is skewed as an input, while performing the classification, it should rotate the image to 90 Degree or more and same should get downloaded as an output post completion of the Classification and Extraction
 
l  File merging and Re-ordering-
If an invoice is available in file1 out of multiple files in the same batch and Supporting Document of the respective invoice is available in file2, we should have option to merge the SD from File2 with Invoice in file1 and vice versa. This should have option of re-ordering of the page count with respect to the changes
 
l  File lock and release-
This feature will function as, once a file is opened by any indexer, it will be locked for 30 minutes and no one else can work on that file. Also, if an indexer opens multiple tabs or try to work on multiple files together, it will release the older files and allow to work on the latest file only to avoid the ambiguity 
 
l  Real time generation of reports of various doc lifecycle-
There should be an option available which can be used to generate    the reports on the real time for the document processing stats from Admin panel
 
 
Note- After the Go Live, the timeline for the Phase 1 hyper-care stage is 75 days where the AmyGB team will analyze the accuracy on the area of opportunities and fix the bugs comes on the way
 
 
II.      Phase 2 Scope:
Features to be considered for Phase 2 are as follows based on the discussions with the REQUORDIT team.
 
1.  	Table extraction spread across multiple pages within same/different files
2.  	Draw table bounds if table missed or incomplete table identified
3.  	Addition of rows at end of the table (new rows or missing row)
4.  	Merge/split/editing of rows of the table
5.  	QC of the extracted tabular data/line items with option to pick values via snippet
   	Phase 2 will consist scenarios related to the above categories described as below
l  Table extraction spread across multiple pages within same/different files-
This feature should allow the data to be extracted from the tabular invoices across multiple pages within same/different files
 
l  Draw table bounds if table is missed or incomplete -
Draw table bounds if table is missed or incomplete table identification for reprocessing via OCR
 
l  Addition of rows-
This feature will allow to add of rows at end of the table (new rows or missing row)
 
l  Merge/split/editing of rows of the table-
This feature should allow to merge/split/Edit the table rows where it is needed
 
l  QC of the extracted tabular data/line items -
This feature should allow to do the QC of the extracted tabular data/line items with option to pick values via snippet
 
 
Note- After the Go Live, the timeline for the Phase 2 to start is after 75 days, the requirements for the Phase 2 will commence.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
III.   Phase 3 Scope:
Features to be considered for Phase 3 are as follows based on the discussions with the REQUORDIT team.
 
1.     Feedback Module
 
 
   	Phase 3 will consist scenarios related to the above categories described as below
l  Feedback Module-
This feature should allow the application to accept the feedback bases the inputs from the users. This will allow Key - Value Feedback - Instead of manual corrections on all key value extraction give feedback on selected failed files. The USP of this feature is that it also retrains the doc extraction module so that similar errors are not repeated in future batches
 
 
5.	Reports and Accuracy Logics
This section will give a clarity on the Mechanism of Reports for Uploaded and Processed Data.
 
After we go live with the solution in place, we shall generate the reports for document life cycle which should have visibility on page level details of the Invoices and supporting documents for billing purpose.
 
l  Monthly Billing Reports: Monthly billing reports will be created for billing purpose. This report will contain all the invoices along with page detail processed by the IDP for a specific month. This report will be used by AmyGB for all the invoicing purpose
 
 
6.	Billing Logic
The billing will be based on the total number of pages of the Invoices processed by the application. Commercials are closed over other email thread.
 
 
 
 
 
7.	Technical Infrastructure Requirement Details. [PROD & UAT]
This section will list down all the technical level infrastructural details for Production Server that will be necessary for this project. The scope of the project will require technical infrastructure at the REQUORDIT UAT server and also at the Production server.
 
 
 
 
 
 
Infrastructure for UAT/Staging Server for UAT Testing:
After the training of the Application, the codes will be deployed on the UAT server of the REQUORDIT. It is important for the UAT server infrastructure to be similar to PROD server, as all the initial level testing will get carried out on the UAT server. UAT server should also have the similar API’s as the PROD server for the AmyGB to carry out the UAT level testing for the same.
 
Note- Any changes, bug fix or new requirements to be tested on Staging first before deploying it to Prod to have the no impact on the Prod environment
 
 
THANK YOU
 


Checking -> xlsx ['xls', 'xlsx']
Checking -> xlsx ['docx']
Checking -> xlsx ['xls', 'xlsx']
SENDING TEST IMP ANALYSIS!!
 {
    "impacted_method": [
        "Based on the analysis of the code change, I've identified the following test cases that require review or may be impacted by the code changes:\n\n1. **JSON parsing tests**: Since the code change modifies the response from the POST request, it may affect the JSON parsing in the downstream file. Test cases that verify JSON parsing should be reviewed and re-run to ensure they still pass.\n2. **Character encoding tests**: The removal of explicit UTF-8 decoding may lead to issues with character encoding. Test cases that verify character encoding should be reviewed and re-run to ensure they still pass.\n3. **String manipulation tests**: Since the code change affects the way strings are handled, test cases that verify string manipulation (e.g., concatenation, substring extraction) should be reviewed and re-run to ensure they still pass.\n\nAdditionally, test cases related to the following sections of the document may also be impacted:\n\n1. **Reports and Accuracy Logics**: If the code change affects the way data is processed and reported, test cases that verify report generation and accuracy may need to be re-run.\n2. **Technical Infrastructure Requirement Details**: If the code change affects the technical infrastructure, test cases that verify technical requirements (e.g., API integrations, server setup) may need to be re-run.\n\nPlease review these test cases and adjust them as necessary to ensure that the code change does not introduce any unexpected behavior or errors."
    ],
    "impacted_code_snippet": [],
    "impacted_code_range": [],
    "impacted_code_context": [
        "What a long document!\n\nAfter analyzing the summary of the code changes and the provided test cases, I'll categorize them into three sections: **Unimpacted**, **Potential Impact**, and **Undetermined**.\n\n**Unimpacted** (no review required):\n\n* Test cases that are not related to the code changes, such as:\n\t+ Purpose of the Document\n\t+ Scope of Work\n\t+ Reports and Accuracy Logics\n\t+ Billing Logic\n\t+ Technical Infrastructure Requirement Details\n\n**Potential Impact** (review recommended):\n\n* Test cases that might be affected by the code changes, such as:\n\t+ Classification/Re-Classification\n\t+ Extraction & Review\n\t+ Supervisor QC model\n\t+ User & Team Management\n\t+ Rotation of a file\n\t+ File merging and Re-ordering\n\t+ File lock and release\n\t+ Real-time generation of reports\n\t+ Tabular Content Extraction\n\t+ Feedback Module\n\nThese test cases might be impacted by the code changes, as they involve data processing, extraction, and review, which are related to the updated code.\n\n**Undetermined** (further analysis required):\n\n* Test cases that are not explicitly mentioned in the code changes, but might be related to the updated code, such as:\n\t+ Page merging and reordering\n\t+ Draw table bounds\n\t+ Merge/split/editing of rows\n\t+ QC of extracted tabular data/line items\n\t+ Monthly billing reports\n\nThese test cases require further analysis to determine if they are impacted by the code changes.\n\nPlease review the code changes and the test cases carefully to ensure that the updates do not introduce any issues or regressions."
    ],
    "criticality": [],
    "impact_analysis": [],
    "impact_type": []
} 
 3311
